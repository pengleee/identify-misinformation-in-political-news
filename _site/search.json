[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this section we want to try unsupervised learning techniques such as clustering and dimentionality reduction in an attempt to get a better understanding of our data.\nThe first unsupervised technique we used in a dimensionality reduction technique. The goal of such techniques is to compress the data down from a large number of attributes to 2-3 attributes so that we can visualize these attributes. There’s different options on how to do this with the main two being t-SNE and PCA. For our analysis we chose to use t-SNE due to the non-linear transformations the technique brings which can help show complex non-linear clusters that PCA would not be able to visualize in the same way.\n\n#Load data\nimport pandas as pd\ndf = pd.read_csv(\"../../data/Clean_Data/liar_dataset/train.csv\")\n\n\n#First couple rows\ndf.head()\n\n\n\n\n\n\n\n\nID\nLabel\nStatement\nSubjects\nSpeaker\nJob_Title\nState_Info\nParty\nBarely_True_Count\nFalse_Count\nHalf_True_Count\nMostly_True_Count\nPants_On_Fire_Count\nContext\n\n\n\n\n0\n2635.json\nfalse\nSays the Annies List political group supports ...\nabortion\ndwayne-bohac\nState representative\nTexas\nrepublican\n0.0\n1.0\n0.0\n0.0\n0.0\na mailer\n\n\n1\n10540.json\nhalf-true\nWhen did the decline of coal start? It started...\nenergy,history,job-accomplishments\nscott-surovell\nState delegate\nVirginia\ndemocrat\n0.0\n0.0\n1.0\n1.0\n0.0\na floor speech.\n\n\n2\n324.json\nmostly-true\nHillary Clinton agrees with John McCain \"by vo...\nforeign-policy\nbarack-obama\nPresident\nIllinois\ndemocrat\n70.0\n71.0\n160.0\n163.0\n9.0\nDenver\n\n\n3\n1123.json\nfalse\nHealth care reform legislation is likely to ma...\nhealth-care\nblog-posting\nNaN\nNaN\nnone\n7.0\n19.0\n3.0\n5.0\n44.0\na news release\n\n\n4\n9028.json\nhalf-true\nThe economic turnaround started at the end of ...\neconomy,jobs\ncharlie-crist\nNaN\nFlorida\ndemocrat\n15.0\n9.0\n20.0\n19.0\n2.0\nan interview on CNN\n\n\n\n\n\n\n\nWe will start by doing some data preprocessing. Our steps are as follows\n\nDefine function to clean data and apply the function to the statements in the liar liar dataset\nConstruct a document-term matrix\nDo dimensionality reduction with NMF\nPlot the words most likely to appear in each topic and analyze\n\n\nfrom nltk.corpus import stopwords\nimport string\nstopwords = set(stopwords.words(\"english\"))\n\ndef clean_statements(text):\n    tmp = ''\n    for i in text:\n        if i.isdigit() != True and i not in string.punctuation:\n            tmp = tmp + i\n\n    text = tmp\n\n    text = text.lower()\n\n    tmp = ''\n    tokenized_text = text.split()\n    for i in tokenized_text:\n        if i not in stopwords:\n            tmp = tmp + i + \" \"\n\n    text = tmp\n\n    return text\n\ndf[\"Statement_Cleaned\"] = df[\"Statement\"].apply(clean_statements)\n\nWe start by removing punctuation marks, numbers, and replacing all uppercase letters with lowercase ones. We will then remove common words that likely have give no information into the legitimacy of a news source. These words commonly called stop words such as ”the”, ”a”, and ”to” were removed using nltk.corpus’s stop words list.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nvectorizer = TfidfVectorizer(min_df=0.01, max_df=0.5)\ndtm = vectorizer.fit_transform(df[\"Statement_Cleaned\"])\ndtm.shape\n\n(10240, 161)\n\n\nNext we will vectorize our data using the TfidfVectorizer. The point of this is to covert our textual data into a numerical format that can be processed by and learned from by a computer.\n\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nnmf_model = NMF(n_components=6, random_state=5000)\nnmf_model.fit(dtm)\n\nNMF(n_components=6, random_state=5000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  NMF?Documentation for NMFiFittedNMF(n_components=6, random_state=5000) \n\n\nThe next step is to use NMF topic modeling to bin the different words in our model into 6 different topics. These topics are assigned by the model with the only hyperparaemter we chose being the number of topics as 6. We chose this to explore how closely these 6 randomly generated clusters would align with the 6 truth categories. If we see that these topics assigned by the model have well defined clusters and contain largely one of the truth categories this would be a very good sign that we could use supervised learning techniques to predict the truthfulness of a statement. However, NMF doesn’t inherently align its clusters with external categories, so the clusters might not align perfectly.\n\nimport matplotlib.pyplot as plt\n\ndef plot_top_words(model, word_list, n_top_words=20):\n    fig, axes = plt.subplots(2, 3, figsize=(20, 15)) #, sharex=True)\n    axes = axes.flatten()\n    for topic_idx, topic in enumerate(model.components_):\n        top_features_ind = topic.argsort()[-n_top_words:]\n        top_features = word_list[top_features_ind]\n        weights = topic[top_features_ind]\n\n        ax = axes[topic_idx]\n        ax.barh(top_features, weights, height=0.7)\n        ax.set_title(f\"Topic {topic_idx}\", fontdict={\"fontsize\": 30})\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n        for i in \"top right left\".split():\n            ax.spines[i].set_visible(False)\n        fig.suptitle(\"Topics in NMF model\", fontsize=40)\n\n    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.5, hspace=0.25)\n    plt.show()\n\n# Your code here (use plot_top_words() to display the distributions of top words\n# for the four estimated topics)\n\nplot_top_words(nmf_model, vectorizer.get_feature_names_out())\n\n\n\n\n\n\n\n\nHere we simply take the results from the previous NMF model to create a graphic that shows the most common words in each topic. What we can see from the above graphic is that each topic has relatively different words that seem to represent different topics. For example topic 4 seems to be about healthcare mentioning insurance, health, care, bill, obamacare, and many others as some of the most common words. Similar thinking could be applied to the other topics as well.\nFinally we get to t-SNE our dimensionality reduction technique that we will use the lower the number of dimensions in our text dataset in order to plot it in a 2 dimensional plane.\n\nterm_topic_df = pd.DataFrame(nmf_model.components_.T)\nterm_topic_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.031090\n0.002590\n0.024015\n0.044204\n0.195420\n0.000000\n\n\n1\n0.000000\n0.033593\n0.024379\n0.106482\n0.031424\n0.028464\n\n\n2\n0.000000\n0.011236\n0.292610\n0.061035\n0.026381\n0.004829\n\n\n3\n0.000000\n0.098518\n0.000000\n0.143353\n0.009457\n0.026065\n\n\n4\n0.019502\n0.065047\n0.031024\n0.114156\n0.000496\n0.140730\n\n\n\n\n\n\n\n\nfrom sklearn.manifold import TSNE\ntsne_topic_model = TSNE()\ntsne_topic_projections = tsne_topic_model.fit_transform(term_topic_df)\ntsne_topic_projections.shape\n\n(161, 2)\n\n\n\n\ntopics = nmf_model.transform(dtm).argmax(axis=1)\n\ntsne = TSNE(n_components=2, perplexity=50, random_state=5000)\ntsne_topic_projections = tsne.fit_transform(nmf_model.transform(dtm))\n\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(tsne_topic_projections[:, 0], tsne_topic_projections[:, 1], c=topics, alpha=0.8)\nplt.colorbar(scatter, label=\"Topic\")\nplt.title(\"t-SNE Visualization of Topic Modeling\")\nplt.xlabel(\"t-SNE Dimension 1\")\nplt.ylabel(\"t-SNE Dimension 2\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from this graph that we were able to make some decently defined clusters with our t-SNE model. Now we want to check if these clusters could relate to the actual labels. These labels are:\n\npants-fire\nfalse\nbarely-true\nhaLf-true\nmostly-true\ntrue\n\nOur final step is to compare these results to our actual truth values to see if there’s any correlation between truth values and our model made topic modeling. If the truth values of these different clusters differ, this would suggest we could nicely separate our data by truth values as is our goal.\nWe will start by using a crosstab to visualize the amount of each truth type in each group\n\ndata = pd.DataFrame({\n    \"x\": tsne_topic_projections[:, 0],\n    \"y\": tsne_topic_projections[:, 1],\n    \"topic\": topics,\n    \"true_label\": df[\"Label\"] \n})\n\ncomparison = pd.crosstab(data[\"topic\"], data[\"true_label\"])\ncomparison\n\n\n\n\n\n\n\ntrue_label\nbarely-true\nfalse\nhalf-true\nmostly-true\npants-fire\ntrue\n\n\ntopic\n\n\n\n\n\n\n\n\n\n\n0\n373\n432\n397\n316\n185\n290\n\n\n1\n108\n114\n204\n232\n34\n170\n\n\n2\n175\n225\n193\n127\n137\n113\n\n\n3\n719\n838\n968\n960\n333\n820\n\n\n4\n195\n258\n215\n185\n112\n159\n\n\n5\n84\n128\n137\n142\n38\n124\n\n\n\n\n\n\n\nIn order to better visualize thow each of these topics may fall into one of the different truth types we have bucketed the truth types into two categoreis:\n\nFalses which contains the pants-fire, false, and barely-true labels in each topic\nTruths which contains the half-true, mostly-true, and true labels in each topic\nRatio This is the ratio of \\(\\frac{Truths}{Falses}\\)\n\n\nfalses = comparison[\"barely-true\"] + comparison[\"false\"] + comparison[\"pants-fire\"]\ntruths = comparison[\"mostly-true\"] + comparison[\"half-true\"] + comparison[\"true\"]\n\ndf_for_tf = pd.DataFrame({\n    \"Falses\": falses,\n    \"Truths\": truths,\n    \"Ratio\": truths/falses\n})\n\ndf_for_tf.head(6)\n\n\n\n\n\n\n\n\nFalses\nTruths\nRatio\n\n\ntopic\n\n\n\n\n\n\n\n0\n990\n1003\n1.013131\n\n\n1\n256\n606\n2.367188\n\n\n2\n537\n433\n0.806331\n\n\n3\n1890\n2748\n1.453968\n\n\n4\n565\n559\n0.989381\n\n\n5\n250\n403\n1.612000\n\n\n\n\n\n\n\nFrom here we can see that the different topics do have significantly different truth ratios with topic 1 having 2.37 times more truths than falses and topic 2 having almost 20% less truths than falses. It is important to reference above what we have definded as truths and falses to understand the full context of this result.\n\n\nIn this section we will run a simple KMeans algorithm to test the effect of an extremely simple clustering algorithm on our data. Specifically we will be seeing if we can create an unsupervised learning model that can use statement length to differentiate the different statements into bins. We will visualize the results and comment on the performance.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\n\ndf['statement_length'] = df['Statement'].apply(lambda x: len(str(x).split()))\n\nlabel_encoder = LabelEncoder()\ndf['label_encoded'] = label_encoder.fit_transform(df['Label'])\n\nX = df[['statement_length']]\n\nkmeans = KMeans(n_clusters=6)\ndf['cluster'] = kmeans.fit_predict(X)\n\nplt.figure(figsize=(8, 6))\nfor label in df['Label'].unique():\n    subset = df[df['Label'] == label]\n    plt.scatter(subset['statement_length'], subset['cluster'], label=label, alpha=0.5)\n\nplt.title(\"Clusters Based On Statement Length For Each Label\")\nplt.xlabel(\"Statement Length\")\nplt.ylabel(\"Cluster\")\nplt.legend(title=\"Label\")\nplt.show()\n\n\n\n\n\n\n\n\nWe found that this clustering algorithm does not do a very good job of differentiating between different labels. This is likely for two reasons. Firstly k-means is an extremely simple model that only can use linear decision boundaries. Second the statement length may just not be a very good method in predicting a label.",
    "crumbs": [
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#clustering",
    "href": "technical-details/unsupervised-learning/main.html#clustering",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "In this section we will run a simple KMeans algorithm to test the effect of an extremely simple clustering algorithm on our data. Specifically we will be seeing if we can create an unsupervised learning model that can use statement length to differentiate the different statements into bins. We will visualize the results and comment on the performance.\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\n\ndf['statement_length'] = df['Statement'].apply(lambda x: len(str(x).split()))\n\nlabel_encoder = LabelEncoder()\ndf['label_encoded'] = label_encoder.fit_transform(df['Label'])\n\nX = df[['statement_length']]\n\nkmeans = KMeans(n_clusters=6)\ndf['cluster'] = kmeans.fit_predict(X)\n\nplt.figure(figsize=(8, 6))\nfor label in df['Label'].unique():\n    subset = df[df['Label'] == label]\n    plt.scatter(subset['statement_length'], subset['cluster'], label=label, alpha=0.5)\n\nplt.title(\"Clusters Based On Statement Length For Each Label\")\nplt.xlabel(\"Statement Length\")\nplt.ylabel(\"Cluster\")\nplt.legend(title=\"Label\")\nplt.show()\n\n\n\n\n\n\n\n\nWe found that this clustering algorithm does not do a very good job of differentiating between different labels. This is likely for two reasons. Firstly k-means is an extremely simple model that only can use linear decision boundaries. Second the statement length may just not be a very good method in predicting a label.",
    "crumbs": [
      "Technical details",
      "Unsupervised Learning"
    ]
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Political misinformation and disinformation have become significant challenges in today’s digital age, potentially threatening democratic processes and public discourse. This analysis examines three key datasets to understand patterns in political statements and news coverage:\n\nThe LIAR dataset containing fact-checked political statements\nMediaCloud API data providing news articles from various media outlets\nNews API data offering recent political news coverage",
    "crumbs": [
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "technical-details/eda/main.html#liar-dataset",
    "href": "technical-details/eda/main.html#liar-dataset",
    "title": "Exploratory Data Analysis",
    "section": "LIAR Dataset",
    "text": "LIAR Dataset\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset\nfile_path = '../../data/Clean_Data/liar_dataset/train.csv'\nliar_df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(liar_df.head())\n\n# Display basic information about the dataset\nprint(liar_df.info())\n\n# Display summary statistics of the dataset\nprint(liar_df.describe())\n\n# Check for missing values\nprint(liar_df.isnull().sum())\n\n# Plot the distribution of the target variable\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Label', data=liar_df)\nplt.title('Distribution of Labels')\nplt.show()\n\n# Plot the distribution of the word count in statements\nliar_df['word_count'] = liar_df['Statement'].apply(lambda x: len(str(x).split()))\nplt.figure(figsize=(10, 6))\nsns.histplot(liar_df['word_count'], bins=30, kde=True)\nplt.title('Distribution of Word Count in Statements')\nplt.show()\n\n           ID        Label                                          Statement  \\\n0   2635.json        false  Says the Annies List political group supports ...   \n1  10540.json    half-true  When did the decline of coal start? It started...   \n2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n3   1123.json        false  Health care reform legislation is likely to ma...   \n4   9028.json    half-true  The economic turnaround started at the end of ...   \n\n                             Subjects         Speaker             Job_Title  \\\n0                            abortion    dwayne-bohac  State representative   \n1  energy,history,job-accomplishments  scott-surovell        State delegate   \n2                      foreign-policy    barack-obama             President   \n3                         health-care    blog-posting                   NaN   \n4                        economy,jobs   charlie-crist                   NaN   \n\n  State_Info       Party  Barely_True_Count  False_Count  Half_True_Count  \\\n0      Texas  republican                0.0          1.0              0.0   \n1   Virginia    democrat                0.0          0.0              1.0   \n2   Illinois    democrat               70.0         71.0            160.0   \n3        NaN        none                7.0         19.0              3.0   \n4    Florida    democrat               15.0          9.0             20.0   \n\n   Mostly_True_Count  Pants_On_Fire_Count              Context  \n0                0.0                  0.0             a mailer  \n1                1.0                  0.0      a floor speech.  \n2              163.0                  9.0               Denver  \n3                5.0                 44.0       a news release  \n4               19.0                  2.0  an interview on CNN  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10240 entries, 0 to 10239\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   10240 non-null  object \n 1   Label                10240 non-null  object \n 2   Statement            10240 non-null  object \n 3   Subjects             10238 non-null  object \n 4   Speaker              10238 non-null  object \n 5   Job_Title            7342 non-null   object \n 6   State_Info           8030 non-null   object \n 7   Party                10238 non-null  object \n 8   Barely_True_Count    10238 non-null  float64\n 9   False_Count          10238 non-null  float64\n 10  Half_True_Count      10238 non-null  float64\n 11  Mostly_True_Count    10238 non-null  float64\n 12  Pants_On_Fire_Count  10238 non-null  float64\n 13  Context              10138 non-null  object \ndtypes: float64(5), object(9)\nmemory usage: 1.1+ MB\nNone\n       Barely_True_Count   False_Count  Half_True_Count  Mostly_True_Count  \\\ncount       10238.000000  10238.000000     10238.000000       10238.000000   \nmean           11.533210     13.286482        17.133718          16.434265   \nstd            18.973764     24.112936        35.846511          36.151688   \nmin             0.000000      0.000000         0.000000           0.000000   \n25%             0.000000      0.000000         0.000000           0.000000   \n50%             2.000000      2.000000         3.000000           3.000000   \n75%            12.000000     12.000000        13.000000          11.000000   \nmax            70.000000    114.000000       160.000000         163.000000   \n\n       Pants_On_Fire_Count  \ncount         10238.000000  \nmean              6.201407  \nstd              16.128927  \nmin               0.000000  \n25%               0.000000  \n50%               1.000000  \n75%               5.000000  \nmax             105.000000  \nID                        0\nLabel                     0\nStatement                 0\nSubjects                  2\nSpeaker                   2\nJob_Title              2898\nState_Info             2210\nParty                     2\nBarely_True_Count         2\nFalse_Count               2\nHalf_True_Count           2\nMostly_True_Count         2\nPants_On_Fire_Count       2\nContext                 102\ndtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom wordcloud import WordCloud\n\n# Combine all statements into a single string\ntext = ' '.join(liar_df['Statement'].dropna().tolist())\n\n# Generate the word cloud\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n\n# Display the word cloud\nplt.figure(figsize=(10, 6))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Statements')\nplt.show()",
    "crumbs": [
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "technical-details/eda/main.html#mediacloud-api-data",
    "href": "technical-details/eda/main.html#mediacloud-api-data",
    "title": "Exploratory Data Analysis",
    "section": "MediaCloud API Data",
    "text": "MediaCloud API Data\n\n# Load the dataset\nfile_path = '../../data/Clean_Data/mediacloud_api_data/politics-storylist-20231129_20231130.csv'\nmc_df = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(mc_df.head())\n\n# Display basic information about the dataset\nprint(mc_df.info())\n\n# Display summary statistics of the dataset\nprint(mc_df.describe())\n\n# Check for missing values\nprint(mc_df.isnull().sum())\n\n                                                  id publish_date  \\\n0  7e06a4550edbe6821abfff04cf9bbbf7ea6523f778cafc...   2023-11-30   \n1  9539e8b50a475270c67736b536205ae460feca342a875b...   2023-11-29   \n2  9e4c35528bd0401beafa6b3daa58f55d2603dfdf737cc1...   2023-11-30   \n3  65b94f2969eb37896b58ecf9d8735949b82062e323b331...   2023-11-29   \n4  37e077ba3c89f3b590622fd6afb99e66bcab607526f17d...   2023-11-29   \n\n                                               title  \\\n0  President Biden Is Sharpening His Attacks On F...   \n1  The GOP Is Having An Identity Crisis Over Amer...   \n2  The Downballot: Should Democrats freak out ove...   \n3  Senate panel approves former Maryland Gov. Mar...   \n4  'Our eyes are open': Mary Trump calls out 'MAG...   \n\n                                                 url language  \\\n0  https://www.npr.org/2023/11/30/1197958301/npr-...       en   \n1  https://www.npr.org/2023/11/29/1197958272/npr-...       en   \n2  https://www.dailykos.com/stories/2023/11/30/22...       en   \n3  https://www.baltimoresun.com/politics/bs-md-po...       en   \n4     https://www.rawstory.com/mary-trump-maga-mike/       en   \n\n         media_name                indexed_date  \n0           npr.org  2024-05-30 12:30:00.190537  \n1           npr.org  2024-05-30 12:29:05.342420  \n2      dailykos.com  2024-03-06 23:08:38.168700  \n3  baltimoresun.com  2024-02-15 17:25:16.315609  \n4      rawstory.com  2024-02-15 17:14:29.895973  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1203 entries, 0 to 1202\nData columns (total 7 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   id            1203 non-null   object\n 1   publish_date  1203 non-null   object\n 2   title         1203 non-null   object\n 3   url           1203 non-null   object\n 4   language      1203 non-null   object\n 5   media_name    1203 non-null   object\n 6   indexed_date  1203 non-null   object\ndtypes: object(7)\nmemory usage: 65.9+ KB\nNone\n                                                       id publish_date  \\\ncount                                                1203         1203   \nunique                                               1203            2   \ntop     7aa9fec6a1e2a08b25cfe6950748540e0854c673cb828c...   2023-11-30   \nfreq                                                    1          632   \n\n                                                    title  \\\ncount                                                1203   \nunique                                               1106   \ntop     Inside the violent threat against the Beatles’...   \nfreq                                                    6   \n\n                                                      url language  \\\ncount                                                1203     1203   \nunique                                               1203        2   \ntop     https://www.benzinga.com/pressreleases/23/11/g...       en   \nfreq                                                    1     1202   \n\n         media_name         indexed_date  \ncount          1203                 1203  \nunique          111                  846  \ntop     cbsnews.com  2023-12-01 05:01:37  \nfreq            143                   21  \nid              0\npublish_date    0\ntitle           0\nurl             0\nlanguage        0\nmedia_name      0\nindexed_date    0\ndtype: int64\n\n\n\n# Display the distribution of articles by media_name\nplt.figure(figsize=(12, 6))\nmc_df['publish_date'].value_counts().sort_index().plot(kind='line')\nplt.title('Distribution of Articles Over Time')\nplt.xlabel('Date')\nplt.ylabel('Number of Articles')\nplt.show()\n\n# Display the distribution of articles by language\nplt.figure(figsize=(12, 6))\nmc_df['language'].value_counts().plot(kind='bar')\nplt.title('Distribution of Articles by Language')\nplt.xlabel('Language')\nplt.ylabel('Number of Articles')\nplt.show()\n\n# Display the distribution of article titles length\nmc_df['title_length'] = mc_df['title'].apply(len)\nplt.figure(figsize=(12, 6))\nsns.histplot(mc_df['title_length'], bins=30, kde=True)\nplt.title('Distribution of Article Title Length')\nplt.xlabel('Title Length')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Combine all statements into a single string\ntext = ' '.join(liar_df['Statement'].dropna().tolist())\n\n# Generate the word cloud\nwordcloud = WordCloud(\n    width=1200, \n    height=800,\n    background_color='white',\n    max_words=100,\n    collocations=False\n).generate(text)\n\n# Display the word cloud\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Political Statements')\nplt.show()",
    "crumbs": [
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "technical-details/eda/main.html#news-api-data",
    "href": "technical-details/eda/main.html#news-api-data",
    "title": "Exploratory Data Analysis",
    "section": "News API Data",
    "text": "News API Data\n\n# Load the dataset\nfile_path = '../../data/Clean_Data/news_api_data/news_api_data.csv'\nnews_df  = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(news_df .head())\n\n# Display basic information about the dataset\nprint(news_df .info())\n\n# Display summary statistics of the dataset\nprint(news_df .describe())\n\n# Check for missing values\nprint(news_df .isnull().sum())\n\n             Source               Author  \\\n0               NPR         Kirk Siegler   \n1      Fast Company  Chris Stokel-Walker   \n2          BBC News                  NaN   \n3          BBC News                  NaN   \n4  Business Insider        Madeline Berg   \n\n                                               Title  \\\n0  Trump picks North Dakota Gov. Doug Burgum to l...   \n1  How long will Elon Musk and Donald Trump’s lov...   \n2  How US election fraud claims changed as Trump won   \n3  Meet the DIY group for men fixing loneliness a...   \n4  Sam Altman is getting into the transition team...   \n\n                                         Description  \\\n0  Burgum ran in the 2024 Republican presidential...   \n1  Two massive egos and two idiosyncratic entrepr...   \n2  Both Trump fans and Harris supporters baseless...   \n3  Group that focuses on DIY projects is oversubs...   \n4  OpenAI CEO Sam Altman has been chosen to cocha...   \n\n                                                 URL  Post_Year  Post_Month  \\\n0  https://www.npr.org/2024/11/14/g-s1-34190/trum...       2024          11   \n1  https://www.fastcompany.com/91228113/how-long-...       2024          11   \n2     https://www.bbc.com/news/articles/cy9j8r8gg0do       2024          11   \n3     https://www.bbc.com/news/articles/c1wje3lnw4lo       2024          11   \n4  https://www.businessinsider.com/sam-altman-tap...       2024          11   \n\n   Post_Day Post_Time                                               Post  \n0        15        AM  President-elect Donald Trump said Thursday nig...  \n1        13        PM  The bromance between Donald Trump and Elon Mus...  \n2         9        AM  In the build-up to Tuesdays US election, claim...  \n3         2        AM  Stuart Gregory (left) and Bob Miles (right) we...  \n4        18        PM  Sam Altman has been tapped to cochair San Fran...  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100 entries, 0 to 99\nData columns (total 10 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   Source       100 non-null    object\n 1   Author       86 non-null     object\n 2   Title        100 non-null    object\n 3   Description  100 non-null    object\n 4   URL          100 non-null    object\n 5   Post_Year    100 non-null    int64 \n 6   Post_Month   100 non-null    int64 \n 7   Post_Day     100 non-null    int64 \n 8   Post_Time    100 non-null    object\n 9   Post         100 non-null    object\ndtypes: int64(3), object(7)\nmemory usage: 7.9+ KB\nNone\n       Post_Year  Post_Month    Post_Day\ncount      100.0       100.0  100.000000\nmean      2024.0        11.0   11.430000\nstd          0.0         0.0    6.332943\nmin       2024.0        11.0    1.000000\n25%       2024.0        11.0    6.000000\n50%       2024.0        11.0   12.000000\n75%       2024.0        11.0   17.000000\nmax       2024.0        11.0   24.000000\nSource          0\nAuthor         14\nTitle           0\nDescription     0\nURL             0\nPost_Year       0\nPost_Month      0\nPost_Day        0\nPost_Time       0\nPost            0\ndtype: int64\n\n\n\n# Plot the distribution of articles by source\nplt.figure(figsize=(12, 6))\nsns.countplot(y='Source', data=news_df, order=news_df['Source'].value_counts().index)\nplt.title('Distribution of Articles by Source')\nplt.xlabel('Number of Articles')\nplt.ylabel('Source')\nplt.show()\n\n\n\n# Plot the distribution of article title lengths\nnews_df['title_length'] = news_df['Title'].apply(len)\nplt.figure(figsize=(10, 6))\nsns.histplot(news_df['title_length'], bins=30, kde=True)\nplt.title('Distribution of Article Title Length')\nplt.xlabel('Title Length')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n# Plot the distribution of articles by post day\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Post_Day', data=news_df)\nplt.title('Distribution of Articles by Post Day in Nov. 2024')\nplt.xlabel('Day')\nplt.ylabel('Number of Articles')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Combine all titles into a single string\ntext = ' '.join(news_df['Title'].dropna().tolist())\n\n# Generate the word cloud\nwordcloud = WordCloud(\n    width=1200, \n    height=800,\n    background_color='white',\n    max_words=100,\n    collocations=False\n).generate(text)\n\n# Display the word cloud\nplt.figure(figsize=(15, 10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of News Article Titles')\nplt.show()",
    "crumbs": [
      "Technical details",
      "Exploratory Data Analysis"
    ]
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "We will go through each of our datasources cleaning and combining datasets where necessary. It includes the following sections:",
    "crumbs": [
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#sources",
    "href": "technical-details/data-cleaning/main.html#sources",
    "title": "Data Cleaning",
    "section": "Sources",
    "text": "Sources\nhttps://stackoverflow.com/questions/127803/how-do-i-parse-an-iso-8601-formatted-date-and-time",
    "crumbs": [
      "Technical details",
      "Data Cleaning"
    ]
  },
  {
    "objectID": "data/Raw_Data/news_api.html",
    "href": "data/Raw_Data/news_api.html",
    "title": "DSAN5000 Project",
    "section": "",
    "text": "import requests\nimport json\nimport datetime\n\ntoday_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\nthirty_days_ago = (datetime.datetime.now() - datetime.timedelta(days=30)).strftime(\"%Y-%m-%d\")\n\nwith open('/Users/pengli/.api-keys.json') as f:\n    keys = json.load(f)\n\n# Your API key\nAPI_KEY = keys['NEWS_API_KEY']\n\ndef search_news(search_query, from_date=thirty_days_ago, to_date=today_date, verbose=False):\n    '''\n    date format: YYYY-MM-DD\n    from_date cannot exceed 30 days in the past from the current date due to the limitation of a 30-day window for the free tier of newsapi\n    '''\n    \n    # Base URL\n    url = \"https://newsapi.org/v2/everything\"\n\n    # Parameters for the API request\n    params = {\n        \"q\": search_query,               # Keyword to search for\n        \"language\": \"en\",              # Language filter\n        \"sortBy\": \"relevancy\",         # Sorting (relevancy, popularity, publishedAt)\n        # \"pageSize\": 20,                # Number of results per page\n        \"from\": from_date,               # Start date\n        \"to\": to_date,                 # End date\n        \"apiKey\": API_KEY              # API key\n    }\n    \n    # Create a timestamped output filename\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_filename = f\"{search_query.replace(' ', '_')}_{timestamp}.json\"\n    \n    # Send GET request\n    response = requests.get(url, params=params)\n\n    # Parse JSON response\n    if response.status_code == 200:\n        data = response.json()\n        articles = data.get(\"articles\", [])\n        if verbose:\n            for article in articles:\n                print(f\"Title: {article['title']}\")\n                print(f\"Source: {article['source']['name']}\")\n                print(f\"Published At: {article['publishedAt']}\")\n                print(f\"URL: {article['url']}\")\n                print(\"\\n\")\n        # Save results to a JSON file in the ./data directory\n        output_filename = f\"./news_api_data/{search_query.replace(' ', '_')}_{timestamp}.json\"\n        with open(output_filename, 'w') as f:\n            json.dump(data, f, indent=4)\n    else:\n        print(f\"Error: {response.status_code}\")\n\n\nsearch_news('politics')"
  },
  {
    "objectID": "data/Raw_Data/allthenews_data_kaggle.html",
    "href": "data/Raw_Data/allthenews_data_kaggle.html",
    "title": "DSAN5000 Project",
    "section": "",
    "text": "import kagglehub\n\n# Download latest version\n# https://www.kaggle.com/datasets/davidmckinley/all-the-news-dataset\npath = kagglehub.dataset_download(\"davidmckinley/all-the-news-dataset\")\n\nprint(\"Path to dataset files:\", path)\nfile_path = path + '/all-the-news-2-1.csv'\n\nPath to dataset files: /Users/pengli/.cache/kagglehub/datasets/davidmckinley/all-the-news-dataset/versions/1\n\n\n\nimport pandas as pd\n\n# Define the path to the large CSV file\ninput_csv = file_path\n\n# Define the size of each chunk (number of rows per chunk)\nchunksize = 10000\n\n# Create an iterator to read the CSV file in chunks\nchunk_iter = pd.read_csv(input_csv, chunksize=chunksize)\n\n# Iterate over the chunks and write each one to a new CSV file\nfor i, chunk in enumerate(chunk_iter):\n    if i &lt; 220:\n        continue\n    else:\n        output_csv = f'../data/raw-data/chunk_{i}.csv'\n        chunk.to_csv(output_csv, index=False)\n        print(f'Chunk {i} written to {output_csv}')\n\nChunk 220 written to ../data/raw-data/chunk_220.csv\nChunk 221 written to ../data/raw-data/chunk_221.csv\nChunk 222 written to ../data/raw-data/chunk_222.csv\nChunk 223 written to ../data/raw-data/chunk_223.csv\nChunk 224 written to ../data/raw-data/chunk_224.csv\nChunk 225 written to ../data/raw-data/chunk_225.csv\nChunk 226 written to ../data/raw-data/chunk_226.csv\nChunk 227 written to ../data/raw-data/chunk_227.csv\nChunk 228 written to ../data/raw-data/chunk_228.csv\nChunk 229 written to ../data/raw-data/chunk_229.csv\nChunk 230 written to ../data/raw-data/chunk_230.csv\nChunk 231 written to ../data/raw-data/chunk_231.csv\nChunk 232 written to ../data/raw-data/chunk_232.csv\nChunk 233 written to ../data/raw-data/chunk_233.csv\nChunk 234 written to ../data/raw-data/chunk_234.csv\nChunk 235 written to ../data/raw-data/chunk_235.csv\nChunk 236 written to ../data/raw-data/chunk_236.csv\nChunk 237 written to ../data/raw-data/chunk_237.csv\nChunk 238 written to ../data/raw-data/chunk_238.csv\nChunk 239 written to ../data/raw-data/chunk_239.csv\nChunk 240 written to ../data/raw-data/chunk_240.csv\nChunk 241 written to ../data/raw-data/chunk_241.csv\nChunk 242 written to ../data/raw-data/chunk_242.csv\nChunk 243 written to ../data/raw-data/chunk_243.csv\nChunk 244 written to ../data/raw-data/chunk_244.csv\nChunk 245 written to ../data/raw-data/chunk_245.csv\nChunk 246 written to ../data/raw-data/chunk_246.csv\nChunk 247 written to ../data/raw-data/chunk_247.csv\nChunk 248 written to ../data/raw-data/chunk_248.csv\nChunk 249 written to ../data/raw-data/chunk_249.csv\nChunk 250 written to ../data/raw-data/chunk_250.csv\nChunk 251 written to ../data/raw-data/chunk_251.csv\nChunk 252 written to ../data/raw-data/chunk_252.csv\nChunk 253 written to ../data/raw-data/chunk_253.csv\nChunk 254 written to ../data/raw-data/chunk_254.csv\nChunk 255 written to ../data/raw-data/chunk_255.csv\nChunk 256 written to ../data/raw-data/chunk_256.csv\nChunk 257 written to ../data/raw-data/chunk_257.csv\nChunk 258 written to ../data/raw-data/chunk_258.csv\nChunk 259 written to ../data/raw-data/chunk_259.csv\nChunk 260 written to ../data/raw-data/chunk_260.csv\nChunk 261 written to ../data/raw-data/chunk_261.csv\nChunk 262 written to ../data/raw-data/chunk_262.csv\nChunk 263 written to ../data/raw-data/chunk_263.csv\nChunk 264 written to ../data/raw-data/chunk_264.csv\nChunk 265 written to ../data/raw-data/chunk_265.csv\nChunk 266 written to ../data/raw-data/chunk_266.csv\nChunk 267 written to ../data/raw-data/chunk_267.csv\nChunk 268 written to ../data/raw-data/chunk_268.csv"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-collection/news_api.html",
    "href": "technical-details/data-collection/news_api.html",
    "title": "News API",
    "section": "",
    "text": "News API\nThis data comes from a simple HTTP REST API for searching and retrieving live articles from all over the web. It provides access to headlines and articles from over 80,000 news sources worldwide, supporting features like keyword search, source filtering, and date range queries.\nThis data was the most complicated to extract into a useful format due the the data originally formatted as a JSON file. Reading in the JSON file and looking through the different keys avaialable we were able to extract the data needed ending up with a csv dataframe containing 10 attributes which include data such as article content, article dates/times, article source, article author, and article title.\nIn this section we also attempted to label each of the articles using a Open AI’s API so we could test our model on a second dataset of raw data. Unfortunately, what we found is that none of the OpenAI API models have access to real-time data in order to validate the claims. We did some research into other LLM APIs and found that none of the large LLM APIs have access to real-time or constantly updated information.\nimport requests\nimport json\nimport datetime\n\ntoday_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\nthirty_days_ago = (datetime.datetime.now() - datetime.timedelta(days=30)).strftime(\"%Y-%m-%d\")\n\nwith open('/Users/pengli/.api-keys.json') as f:\n    keys = json.load(f)\n\n# Your API key\nAPI_KEY = keys['NEWS_API_KEY']\n\ndef search_news(search_query, from_date=thirty_days_ago, to_date=today_date, verbose=False):\n    '''\n    date format: YYYY-MM-DD\n    from_date cannot exceed 30 days in the past from the current date due to the limitation of a 30-day window for the free tier of newsapi\n    '''\n    \n    # Base URL\n    url = \"https://newsapi.org/v2/everything\"\n\n    # Parameters for the API request\n    params = {\n        \"q\": search_query,               # Keyword to search for\n        \"language\": \"en\",              # Language filter\n        \"sortBy\": \"relevancy\",         # Sorting (relevancy, popularity, publishedAt)\n        # \"pageSize\": 20,                # Number of results per page\n        \"from\": from_date,               # Start date\n        \"to\": to_date,                 # End date\n        \"apiKey\": API_KEY              # API key\n    }\n    \n    # Create a timestamped output filename\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    output_filename = f\"{search_query.replace(' ', '_')}_{timestamp}.json\"\n    \n    # Send GET request\n    response = requests.get(url, params=params)\n\n    # Parse JSON response\n    if response.status_code == 200:\n        data = response.json()\n        articles = data.get(\"articles\", [])\n        if verbose:\n            for article in articles:\n                print(f\"Title: {article['title']}\")\n                print(f\"Source: {article['source']['name']}\")\n                print(f\"Published At: {article['publishedAt']}\")\n                print(f\"URL: {article['url']}\")\n                print(\"\\n\")\n        # Save results to a JSON file in the ./data directory\n        output_filename = f\"./news_api_data/{search_query.replace(' ', '_')}_{timestamp}.json\"\n        with open(output_filename, 'w') as f:\n            json.dump(data, f, indent=4)\n    else:\n        print(f\"Error: {response.status_code}\")\nsearch_news('politics')"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html",
    "href": "technical-details/data-collection/mediacloud_api.html",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "This data comes from a comprehensive platform offering access to a vast collection of news articles and media content. The API enables programmatic access to analyze news coverage, track topics across different media sources, and study media ecosystems over time.\nReference:\n\nhttps://github.com/mediacloud/api-tutorial-notebooks\nhttps://www.mediacloud.org\n\n# ! pip install mediacloud\nimport json\nimport os\n\nhome_dir = os.environ['HOME']\n\nwith open(f'{home_dir}/.api-keys.json') as f:\n    keys = json.load(f)\n\n# Your API key\nAPI_KEY = keys['MEDIACLOUD_API_KEY']\n\n\n# Set up your API key and import needed things\nimport os, mediacloud.api\nfrom importlib.metadata import version\nfrom dotenv import load_dotenv\nimport datetime as dt\nfrom IPython.display import JSON\nimport bokeh.io\n\nbokeh.io.reset_output()\nbokeh.io.output_notebook()\nsearch_api = mediacloud.api.SearchApi(API_KEY)\nf'Using Media Cloud python client v{version(\"mediacloud\")}'\n\n\n&lt;a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"&gt;&lt;/a&gt;\n&lt;span id=\"f82c1b51-f0a0-43e6-aa84-8c61f4d54897\"&gt;Loading BokehJS ...&lt;/span&gt;\n\n'Using Media Cloud python client v4.3.0'\n\n\n\n# check how many stories include the query phrase in the Washington Post (media id #2)\nmy_query = 'politics' # note the double quotes used to indicate use of the whole phrase\nstart_date = dt.date(2019, 7, 1)\nend_date = dt.date(2023, 7, 1)\nsources = [1, 2, 3] # NY Times, Washington Post, CS Monitor\n\nsearch_api.story_count(my_query, start_date, end_date, source_ids=sources)\nmy_query_name = my_query.replace('\"', '')\n# you can see this count by day as well\nresults = search_api.story_count_over_time(my_query, start_date, end_date, source_ids=sources)\n# and you can chart attention over time with some simple notebook work (using Bokeh here)\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\ndf = pd.DataFrame.from_dict(results)\ndf['date']= pd.to_datetime(df['date'])\nsource = ColumnDataSource(df)\np = figure(x_axis_type=\"datetime\", width=900, height=250)\np.line(x='date', y='count', line_width=2, source=source)  # your could use `ratio` instead of `count` to see normalized attention\nshow(p)\n\n\n\n\n\nresults = search_api.story_count(my_query, start_date, end_date, source_ids=sources)\nsource_ratio = results['relevant'] / results['total']\nf'{source_ratio:.2%} of the media sources {sources} stories were about {my_query_name}'\n'11.46% of the media sources [1, 2, 3] stories were about politics'\n\n\n\n\n# check in our collection of country-level US National media sources\nUS_NATIONAL_COLLECTION = 34412234\nresults = search_api.story_count(my_query, start_date, end_date, collection_ids=[US_NATIONAL_COLLECTION])\nus_country_ratio = results['relevant'] / results['total']\n# '{:.2%} of stories from national-level US media sources mentioneded \"climate change\"'.format(us_country_ratio)\nf'{us_country_ratio:.2%} of stories from national-level US media sources mentioneded {my_query_name}'\n'4.94% of stories from national-level US media sources mentioneded politics'\n# now we can compare this to the source-level coverage\ncoverage_ratio = 1 / (source_ratio / us_country_ratio)\nf'{my_query_name} received {coverage_ratio:.2%} times less coverage in {sources} than you might expect based on other US national papers'\n'politics received 43.07% times less coverage in [1, 2, 3] than you might expect based on other US national papers'\n# or compare to another country (India in this case)\nINDIA_NATIONAL = 34412118\nresults = search_api.story_count('\"climate change\"', start_date, end_date, collection_ids=[INDIA_NATIONAL])\nindia_country_ratio = results['relevant'] / results['total']\nf'{india_country_ratio:.2%} of stories from national-level Indian media sources mentioned {my_query_name}'\n'0.50% of stories from national-level Indian media sources mentioned politics'\ncoverage_ratio =  1 / (india_country_ratio / us_country_ratio)\nf'at the national level {my_query_name} is covered {coverage_ratio:.2%} times less in India than the US'\n'at the national level politics is covered 987.39% times less in India than the US'\n\n\n\n# grab the most recent stories about this issue\nstories, _ = search_api.story_list(my_query, start_date, end_date)\nstories[:3]\n[{'id': '5b95e9e6c1aafd926e6adf48bb3fd28e7241fff54322889d811bda065e6bbde8',\n  'media_name': 'yourmiddleeast.com',\n  'media_url': 'yourmiddleeast.com',\n  'title': 'The Arabs’ Moment',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://yourmiddleeast.com/2020/03/06/the-arabs-moment/',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 44, 377339)},\n {'id': 'f709469e307fcba31cfd1fca3a3165c76754c9b8a42f72f1e42423b38f791977',\n  'media_name': 'yahoo.com',\n  'media_url': 'yahoo.com',\n  'title': \"Black voters power Joe Biden's Super Tuesday success\",\n  'publish_date': datetime.date(2020, 3, 5),\n  'url': 'https://news.yahoo.com/black-voters-power-bidens-super-055038667.html',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 42, 376413)},\n {'id': 'cd8607202f26bed7d766b5a884decf83e73ebbeea46095ebbb6f0fb1379ba6b9',\n  'media_name': 'patrika.com',\n  'media_url': 'patrika.com',\n  'title': 'ईरान: Coronavirus से विदेश मंत्री के सलाहकार की गई जान, देश में अब तक 107 की मौत',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://www.patrika.com/gulf-news/iran-foreign-minister-s-advisor-hossein-sheikholeslam-dead-by-coronavirus-107-dead-in-country-so-far-5864564/',\n  'language': 'hi',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 39, 796407)}]\n# let's fetch all the stories matching our query on one day\nall_stories = []\nmore_stories = True\npagination_token = None\nstory_start_date = dt.date(2023,11,29)\nstory_end_date = dt.date(2023,11,30)\nwhile more_stories:\n    page, pagination_token = search_api.story_list(my_query, story_start_date, story_end_date,\n                                                   collection_ids=[US_NATIONAL_COLLECTION],\n                                                   pagination_token=pagination_token)\n    all_stories += page\n    more_stories = pagination_token is not None\nlen(all_stories)\n1203\n\n\nimport csv\n\nstory_start_date_str = story_start_date.strftime('%Y%m%d')\nstory_end_date_str = story_end_date.strftime('%Y%m%d')\n\nfieldnames = ['id', 'publish_date', 'title', 'url', 'language', 'media_name', 'media_url', 'indexed_date']\noutput_filename = f\"./mediacloud_api_data/{my_query_name.replace(' ', '_')}-storylist-{story_start_date_str}_{story_end_date_str}.csv\"\noutput_dir = os.path.dirname(output_filename)\n\nif output_dir and not os.path.exists(output_dir):\n    os.makedirs(output_dir)  # Create the directory if it doesn't exist\n    \nwith open(output_filename, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n    writer.writeheader()\n    for s in all_stories:\n        writer.writerow(s)\n\n\n\n\n# List media producing the most stories matching the search\nINDIA_NATIONAL = 34412118\nresults = search_api.sources(my_query, start_date, end_date, collection_ids=[INDIA_NATIONAL])"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html#set-up-api-key",
    "href": "technical-details/data-collection/mediacloud_api.html#set-up-api-key",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "# Set up your API key and import needed things\nimport os, mediacloud.api\nfrom importlib.metadata import version\nfrom dotenv import load_dotenv\nimport datetime as dt\nfrom IPython.display import JSON\nimport bokeh.io\n\nbokeh.io.reset_output()\nbokeh.io.output_notebook()\nsearch_api = mediacloud.api.SearchApi(API_KEY)\nf'Using Media Cloud python client v{version(\"mediacloud\")}'\n\n\n&lt;a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"&gt;&lt;/a&gt;\n&lt;span id=\"f82c1b51-f0a0-43e6-aa84-8c61f4d54897\"&gt;Loading BokehJS ...&lt;/span&gt;\n\n'Using Media Cloud python client v4.3.0'"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html#attention-from-a-single-media-source",
    "href": "technical-details/data-collection/mediacloud_api.html#attention-from-a-single-media-source",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "# check how many stories include the query phrase in the Washington Post (media id #2)\nmy_query = 'politics' # note the double quotes used to indicate use of the whole phrase\nstart_date = dt.date(2019, 7, 1)\nend_date = dt.date(2023, 7, 1)\nsources = [1, 2, 3] # NY Times, Washington Post, CS Monitor\n\nsearch_api.story_count(my_query, start_date, end_date, source_ids=sources)\nmy_query_name = my_query.replace('\"', '')\n# you can see this count by day as well\nresults = search_api.story_count_over_time(my_query, start_date, end_date, source_ids=sources)\n# and you can chart attention over time with some simple notebook work (using Bokeh here)\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\ndf = pd.DataFrame.from_dict(results)\ndf['date']= pd.to_datetime(df['date'])\nsource = ColumnDataSource(df)\np = figure(x_axis_type=\"datetime\", width=900, height=250)\np.line(x='date', y='count', line_width=2, source=source)  # your could use `ratio` instead of `count` to see normalized attention\nshow(p)\n\n\n\n\n\nresults = search_api.story_count(my_query, start_date, end_date, source_ids=sources)\nsource_ratio = results['relevant'] / results['total']\nf'{source_ratio:.2%} of the media sources {sources} stories were about {my_query_name}'\n'11.46% of the media sources [1, 2, 3] stories were about politics'"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html#research-within-a-country---using-collections",
    "href": "technical-details/data-collection/mediacloud_api.html#research-within-a-country---using-collections",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "# check in our collection of country-level US National media sources\nUS_NATIONAL_COLLECTION = 34412234\nresults = search_api.story_count(my_query, start_date, end_date, collection_ids=[US_NATIONAL_COLLECTION])\nus_country_ratio = results['relevant'] / results['total']\n# '{:.2%} of stories from national-level US media sources mentioneded \"climate change\"'.format(us_country_ratio)\nf'{us_country_ratio:.2%} of stories from national-level US media sources mentioneded {my_query_name}'\n'4.94% of stories from national-level US media sources mentioneded politics'\n# now we can compare this to the source-level coverage\ncoverage_ratio = 1 / (source_ratio / us_country_ratio)\nf'{my_query_name} received {coverage_ratio:.2%} times less coverage in {sources} than you might expect based on other US national papers'\n'politics received 43.07% times less coverage in [1, 2, 3] than you might expect based on other US national papers'\n# or compare to another country (India in this case)\nINDIA_NATIONAL = 34412118\nresults = search_api.story_count('\"climate change\"', start_date, end_date, collection_ids=[INDIA_NATIONAL])\nindia_country_ratio = results['relevant'] / results['total']\nf'{india_country_ratio:.2%} of stories from national-level Indian media sources mentioned {my_query_name}'\n'0.50% of stories from national-level Indian media sources mentioned politics'\ncoverage_ratio =  1 / (india_country_ratio / us_country_ratio)\nf'at the national level {my_query_name} is covered {coverage_ratio:.2%} times less in India than the US'\n'at the national level politics is covered 987.39% times less in India than the US'"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html#listing-stories",
    "href": "technical-details/data-collection/mediacloud_api.html#listing-stories",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "# grab the most recent stories about this issue\nstories, _ = search_api.story_list(my_query, start_date, end_date)\nstories[:3]\n[{'id': '5b95e9e6c1aafd926e6adf48bb3fd28e7241fff54322889d811bda065e6bbde8',\n  'media_name': 'yourmiddleeast.com',\n  'media_url': 'yourmiddleeast.com',\n  'title': 'The Arabs’ Moment',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://yourmiddleeast.com/2020/03/06/the-arabs-moment/',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 44, 377339)},\n {'id': 'f709469e307fcba31cfd1fca3a3165c76754c9b8a42f72f1e42423b38f791977',\n  'media_name': 'yahoo.com',\n  'media_url': 'yahoo.com',\n  'title': \"Black voters power Joe Biden's Super Tuesday success\",\n  'publish_date': datetime.date(2020, 3, 5),\n  'url': 'https://news.yahoo.com/black-voters-power-bidens-super-055038667.html',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 42, 376413)},\n {'id': 'cd8607202f26bed7d766b5a884decf83e73ebbeea46095ebbb6f0fb1379ba6b9',\n  'media_name': 'patrika.com',\n  'media_url': 'patrika.com',\n  'title': 'ईरान: Coronavirus से विदेश मंत्री के सलाहकार की गई जान, देश में अब तक 107 की मौत',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://www.patrika.com/gulf-news/iran-foreign-minister-s-advisor-hossein-sheikholeslam-dead-by-coronavirus-107-dead-in-country-so-far-5864564/',\n  'language': 'hi',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 39, 796407)}]\n# let's fetch all the stories matching our query on one day\nall_stories = []\nmore_stories = True\npagination_token = None\nstory_start_date = dt.date(2023,11,29)\nstory_end_date = dt.date(2023,11,30)\nwhile more_stories:\n    page, pagination_token = search_api.story_list(my_query, story_start_date, story_end_date,\n                                                   collection_ids=[US_NATIONAL_COLLECTION],\n                                                   pagination_token=pagination_token)\n    all_stories += page\n    more_stories = pagination_token is not None\nlen(all_stories)\n1203\n\n\nimport csv\n\nstory_start_date_str = story_start_date.strftime('%Y%m%d')\nstory_end_date_str = story_end_date.strftime('%Y%m%d')\n\nfieldnames = ['id', 'publish_date', 'title', 'url', 'language', 'media_name', 'media_url', 'indexed_date']\noutput_filename = f\"./mediacloud_api_data/{my_query_name.replace(' ', '_')}-storylist-{story_start_date_str}_{story_end_date_str}.csv\"\noutput_dir = os.path.dirname(output_filename)\n\nif output_dir and not os.path.exists(output_dir):\n    os.makedirs(output_dir)  # Create the directory if it doesn't exist\n    \nwith open(output_filename, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n    writer.writeheader()\n    for s in all_stories:\n        writer.writerow(s)"
  },
  {
    "objectID": "technical-details/data-collection/mediacloud_api.html#top-media-sources",
    "href": "technical-details/data-collection/mediacloud_api.html#top-media-sources",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "# List media producing the most stories matching the search\nINDIA_NATIONAL = 34412118\nresults = search_api.sources(my_query, start_date, end_date, collection_ids=[INDIA_NATIONAL])"
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/collaborators-progress-log.html",
    "href": "technical-details/collaborators-progress-log.html",
    "title": "Collaborators Progress log",
    "section": "",
    "text": "Project contribution log:\n11-12-2024\n\nBrainstorm the topics and proposals for the project\n\n11-24-2024\n\nData Collection: NEWS API, Media Cloud API, allthenews dataset, LIAR dataset\nWrite functions to customize the range of news data gathering\n\n12-09-2024\n\nSupervised Learning\nSentiment Analysis\n\n12-12-2024\n\nUpload report instruction and example as reference to the websitete, begin sentiment analysis\n\n12-13-2024\n\nExploratory Data Analysis\n\n12-14-2024\n\nOrganize the website directory\nAdd Word Cloud to EDA\n\n12-16-2024\n\nWrite the sections of Supervised Learning, Results and Analysis, Conclusion for the report\nReformat the pdf report to qmd to to better showcase on the website\nOrganize the website tabs",
    "crumbs": [
      "Technical details",
      "Collaborators Progress Log"
    ]
  },
  {
    "objectID": "technical-details/collaborators-progress-log.html#member-1-peng-li",
    "href": "technical-details/collaborators-progress-log.html#member-1-peng-li",
    "title": "Collaborators Progress log",
    "section": "",
    "text": "Project contribution log:\n11-12-2024\n\nBrainstorm the topics and proposals for the project\n\n11-24-2024\n\nData Collection: NEWS API, Media Cloud API, allthenews dataset, LIAR dataset\nWrite functions to customize the range of news data gathering\n\n12-09-2024\n\nSupervised Learning\nSentiment Analysis\n\n12-12-2024\n\nUpload report instruction and example as reference to the websitete, begin sentiment analysis\n\n12-13-2024\n\nExploratory Data Analysis\n\n12-14-2024\n\nOrganize the website directory\nAdd Word Cloud to EDA\n\n12-16-2024\n\nWrite the sections of Supervised Learning, Results and Analysis, Conclusion for the report\nReformat the pdf report to qmd to to better showcase on the website\nOrganize the website tabs",
    "crumbs": [
      "Technical details",
      "Collaborators Progress Log"
    ]
  },
  {
    "objectID": "technical-details/collaborators-progress-log.html#member-2-jonah-lichtenthal",
    "href": "technical-details/collaborators-progress-log.html#member-2-jonah-lichtenthal",
    "title": "Collaborators Progress log",
    "section": "Member-2: Jonah Lichtenthal",
    "text": "Member-2: Jonah Lichtenthal\nProject contribution log:\n11-12-2024\n\nInitial meeting to discuss topics\n\n11-18-2024\n\nMeet to build landing page, get data, and assign tasks for next meeting\n\n11-19-2024\n\nUploaded my section of literature review\n\n11-26-2024\n\nCleaned data\n\n12-12-2024\n\nAttempted to label data with LLM\nStarted unsupervised learning section\n\n12-13-2024\n\nFinished unsupervised learning\n\n12-13-2024 - 12-16-2024\n\nCreated report using laTeX on overleaf adding teammate as a collaborator\nCreated outline for report\nWrote abstract, wrote introduction, Copied over/formatted literature review, wrote dataset preprocessing section, added google trends analysis to EDA, Wrote explanations for other charts in EDA, wrote unsupervised learning section",
    "crumbs": [
      "Technical details",
      "Collaborators Progress Log"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Introduction",
    "text": "Introduction\nIn recent years, the proliferation of misinformation (no matter intentional or not) has become a significant concern, particularly in the realm of political news. The rapid dissemination of false information can influence public opinion, disrupt democratic processes, and contribute to societal polarization. As such, identifying and mitigating misinformation is crucial for maintaining the integrity of information ecosystems.\nThis project aims to explore the linguistic markers that distinguish misinformation from credible political news articles. By leveraging advanced natural language processing (NLP) techniques and machine learning models, we seek to uncover patterns and features that are indicative of misinformation. Our goal is to develop robust methods for detecting and analyzing misinformation, thereby contributing to the broader effort to combat the spread of false information in the digital age.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Project Summary",
    "text": "Project Summary\nOur project covers the ongoing crisis of fake news within our society. From politics to conspericy theories, fake news effects us everyday. We will go through a comprehensive overview of what fake news looks like in today’s society and suggest a data-centric approch to solving this ongoing crisis.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Motivation",
    "text": "Motivation\n\nLanguage is the foundation of civilization. It is the glue that holds a people together. It is the first weapon drawn in a conflict.1\n\nLanguage shapes our understanding of the world and influences our perceptions and actions. In the context of political news, language can be used to inform, persuade, or deceive. The strategic use of language in misinformation can manipulate public opinion and obscure the truth. By identifying linguistic markers of misinformation, we can develop tools to detect and counteract false narratives, thereby promoting a more informed and engaged citizenry. This project is motivated by the need to safeguard the integrity of political discourse and ensure that democratic processes are based on accurate and reliable information.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Research Questions",
    "text": "Research Questions\n\nWhat role does sentiment polarity play in distinguishing misinformation from credible political articles?\nHow do linguistic patterns of misinformation differ across political ideologies?\nHow do linguistic markers of misinformation evolve over time in response to political events (like 2024 US Election)?\nCan multimodal features (text + metadata) improve the detection of linguistic markers in misinformation?\nHow does the use of rhetorical devices correlate with the perceived credibility of misinformation articles?",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Literature Review",
    "text": "Literature Review\n\nFake News Detection on Social Media: A Data Mining Perspective\nFake News Detection on Social Media: A Data Mining Perspective is one of the most cited articles on Google Scholar about fake news detection, likely due to its extensive scope. It provides a comprehensive overview, including an exploration of what constitutes fake news, common challenges, and a detailed outline of various detection approaches. These include linguistic approaches (analyzing how the post is worded), visual approaches (detecting fake images), user-based approaches (evaluating the credibility of the poster), post-based approaches (examining how the post was received), and network-based approaches (analyzing where the post was shared).2\n\n\nA Survey on Natural Language Processing for Fake News Detection\nA Survey on Natural Language Processing for Fake News Detection is one of the most comprehensive surveys on fake news detection using Natural Language Processing. It begins by defining fake news and related tasks such as fact-checking, rumor detection, stance detection, and sentiment analysis. The paper provides an extensive review of datasets and methods, covering classification and regression tasks for fake news detection. The approaches include SVMs, Naive Bayes, LSTMs, CNNs, and attention mechanisms. The paper also discusses dataset limitations and recommendations for future work.3\n\n\nAutomatic Detection of Fake News\nAutomatic Detection of Fake News is an innovative study on fake news detection, this article stands out for introducing datasets designed for detecting fake news across multiple domains. The paper provides a comprehensive exploration of linguistic differences between fake and legitimate news, examining features such as syntax, readability, and psycholinguistics. The paper builds models using SVMs that achieve up to 78% accuracy.4\n\n\nTowards News Verification: Deception Detection Methods for News Discourse\nThe researchers investigated whether rhetorical structures and discourse features in news text could reliably predict its veracity. Using data from NPR’s “Bluff the Listener” segment, they employed Rhetorical Structure Theory (RST) and Vector Space Modelling (VSM) to analyse the discourse5\n\n\nDo You Speak Disinformation? Computational Detection of Deceptive News-Like Content Using Linguistic and Stylistic Features\nThe authors leverage explainable machine learning, specifically tree-based models, to analyse linguistic and stylistic features distinguishing genuine from deceptive articles. A substantial, diverse dataset of manually annotated articles is used to train and test the models, addressing limitations of previous studies which often relied on smaller or biased datasets. The results reveal specific linguistic patterns—such as headline length, use of exclamation marks, and presence of past tense verbs—that significantly contribute to accurate disinformation detection, providing valuable insights for journalists, fact-checkers, and algorithm developers. The study ultimately aims to improve automated disinformation detection methods and enhance public understanding of the “language of fake news”.6\n\n\n“Liar, Liar Pants on Fire”: A New Benchmark Dataset for Fake News Detection\nThis paper introduces LIAR, a new, significantly larger dataset (12,836 manually labelled statements) for fake news detection, addressing the previous lack of sufficient labelled data for robust machine learning models. The dataset, sourced from PolitiFact.com, includes rich metadata such as speaker affiliation and context, enabling more sophisticated analysis than previous, smaller datasets. The authors evaluate various machine learning models on LIAR, including a novel hybrid convolutional neural network that integrates textual and metadata features, demonstrating that incorporating metadata improves fake news detection accuracy. The paper’s purpose is to provide a valuable resource for the research community and showcase the benefits of using this enhanced dataset for improving fake news detection algorithms.7",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about-the-authors.html",
    "href": "about-the-authors.html",
    "title": "About the Authors",
    "section": "",
    "text": "Introduction\nPeng Li comes from Shaoxing, Zhejiang, China. He received his B.S. in Data Science and Big Data Technology from the Chinese University of Hong Kong, Shenzhen (CUHK-SZ) in 2024. He also studied at University of California, Irvine (UCI) and University of Bristol, England as visiting students.\nEducation\n\n2024-2026 (Expected): M.S., Data Science and Analytics, Georgetown University\n2020-2024: B.S., Data Science and Big Data Technology, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ)\n\nResearch Interests: NLP, Computational Linguistics\nHobbies: Gaming, reading, writing and photographing\nEmail: pl800@georgetown.edu\n\n\n\n\nIntroduction\nJonah Lichtenthal comes from Millwood, NY, USA. He received his B.S. in Business Analytics from University of Rochester in 2024.\nEducation\n\n2024-2026 (Expected): M.S., Data Science and Analytics, Georgetown University\n2020-2024: B.S., Business Analytics, University of Rochester\n\nResearch Interests: NLP, Deep Learning, LLMs\nHobbies: Gaming, Esports, Swimming, Table Tennis\nEmail: jgl72@georgetown.edu",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "about-the-authors.html#peng-li",
    "href": "about-the-authors.html#peng-li",
    "title": "About the Authors",
    "section": "",
    "text": "Introduction\nPeng Li comes from Shaoxing, Zhejiang, China. He received his B.S. in Data Science and Big Data Technology from the Chinese University of Hong Kong, Shenzhen (CUHK-SZ) in 2024. He also studied at University of California, Irvine (UCI) and University of Bristol, England as visiting students.\nEducation\n\n2024-2026 (Expected): M.S., Data Science and Analytics, Georgetown University\n2020-2024: B.S., Data Science and Big Data Technology, The Chinese University of Hong Kong, Shenzhen (CUHK-SZ)\n\nResearch Interests: NLP, Computational Linguistics\nHobbies: Gaming, reading, writing and photographing\nEmail: pl800@georgetown.edu",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "about-the-authors.html#jonah-lichtenthal",
    "href": "about-the-authors.html#jonah-lichtenthal",
    "title": "About the Authors",
    "section": "",
    "text": "Introduction\nJonah Lichtenthal comes from Millwood, NY, USA. He received his B.S. in Business Analytics from University of Rochester in 2024.\nEducation\n\n2024-2026 (Expected): M.S., Data Science and Analytics, Georgetown University\n2020-2024: B.S., Business Analytics, University of Rochester\n\nResearch Interests: NLP, Deep Learning, LLMs\nHobbies: Gaming, Esports, Swimming, Table Tennis\nEmail: jgl72@georgetown.edu",
    "crumbs": [
      "About the Authors"
    ]
  },
  {
    "objectID": "assets/tree.html",
    "href": "assets/tree.html",
    "title": "DSAN5000 Project",
    "section": "",
    "text": ".\n├── README.md\n├── _quarto.yml\n├── about-the-authors.qmd\n├── assets\n│   ├── 5000-EDA\n│   │   ├── cd-wordcloud.png\n│   │   ├── liar-wordcloud.png\n│   │   ├── mc-title-length.png\n│   │   ├── news-length.png\n│   │   ├── news-source.png\n│   │   └── news-wordcloud.png\n│   ├── 5000-images\n│   │   ├── 5000 Project Report Crosstable 2.JPG\n│   │   ├── Common Words.png\n│   │   ├── Fake News Google Trends.JPG\n│   │   ├── Ratios.JPG\n│   │   └── t-SNE Visualization.png\n│   ├── 5000-supervised-learning\n│   │   ├── adj-pos.png\n│   │   ├── distilbert-sentiment.png\n│   │   ├── top-linguistic-markers.png\n│   │   └── vader-sentiment.png\n│   ├── gu-logo.png\n│   ├── index.qmd\n│   ├── jl-image.jpg\n│   ├── nature.csl\n│   ├── pl-image.jpg\n│   ├── references.bib\n│   └── tree.qmd\n├── build.sh\n├── build_and_deploy.sh\n├── data\n│   ├── Clean_Data\n│   │   ├── default-data\n│   │   │   └── countries_population.csv\n│   │   ├── liar_dataset\n│   │   │   ├── test.csv\n│   │   │   ├── train.csv\n│   │   │   └── valid.csv\n│   │   ├── mediacloud_api_data\n│   │   │   ├── covid-story-list.csv\n│   │   │   └── politics-storylist-20231129_20231130.csv\n│   │   └── news_api_data\n│   │       └── news_api_data.csv\n│   └── Raw_Data\n│       ├── allthenews_data_kaggle.ipynb\n│       ├── default-data\n│       │   └── countries_population.csv\n│       ├── liar_dataset\n│       │   ├── README.txt\n│       │   ├── test.tsv\n│       │   ├── train.tsv\n│       │   └── valid.tsv\n│       ├── mediacloud_api.ipynb\n│       ├── mediacloud_api_data\n│       │   ├── covid-story-list.csv\n│       │   └── politics-storylist-20231129_20231130.csv\n│       ├── news_api.ipynb\n│       ├── news_api_data\n│       │   └── politics_20241124_214935.json\n│       └── x_api.ipynb\n├── deploy.sh\n├── index.qmd\n├── report\n│   ├── 5000-project-report.pdf\n│   └── report.qmd\n├── requirements.txt\n└── technical-details\n    ├── collaborators-progress-log.qmd\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── main_files\n    │   │   └── libs\n    │   │       ├── bootstrap\n    │   │       │   ├── bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css\n    │   │       │   ├── bootstrap-icons.css\n    │   │       │   ├── bootstrap-icons.woff\n    │   │       │   └── bootstrap.min.js\n    │   │       ├── clipboard\n    │   │       │   └── clipboard.min.js\n    │   │       └── quarto-html\n    │   │           ├── anchor.min.js\n    │   │           ├── popper.min.js\n    │   │           ├── quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css\n    │   │           ├── quarto.js\n    │   │           ├── tippy.css\n    │   │           └── tippy.umd.min.js\n    │   ├── mediacloud_api.qmd\n    │   ├── methods.qmd\n    │   ├── news_api.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   └── su_processed_liar_data.csv\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\n\n28 directories, 80 files",
    "crumbs": [
      "Directory Tree"
    ]
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "",
    "text": "Keywords: Machine Learning, Fake News, Misinformation Detection, Sentiment Analysis, Topic modeling, LIAR, POS Tagging",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#data-collectiondata-cleaning",
    "href": "report/report.html#data-collectiondata-cleaning",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Data Collection/Data Cleaning",
    "text": "Data Collection/Data Cleaning\n\nLIAR dataset 7\nThe main dataset we will use to train and test our model is the LIAR dataset. This dataset is a benchmark dataset for fake news detection, containing 12.8K human-labeled short statements from various contexts, including news articles, TV or radio interviews, campaign speeches, social media posts, and fact-checking websites. Contained within the dataset are the statements made, meta information about the statements (who said it, where it was posted), and a truth value which measures the validity of the statement. The following is a table where I list each of these truth statements ranking them from 1 (least truthful) to 6 (most truthful):\n\n\nRanking of truth values\n\n\n1\nPants on Fire\n\n\n2\nFalse\n\n\n3\nBarely-True\n\n\n4\nHalf-True\n\n\n5\nMostly-True\n\n\n6\nTrue\n\n\n\n\nThis is an already cleaned dataset so there was not much data cleaning required. The only change we made was converting the TSV files to CSV files as these are often easier since they can be examined in excel and are just generally a more common format for datasets.\n\n\nMedia Cloud API Data 8\nThis data comes from a comprehensive platform offering access to a vast collection of news articles and media content. The API enables programmatic access to analyze news coverage, track topics across different media sources, and study media ecosystems over time.\n\n\nNews API Data 9\nThis data comes from a simple HTTP REST API for searching and retrieving live articles from all over the web. It provides access to headlines and articles from over 80,000 news sources worldwide, supporting features like keyword search, source filtering, and date range queries.\nThis data was the most complicated to extract into a useful format due the the data originally formatted as a JSON file. Reading in the JSON file and looking through the different keys avaialable we were able to extract the data needed ending up with a csv dataframe containing 10 attributes which include data such as article content, article dates/times, article source, article author, and article title.\nIn this section we also attempted to label each of the articles using a Open AI’s API so we could test our model on a second dataset of raw data. Unfortunately, what we found is that none of the OpenAI API models have access to real-time data in order to validate the claims. We did some research into other LLM APIs and found that none of the large LLM APIs have access to real-time or constantly updated information.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#exploratory-data-analysis-eda",
    "href": "report/report.html#exploratory-data-analysis-eda",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nGoogle Trends Analysis\nTo start out our EDA process we return to our introduction to visualize what the trend of fake news may look like. The graph from google trends:\n\n\n\nGoogle trends graph for popularity of the term “fake news”10\n\n\n\n\nAs can be seen the the term \"fake news\" was not really used before 2016. Interestingly this is when the extremely controversial candidate of Donald Trump emerged onto the scene. Trump’s often criticism of news sources likely impacted the public’s view on these sources. While this large uptick in 2016 could be seen as the term becoming popular, we also see a large increase in usage in 2020 suggesting election cycles may be having an effect on people’s opinions of the news.\n\n\nNews Source Analysis\nIn this section we will explore the sources of our data in order to have a more concrete understanding of where are data is from. This analysis will contain the following:\n\nDistribution of truth labels in fact-checked statements\nTemporal patterns in news coverage\nSource analysis of news articles\n\nThe following is a barchart containing the most popular sources in our dataset along with the number of times that source appeared.\n\n\n\nDistribution of News API Data by Source\n\n\n\n\nFrom the distribution of News API Data by source, we can see the sources are highly unbalanced. The amount of news gathered from NPR, BBC News and Business Insider is far greater than the other sources.\n\n\nText Analysis\nIn this section we will explore our textual data in order to have a more concrete understanding of what this data looks like. This analysis will contain the following:\n\nWord count distributions\nWord clouds to visualize common themes\nStatement length analysis\n\nFor a dataset on news articles, the non-numerical data contains more value than the numerical data. Especially given our task of evaluating the truthfulness of a statement, the textual data is very important. Identifying which words occur most frequently offers valuable insights that help refine and improve our approach to identify misinformation.\nTo visualize this concept, three wordclouds have been created. These wordclouds show the most common words in each of our datasets. Common words that likely have give no information into whether a word is part of a question or an answer have been removed from the visualizations. These words commonly called stopwords such as \"the\", \"a\", and \"to\" were removed.\n\n\n\nLIAR Word Cloud\n\n\n\n\n\n\n\nMediaCloud API Word Cloud\n\n\n\n\n\n\n\nNews API Word Cloud\n\n\n\n\nThese wordclouds are generated from the article titles to visualize the most common words. We can obviously observe that the names of presidents, president candidates and some socioeconomic celebrities appear frequently in all the wordclouds.\nMore specifically, from these three wordclouds we can see a lot of similarities and also some differences. Certain broad words are common in all three datasets such as \"state\", \"say\", and \"america\". Looking closely into the differences we see that these three datasets are likely from different time periods. This is because the News API dataset has more recent names such as Elon Musk and Donald Trump where the other two datasets focus more on Obama and his policies. Looking closer at the data sources we can see this does seem to be the case.\nIn the next two bar charts we are plotting the lengths of the titles of articles from both the MediaCloud and NewsAPI data.\n\n\n\nDistribution of MediaCloud API by Article Title Length\n\n\n\n\n\n\n\nDistribution of News API by Article Title Length\n\n\n\n\nWe can see from these barcharts that the typical title length is around 75 characters both from MediaCloud API data and News API data, which may imply some inherent characteristics of political news titles.\n\n\nVisualization\n\nDistribution plots of key variables\nTime series analysis of publication patterns\nCategorical data visualization",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#unsupervised-machine-learning",
    "href": "report/report.html#unsupervised-machine-learning",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Unsupervised Machine Learning",
    "text": "Unsupervised Machine Learning\nThe first unsupervised technique we used in a dimensionality reduction technique. The goal of such techniques is to compress the data down from a large number of attributes to 2-3 attributes so that we can visualize these attributes. There’s different options on how to do this with the main two being t-SNE and PCA. For our analysis we chose to use t-SNE due to the non-linear transformations the technique brings which can help show complex non-linear clusters that PCA would not be able to visualize in the same way.\nGiven the most important factors in our data are textual we will be working with this type of data throughout this analysis which does add some complexities. Our workflow is as follows:\n\nClean Text: We start by removing punctuation marks, numbers, and replacing all uppercase letters with lowercase ones. We will then remove common words that likely have give no information into the legitimacy of a news source. These words commonly called stop words such as “the”, “a”, and “to” were removed using nltk.corpus’s stop words list.\nText Vectorization: Next we will vectorize our data using the TfidfVectorizer. The point of this is to covert our textual data into a numerical format that can be processed by and learned from by a computer.\nNMF Model: The next step is to use NMF topic modeling to bin the different words in our model into 6 different topics. These topics are assigned by the model with the only hyperparaemter we chose being the number of topics as 6. We chose this to explore how closely these 6 randomly generated clusters would align with the 6 truth categories. If we see that these topics assigned by the model have well defined clusters and contain largely one of the truth categories this would be a very good sign that we could use supervised learning techniques to predict the truthfulness of a statement. However, NMF doesn’t inherently align its clusters with external categories, so the clusters might not align perfectly.\nPlot Topics: Here we simply take the results from the previous NMF model to create a graphic that shows the most common words in each topic.\n\n\n\nMost common words in each NMFtopic\n\n\n\n\nWhile we do see certain words such as \"texas\" appear in multiple topics these topics seem to be well seperated.\nt-SNE: Finally we get to t-SNE our dimensionality reduction technique that we will use the lower the number of dimensions in our text dataset in order to plot it in a 2 dimensional plane.\nPlot t-SNE: Again, here we simply take the results from the previous model and plot them to create a 2 dimensional graph of our textual data color coded by the topics created by the NMF model in step 3.\n\n\n\nt-SNE Visualization\n\n\n\n\nFrom this result we can see there seems to be decently well defined clusters which suggests there is an accurate way to segment the data.\nExamine results: Our final step is to compare these results to our actual truth values to see if there’s any correlation between truth values and our model made topic modeling. If the truth values of these different clusters differ, this would suggest we could nicely separate our data by truth values as is our goal.\nWe will start by using a crosstab to visualize the amount of each truth type in each group\n\n\n\nCrosstable of truth values vs NMF topics\n\n\n\n\nWe won’t spend too much time on this graphic as it’s a bit much. Instead we have included it for reference and will create a binned the truth types into two categories. The table that follows will contain the following columns for each of the NMF topics created earlier:\nFalses: Number of pants-on-fire statements + number of false statements + number of barely-true statements\nTruths: Number of half-true statements + number of mostly-true statements + number of true statements\nRatio: This contains the ratio of how many more true statements there are then false statements as defined above \\(\\frac{Truths}{Falses}\\)\n\n\n\nBinned crosstable of truth values vs NMF topics\n\n\n\n\nFrom here we can see that the different topics do have significantly different truth ratios with topic 1 having 2.37 times more truths than falses and topic 2 having almost 20% less truths than falses. It is important to reference above what we have defined as truths and falses to understand the full context of this result.\n\nCombining all of our findings from this section we were able to visualize well separated clusters and that these clusters have a pretty significantly different amount of truthful statements in each topics. This suggests we will be able to create a model in the supervised learning section that is able to differentiate true and false statements.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#supervised-machine-learning",
    "href": "report/report.html#supervised-machine-learning",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Supervised Machine Learning",
    "text": "Supervised Machine Learning\nIn this part we mainly implement supervised learning on LIAR dataset to identify linguistic markers.\n\nLoad and Explore the Dataset\nThe dataset is loaded and explored to understand its structure and contents. The LIAR dataset consists of 10,240 entries with 14 columns, including ID, Label, Statement, Subjects, Speaker, Job_Title, State_Info, Party, Barely_True_Count, False_Count, Half_True_Count, Mostly_True_Count, Pants_On_Fire_Count, and Context.\n\n\nPreprocess Text Data\nText data is preprocessed to remove stopwords, punctuation, and to tokenize the text. This step is crucial for extracting meaningful features from the text. The preprocessing function tokenizes the text, removes non-alphanumeric tokens, and filters out stopwords and punctuation.\n\n\nExtract Linguistic Features\nPart-of-Speech (POS) tagging is a process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. Here are some common POS tags and their meanings:\n\n\n\nPOS Tag\nDescription\n\n\n\n\nNN\nNoun, singular or mass\n\n\nNNS\nNoun, plural\n\n\nVB\nVerb, base form\n\n\nVBD\nVerb, past tense\n\n\nVBG\nVerb, gerund or present participle\n\n\nVBZ\nVerb, 3rd person singular present\n\n\nJJ\nAdjective\n\n\nRB\nAdverb\n\n\nIN\nPreposition or subordinating conjunction\n\n\nCD\nCardinal number\n\n\n\n Linguistic features are extracted using TF-IDF for lexical features and POS tagging for syntactic features. The TF-IDF vectorizer is used to transform the cleaned text into a matrix of TF-IDF features. Additionally, POS tagging is performed to extract syntactic features, which are then counted and stored. However, in our research we don’t find a significant correlation between POS tagging and truthfulness categories.\n\n\n\nAdjective Usage by Label\n\n\n\n\n\n\nCorrelate Features with Misinformation\nThe data is split into training and testing sets, and a RandomForestClassifier is trained to predict the labels. The classifier’s performance is evaluated using precision, recall, and F1-score metrics. The results indicate varying levels of accuracy across different truthfulness categories.\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\nbarely-true\n0.19\n0.15\n0.17\n482\n\n\nfalse\n0.24\n0.33\n0.28\n603\n\n\nhalf-true\n0.26\n0.27\n0.26\n675\n\n\nmostly-true\n0.24\n0.28\n0.26\n582\n\n\npants-fire\n0.23\n0.11\n0.15\n236\n\n\ntrue\n0.22\n0.17\n0.19\n494\n\n\naccuracy\n\n\n0.23\n3072\n\n\nmacro avg\n0.23\n0.22\n0.22\n3072\n\n\nweighted avg\n0.23\n0.23\n0.23\n3072\n\n\n\nFeature importance is analyzed to understand which linguistic markers are most indicative of misinformation. The top 10 features are visualized using a bar plot, highlighting the most important lexical features identified by the TF-IDF vectorizer.\n\n\n\nTop Linguistic Markers\n\n\n\n\n\n\nSentiment Analysis\nSentiment analysis is performed using two methods:\n\nVADER Sentiment Analysis: The VADER sentiment analyzer is used to calculate sentiment scores for the cleaned text. The sentiment scores are categorized into positive, negative, and neutral labels. The distribution of sentiment across different truthfulness categories is visualized using a count plot.\n\n\n\nVADER Sentiment Distribution Across Truthfulness Categories\n\n\n\n\nHugging Face Transformers: The Hugging Face pipeline for sentiment analysis is used to classify the sentiment of the cleaned text. The sentiment labels are then grouped by truthfulness categories and visualized using a count plot.\n\n\n\nDistilbert Sentiment Distribution Across Truthfulness Categories",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#unsupervised-machine-learning-results",
    "href": "report/report.html#unsupervised-machine-learning-results",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Unsupervised Machine Learning Results",
    "text": "Unsupervised Machine Learning Results\n\nDimensionality Reduction\nUsing t-SNE for dimensionality reduction, we visualized clusters derived from the textual features of news statements. The resulting plot revealed well-defined clusters, suggesting distinct patterns in the data. The separation between clusters indicates that linguistic markers can differentiate categories of misinformation effectively.\n\n\nTopic Modeling\nApplying NMF for topic modeling, six topics were identified, with each characterized by distinct linguistic features. While some overlap in keywords (e.g., Texas) was observed, most topics demonstrated meaningful separations. A key observation was that certain topics correlated strongly with the “truth values” in the LIAR dataset. For example:\n\nTopic 1: Had a truth-to-false ratio of 2.37, suggesting a higher association with truthful statements.\nTopic 2: Exhibited a 20% higher concentration of false statements, reinforcing its alignment with misinformation patterns.\n\n\n\nCross-Tabulation and Truthfulness Analysis\nBy binning truth values into “True” and “False” categories, we observed significant variability in the distribution across topics. This highlights that the NMF-generated topics capture meaningful linguistic markers that correlate with the veracity of statements. These insights lay a strong foundation for supervised learning.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#supervised-machine-learning-results",
    "href": "report/report.html#supervised-machine-learning-results",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Supervised Machine Learning Results",
    "text": "Supervised Machine Learning Results\n\nModel Performance\nWe implemented a Random Forest Classifier on the LIAR dataset, achieving the following performance metrics across six truth categories:\n\n\n\nCategory\nPrecision\nRecall\nF1-Score\n\n\n\n\nBarely-True\n0.19\n0.15\n0.17\n\n\nFalse\n0.24\n0.33\n0.28\n\n\nHalf-True\n0.26\n0.27\n0.26\n\n\nMostly-True\n0.24\n0.28\n0.26\n\n\nPants-on-Fire\n0.23\n0.11\n0.15\n\n\nTrue\n0.22\n0.17\n0.19\n\n\nOverall\n0.23\n0.23\n0.23\n\n\n\nWhile the model’s overall accuracy remains modest, its ability to differentiate between truthfulness categories provides valuable insights into linguistic markers of misinformation.\n\n\nSentiment Analysis\n\nVADER Sentiment Analysis: The analysis revealed distinct sentiment distributions across truthfulness categories, with highly polarized sentiments more common in false or misleading statements.\nHugging Face Transformers: Similar patterns emerged, with more neutral tones linked to truthful statements and exaggerated tones correlated with misinformation.\n\n\n\nLimitations\nPOS tagging analysis did not yield strong correlations with truthfulness categories, suggesting that syntactic features alone may not be reliable indicators of misinformation.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#key-findings",
    "href": "report/report.html#key-findings",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Key Findings",
    "text": "Key Findings\n\nLinguistic Patterns of Misinformation: Our analysis revealed that linguistic markers such as excessive subjectivity and emotional tone are significantly correlated with misinformation. Features like exaggerated sentiment, frequent use of hyperbolic language, and reliance on emotionally charged words were prominent in misleading content.\nThe Role of Sentiment and Style: Sentiment analysis demonstrated that false or misleading statements often exhibit higher sentiment polarity compared to truthful articles. Misleading content tends to have more extreme positive or negative sentiments, while credible news is characterized by more neutral language. This finding aligns with previous studies suggesting that emotional manipulation is a hallmark of misinformation.\nTopic Clustering and Dimensionality Reduction: Unsupervised learning approaches, such as t-SNE and NMF, were effective in revealing distinct clusters within the dataset. These clusters highlighted clear distinctions in the types of statements categorized as true or false. Notably, certain clusters exhibited a higher proportion of false statements, indicating that clustering techniques could be a valuable tool for misinformation detection.\nChallenges and Limitations: Several limitations emerged from this study. First, the performance of the supervised models was hindered by the complexity and subtlety of linguistic cues associated with misinformation. The overlap of linguistic features between credible and misleading articles posed a significant challenge. Furthermore, part-of-speech tagging did not yield strong correlations with misinformation, indicating that syntactic features alone are insufficient for accurate classification. The reliance on pre-existing datasets like the LIAR dataset also constrained the generalizability of the findings.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#implications",
    "href": "report/report.html#implications",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Implications",
    "text": "Implications\nThe results of this study have several important implications for researchers, fact-checkers, and media platforms:\nFor Researchers: This study highlights the value of combining sentiment analysis and clustering techniques to identify misinformation. Future research could further refine these models and explore deeper semantic features, like rhetorical device analysis.\nFor Fact-Checkers: The identification of specific linguistic markers can help fact-checking organizations develop tools to flag potentially misleading articles for review. This approach could be integrated into existing verification workflows to prioritize high-risk content.\nFor Media Platforms: Social media and news platforms could incorporate machine learning models trained on linguistic markers to detect and flag content that shows signs of misinformation. While human oversight would remain essential, these models could serve as an early warning system for potentially harmful content.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#future-work",
    "href": "report/report.html#future-work",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Future Work",
    "text": "Future Work\nThis project could be the groundwork for future exploration of linguistic markers of misinformation. Several avenues for future research are recommended:\nEnhanced Feature Engineering: Future studies could focus on developing new features, such as semantic embeddings and context-aware representations, to better capture the nuanced nature of misinformation.\nImproved Model Accuracy: Exploring advanced machine learning models, such as deep learning approaches (e.g., BERT or GPT-based classifiers), could significantly enhance classification accuracy.\nBroader Dataset Utilization: Incorporating diverse datasets beyond the LIAR dataset could increase generalizability and improve model robustness.\nMulti-Modal Analysis: Future research could analyze non-textual features, such as metadata, images, and social sharing patterns, to provide a holistic approach to misinformation detection.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#final-thoughts",
    "href": "report/report.html#final-thoughts",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis study underscores the power and promise of machine learning in addressing the ongoing challenge of misinformation. By identifying and understanding the linguistic markers that differentiate misinformation from credible news, researchers and practitioners can contribute to a more informed and resilient society. While there is no “silver bullet” solution, the insights derived from this study provide a strong foundation for future innovations in automated misinformation detection. As the landscape of political communication continues to evolve, so too must our methods for safeguarding the integrity of information in public discourse.",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "report/report.html#footnotes",
    "href": "report/report.html#footnotes",
    "title": "Identify Linguistic Markers of Misinformation in Political News Articles",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://dl.acm.org/doi/10.1145/3137597.3137600↩︎\nhttps://arxiv.org/abs/1811.00770↩︎\nhttps://arxiv.org/abs/1708.07104↩︎\nhttps://doi.org/10.13140/2.1.4822.8166↩︎\nhttps://doi.org/10.1080/21670811.2024.2305792↩︎\nhttps://doi.org/10.48550/arXiv.1705.00648↩︎\nhttps://paperswithcode.com/dataset/liar↩︎\nhttps://www.mediacloud.org/documentation/search-api-guide↩︎\nhttps://newsapi.org/↩︎\nhttps://trends.google.com/trends/explore?date=all&q=%22fake%20news%22↩︎",
    "crumbs": [
      "Report",
      "web"
    ]
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\n\n\n\nGive a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters\n\n\n\nInclude the code you used to implement your workflow.\n\n\n\nSummarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "LLM tools were used in the following way for the tasks below",
    "crumbs": [
      "Technical details",
      "LLM Usage Log"
    ]
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools were used to brainstorm ideas and provide feedback and refine the project plan: Identify Linguistic Markers of Misinformation in Political News Articles\nCreating a general proposal about how to complete the project",
    "crumbs": [
      "Technical details",
      "LLM Usage Log"
    ]
  },
  {
    "objectID": "technical-details/llm-usage-log.html#literature-review",
    "href": "technical-details/llm-usage-log.html#literature-review",
    "title": "LLM usage log",
    "section": "Literature Review",
    "text": "Literature Review\n\nHelping to list some relavant papers about fake news detection\nRecommending available datasets\nText summarization for literature review",
    "crumbs": [
      "Technical details",
      "LLM Usage Log"
    ]
  },
  {
    "objectID": "technical-details/llm-usage-log.html#coding",
    "href": "technical-details/llm-usage-log.html#coding",
    "title": "LLM usage log",
    "section": "Coding:",
    "text": "Coding:\n\nCode commenting and explanatory documentation\nDebugging\nExplaining some code statements",
    "crumbs": [
      "Technical details",
      "LLM Usage Log"
    ]
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading the report",
    "crumbs": [
      "Technical details",
      "LLM Usage Log"
    ]
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html",
    "href": "data/Raw_Data/mediacloud_api.html",
    "title": "Media Cloud: Measuring Attention",
    "section": "",
    "text": "Reference: - https://github.com/mediacloud/api-tutorial-notebooks - https://www.mediacloud.org\n# ! pip install mediacloud\nimport json\nimport os\n\nhome_dir = os.environ['HOME']\n\nwith open(f'{home_dir}/.api-keys.json') as f:\n    keys = json.load(f)\n\n# Your API key\nAPI_KEY = keys['MEDIACLOUD_API_KEY']"
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html#set-up-api-key",
    "href": "data/Raw_Data/mediacloud_api.html#set-up-api-key",
    "title": "Media Cloud: Measuring Attention",
    "section": "Set Up API Key",
    "text": "Set Up API Key\n\n# Set up your API key and import needed things\nimport os, mediacloud.api\nfrom importlib.metadata import version\nfrom dotenv import load_dotenv\nimport datetime as dt\nfrom IPython.display import JSON\nimport bokeh.io\n\nbokeh.io.reset_output()\nbokeh.io.output_notebook()\nsearch_api = mediacloud.api.SearchApi(API_KEY)\nf'Using Media Cloud python client v{version(\"mediacloud\")}'\n\n    \n    \n        \n        Loading BokehJS ...\n    \n\n\n\n\n\n'Using Media Cloud python client v4.3.0'"
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html#attention-from-a-single-media-source",
    "href": "data/Raw_Data/mediacloud_api.html#attention-from-a-single-media-source",
    "title": "Media Cloud: Measuring Attention",
    "section": "Attention from a Single Media Source",
    "text": "Attention from a Single Media Source\n\n# check how many stories include the query phrase in the Washington Post (media id #2)\nmy_query = 'politics' # note the double quotes used to indicate use of the whole phrase\nstart_date = dt.date(2019, 7, 1)\nend_date = dt.date(2023, 7, 1)\nsources = [1, 2, 3] # NY Times, Washington Post, CS Monitor\n\nsearch_api.story_count(my_query, start_date, end_date, source_ids=sources)\nmy_query_name = my_query.replace('\"', '')\n\n\n# you can see this count by day as well\nresults = search_api.story_count_over_time(my_query, start_date, end_date, source_ids=sources)\nresults\n\n[{'date': datetime.date(2019, 7, 1),\n  'total_count': 21,\n  'count': 5,\n  'ratio': 0.23809523809523808},\n {'date': datetime.date(2019, 7, 2),\n  'total_count': 21,\n  'count': 3,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2019, 7, 3),\n  'total_count': 17,\n  'count': 2,\n  'ratio': 0.11764705882352941},\n {'date': datetime.date(2019, 7, 4),\n  'total_count': 9,\n  'count': 1,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2019, 7, 5),\n  'total_count': 15,\n  'count': 2,\n  'ratio': 0.13333333333333333},\n {'date': datetime.date(2019, 7, 6),\n  'total_count': 4,\n  'count': 2,\n  'ratio': 0.5},\n {'date': datetime.date(2019, 7, 7), 'total_count': 3, 'count': 0, 'ratio': 0},\n {'date': datetime.date(2019, 7, 8),\n  'total_count': 19,\n  'count': 4,\n  'ratio': 0.21052631578947367},\n {'date': datetime.date(2019, 7, 9),\n  'total_count': 17,\n  'count': 3,\n  'ratio': 0.17647058823529413},\n {'date': datetime.date(2019, 7, 10),\n  'total_count': 20,\n  'count': 4,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 7, 11),\n  'total_count': 23,\n  'count': 4,\n  'ratio': 0.17391304347826086},\n {'date': datetime.date(2019, 7, 12),\n  'total_count': 20,\n  'count': 3,\n  'ratio': 0.15},\n {'date': datetime.date(2019, 7, 13),\n  'total_count': 4,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 7, 14),\n  'total_count': 8,\n  'count': 1,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 7, 15),\n  'total_count': 13,\n  'count': 2,\n  'ratio': 0.15384615384615385},\n {'date': datetime.date(2019, 7, 16),\n  'total_count': 39,\n  'count': 8,\n  'ratio': 0.20512820512820512},\n {'date': datetime.date(2019, 7, 17),\n  'total_count': 21,\n  'count': 5,\n  'ratio': 0.23809523809523808},\n {'date': datetime.date(2019, 7, 18),\n  'total_count': 25,\n  'count': 6,\n  'ratio': 0.24},\n {'date': datetime.date(2019, 7, 19),\n  'total_count': 67,\n  'count': 53,\n  'ratio': 0.7910447761194029},\n {'date': datetime.date(2019, 7, 20),\n  'total_count': 1,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 7, 21),\n  'total_count': 4,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 7, 22),\n  'total_count': 26,\n  'count': 2,\n  'ratio': 0.07692307692307693},\n {'date': datetime.date(2019, 7, 23),\n  'total_count': 20,\n  'count': 2,\n  'ratio': 0.1},\n {'date': datetime.date(2019, 7, 24),\n  'total_count': 17,\n  'count': 1,\n  'ratio': 0.058823529411764705},\n {'date': datetime.date(2019, 7, 25),\n  'total_count': 14,\n  'count': 2,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2019, 7, 26),\n  'total_count': 18,\n  'count': 6,\n  'ratio': 0.3333333333333333},\n {'date': datetime.date(2019, 7, 27),\n  'total_count': 2,\n  'count': 1,\n  'ratio': 0.5},\n {'date': datetime.date(2019, 7, 28),\n  'total_count': 10,\n  'count': 2,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 7, 29),\n  'total_count': 151,\n  'count': 7,\n  'ratio': 0.046357615894039736},\n {'date': datetime.date(2019, 7, 30),\n  'total_count': 31,\n  'count': 3,\n  'ratio': 0.0967741935483871},\n {'date': datetime.date(2019, 7, 31),\n  'total_count': 22,\n  'count': 1,\n  'ratio': 0.045454545454545456},\n {'date': datetime.date(2019, 8, 1),\n  'total_count': 29,\n  'count': 5,\n  'ratio': 0.1724137931034483},\n {'date': datetime.date(2019, 8, 2),\n  'total_count': 23,\n  'count': 9,\n  'ratio': 0.391304347826087},\n {'date': datetime.date(2019, 8, 3),\n  'total_count': 8,\n  'count': 1,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 8, 4),\n  'total_count': 8,\n  'count': 1,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 8, 5),\n  'total_count': 25,\n  'count': 4,\n  'ratio': 0.16},\n {'date': datetime.date(2019, 8, 6),\n  'total_count': 26,\n  'count': 6,\n  'ratio': 0.23076923076923078},\n {'date': datetime.date(2019, 8, 7),\n  'total_count': 21,\n  'count': 2,\n  'ratio': 0.09523809523809523},\n {'date': datetime.date(2019, 8, 8),\n  'total_count': 21,\n  'count': 5,\n  'ratio': 0.23809523809523808},\n {'date': datetime.date(2019, 8, 9),\n  'total_count': 24,\n  'count': 3,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 8, 10),\n  'total_count': 1,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 8, 11),\n  'total_count': 9,\n  'count': 4,\n  'ratio': 0.4444444444444444},\n {'date': datetime.date(2019, 8, 12),\n  'total_count': 22,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 8, 13),\n  'total_count': 22,\n  'count': 4,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2019, 8, 14),\n  'total_count': 28,\n  'count': 5,\n  'ratio': 0.17857142857142858},\n {'date': datetime.date(2019, 8, 15),\n  'total_count': 23,\n  'count': 1,\n  'ratio': 0.043478260869565216},\n {'date': datetime.date(2019, 8, 16),\n  'total_count': 21,\n  'count': 2,\n  'ratio': 0.09523809523809523},\n {'date': datetime.date(2019, 8, 17),\n  'total_count': 5,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 8, 18),\n  'total_count': 3,\n  'count': 1,\n  'ratio': 0.3333333333333333},\n {'date': datetime.date(2019, 8, 19),\n  'total_count': 15,\n  'count': 3,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 8, 20),\n  'total_count': 21,\n  'count': 3,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2019, 8, 21),\n  'total_count': 32,\n  'count': 4,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 8, 22),\n  'total_count': 44,\n  'count': 2,\n  'ratio': 0.045454545454545456},\n {'date': datetime.date(2019, 8, 23),\n  'total_count': 19,\n  'count': 1,\n  'ratio': 0.05263157894736842},\n {'date': datetime.date(2019, 8, 24),\n  'total_count': 4,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 8, 25),\n  'total_count': 7,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 8, 26),\n  'total_count': 24,\n  'count': 7,\n  'ratio': 0.2916666666666667},\n {'date': datetime.date(2019, 8, 27),\n  'total_count': 20,\n  'count': 4,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 8, 28),\n  'total_count': 28,\n  'count': 3,\n  'ratio': 0.10714285714285714},\n {'date': datetime.date(2019, 8, 29),\n  'total_count': 19,\n  'count': 2,\n  'ratio': 0.10526315789473684},\n {'date': datetime.date(2019, 8, 30),\n  'total_count': 17,\n  'count': 3,\n  'ratio': 0.17647058823529413},\n {'date': datetime.date(2019, 8, 31),\n  'total_count': 4,\n  'count': 1,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 9, 1), 'total_count': 4, 'count': 0, 'ratio': 0},\n {'date': datetime.date(2019, 9, 2),\n  'total_count': 12,\n  'count': 2,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 9, 3),\n  'total_count': 18,\n  'count': 3,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 9, 4),\n  'total_count': 24,\n  'count': 6,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 9, 5),\n  'total_count': 28,\n  'count': 2,\n  'ratio': 0.07142857142857142},\n {'date': datetime.date(2019, 9, 6),\n  'total_count': 18,\n  'count': 5,\n  'ratio': 0.2777777777777778},\n {'date': datetime.date(2019, 9, 7),\n  'total_count': 9,\n  'count': 2,\n  'ratio': 0.2222222222222222},\n {'date': datetime.date(2019, 9, 8),\n  'total_count': 8,\n  'count': 3,\n  'ratio': 0.375},\n {'date': datetime.date(2019, 9, 9),\n  'total_count': 20,\n  'count': 2,\n  'ratio': 0.1},\n {'date': datetime.date(2019, 9, 10),\n  'total_count': 22,\n  'count': 4,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2019, 9, 11),\n  'total_count': 25,\n  'count': 4,\n  'ratio': 0.16},\n {'date': datetime.date(2019, 9, 12),\n  'total_count': 24,\n  'count': 3,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 9, 13),\n  'total_count': 32,\n  'count': 9,\n  'ratio': 0.28125},\n {'date': datetime.date(2019, 9, 14),\n  'total_count': 2,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 9, 15),\n  'total_count': 5,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 9, 16),\n  'total_count': 18,\n  'count': 4,\n  'ratio': 0.2222222222222222},\n {'date': datetime.date(2019, 9, 17),\n  'total_count': 20,\n  'count': 3,\n  'ratio': 0.15},\n {'date': datetime.date(2019, 9, 18),\n  'total_count': 30,\n  'count': 6,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 9, 19),\n  'total_count': 27,\n  'count': 2,\n  'ratio': 0.07407407407407407},\n {'date': datetime.date(2019, 9, 20),\n  'total_count': 23,\n  'count': 2,\n  'ratio': 0.08695652173913043},\n {'date': datetime.date(2019, 9, 21),\n  'total_count': 4,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 9, 22),\n  'total_count': 12,\n  'count': 2,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 9, 23),\n  'total_count': 24,\n  'count': 1,\n  'ratio': 0.041666666666666664},\n {'date': datetime.date(2019, 9, 24),\n  'total_count': 29,\n  'count': 9,\n  'ratio': 0.3103448275862069},\n {'date': datetime.date(2019, 9, 25),\n  'total_count': 27,\n  'count': 8,\n  'ratio': 0.2962962962962963},\n {'date': datetime.date(2019, 9, 26),\n  'total_count': 19,\n  'count': 5,\n  'ratio': 0.2631578947368421},\n {'date': datetime.date(2019, 9, 27),\n  'total_count': 24,\n  'count': 5,\n  'ratio': 0.20833333333333334},\n {'date': datetime.date(2019, 9, 28),\n  'total_count': 9,\n  'count': 3,\n  'ratio': 0.3333333333333333},\n {'date': datetime.date(2019, 9, 29),\n  'total_count': 14,\n  'count': 4,\n  'ratio': 0.2857142857142857},\n {'date': datetime.date(2019, 9, 30),\n  'total_count': 36,\n  'count': 5,\n  'ratio': 0.1388888888888889},\n {'date': datetime.date(2019, 10, 1),\n  'total_count': 33,\n  'count': 5,\n  'ratio': 0.15151515151515152},\n {'date': datetime.date(2019, 10, 2),\n  'total_count': 24,\n  'count': 2,\n  'ratio': 0.08333333333333333},\n {'date': datetime.date(2019, 10, 3),\n  'total_count': 29,\n  'count': 9,\n  'ratio': 0.3103448275862069},\n {'date': datetime.date(2019, 10, 4),\n  'total_count': 23,\n  'count': 1,\n  'ratio': 0.043478260869565216},\n {'date': datetime.date(2019, 10, 5),\n  'total_count': 7,\n  'count': 1,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2019, 10, 6),\n  'total_count': 8,\n  'count': 2,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 10, 7),\n  'total_count': 27,\n  'count': 6,\n  'ratio': 0.2222222222222222},\n {'date': datetime.date(2019, 10, 8),\n  'total_count': 23,\n  'count': 6,\n  'ratio': 0.2608695652173913},\n {'date': datetime.date(2019, 10, 9),\n  'total_count': 17,\n  'count': 4,\n  'ratio': 0.23529411764705882},\n {'date': datetime.date(2019, 10, 10),\n  'total_count': 24,\n  'count': 4,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 10, 11),\n  'total_count': 27,\n  'count': 5,\n  'ratio': 0.18518518518518517},\n {'date': datetime.date(2019, 10, 12),\n  'total_count': 11,\n  'count': 2,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2019, 10, 13),\n  'total_count': 7,\n  'count': 2,\n  'ratio': 0.2857142857142857},\n {'date': datetime.date(2019, 10, 14),\n  'total_count': 28,\n  'count': 5,\n  'ratio': 0.17857142857142858},\n {'date': datetime.date(2019, 10, 15),\n  'total_count': 39,\n  'count': 9,\n  'ratio': 0.23076923076923078},\n {'date': datetime.date(2019, 10, 16),\n  'total_count': 29,\n  'count': 8,\n  'ratio': 0.27586206896551724},\n {'date': datetime.date(2019, 10, 17),\n  'total_count': 24,\n  'count': 6,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 10, 18),\n  'total_count': 24,\n  'count': 4,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 10, 19),\n  'total_count': 4,\n  'count': 1,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 10, 20),\n  'total_count': 10,\n  'count': 3,\n  'ratio': 0.3},\n {'date': datetime.date(2019, 10, 21),\n  'total_count': 23,\n  'count': 4,\n  'ratio': 0.17391304347826086},\n {'date': datetime.date(2019, 10, 22),\n  'total_count': 25,\n  'count': 5,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 10, 23),\n  'total_count': 30,\n  'count': 7,\n  'ratio': 0.23333333333333334},\n {'date': datetime.date(2019, 10, 24),\n  'total_count': 23,\n  'count': 5,\n  'ratio': 0.21739130434782608},\n {'date': datetime.date(2019, 10, 25),\n  'total_count': 19,\n  'count': 2,\n  'ratio': 0.10526315789473684},\n {'date': datetime.date(2019, 10, 26),\n  'total_count': 4,\n  'count': 2,\n  'ratio': 0.5},\n {'date': datetime.date(2019, 10, 27),\n  'total_count': 16,\n  'count': 2,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 10, 28),\n  'total_count': 27,\n  'count': 3,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2019, 10, 29),\n  'total_count': 28,\n  'count': 2,\n  'ratio': 0.07142857142857142},\n {'date': datetime.date(2019, 10, 30),\n  'total_count': 24,\n  'count': 4,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 10, 31),\n  'total_count': 26,\n  'count': 5,\n  'ratio': 0.19230769230769232},\n {'date': datetime.date(2019, 11, 1),\n  'total_count': 28,\n  'count': 11,\n  'ratio': 0.39285714285714285},\n {'date': datetime.date(2019, 11, 2),\n  'total_count': 5,\n  'count': 2,\n  'ratio': 0.4},\n {'date': datetime.date(2019, 11, 3),\n  'total_count': 14,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 11, 4),\n  'total_count': 19,\n  'count': 1,\n  'ratio': 0.05263157894736842},\n {'date': datetime.date(2019, 11, 5),\n  'total_count': 20,\n  'count': 4,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 11, 6),\n  'total_count': 28,\n  'count': 6,\n  'ratio': 0.21428571428571427},\n {'date': datetime.date(2019, 11, 7),\n  'total_count': 24,\n  'count': 6,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 11, 8),\n  'total_count': 28,\n  'count': 6,\n  'ratio': 0.21428571428571427},\n {'date': datetime.date(2019, 11, 9),\n  'total_count': 8,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 11, 10),\n  'total_count': 13,\n  'count': 3,\n  'ratio': 0.23076923076923078},\n {'date': datetime.date(2019, 11, 11),\n  'total_count': 25,\n  'count': 10,\n  'ratio': 0.4},\n {'date': datetime.date(2019, 11, 12),\n  'total_count': 45,\n  'count': 10,\n  'ratio': 0.2222222222222222},\n {'date': datetime.date(2019, 11, 13),\n  'total_count': 25,\n  'count': 10,\n  'ratio': 0.4},\n {'date': datetime.date(2019, 11, 14),\n  'total_count': 32,\n  'count': 2,\n  'ratio': 0.0625},\n {'date': datetime.date(2019, 11, 15),\n  'total_count': 21,\n  'count': 2,\n  'ratio': 0.09523809523809523},\n {'date': datetime.date(2019, 11, 16),\n  'total_count': 16,\n  'count': 3,\n  'ratio': 0.1875},\n {'date': datetime.date(2019, 11, 17),\n  'total_count': 10,\n  'count': 2,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 11, 18),\n  'total_count': 28,\n  'count': 7,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 11, 19),\n  'total_count': 32,\n  'count': 7,\n  'ratio': 0.21875},\n {'date': datetime.date(2019, 11, 20),\n  'total_count': 26,\n  'count': 5,\n  'ratio': 0.19230769230769232},\n {'date': datetime.date(2019, 11, 21),\n  'total_count': 25,\n  'count': 6,\n  'ratio': 0.24},\n {'date': datetime.date(2019, 11, 22),\n  'total_count': 29,\n  'count': 3,\n  'ratio': 0.10344827586206896},\n {'date': datetime.date(2019, 11, 23),\n  'total_count': 11,\n  'count': 5,\n  'ratio': 0.45454545454545453},\n {'date': datetime.date(2019, 11, 24),\n  'total_count': 12,\n  'count': 3,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 11, 25),\n  'total_count': 16,\n  'count': 5,\n  'ratio': 0.3125},\n {'date': datetime.date(2019, 11, 26),\n  'total_count': 21,\n  'count': 6,\n  'ratio': 0.2857142857142857},\n {'date': datetime.date(2019, 11, 27),\n  'total_count': 19,\n  'count': 4,\n  'ratio': 0.21052631578947367},\n {'date': datetime.date(2019, 11, 28),\n  'total_count': 8,\n  'count': 1,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 11, 29),\n  'total_count': 8,\n  'count': 1,\n  'ratio': 0.125},\n {'date': datetime.date(2019, 11, 30),\n  'total_count': 4,\n  'count': 1,\n  'ratio': 0.25},\n {'date': datetime.date(2019, 12, 1),\n  'total_count': 6,\n  'count': 3,\n  'ratio': 0.5},\n {'date': datetime.date(2019, 12, 2),\n  'total_count': 23,\n  'count': 1,\n  'ratio': 0.043478260869565216},\n {'date': datetime.date(2019, 12, 3),\n  'total_count': 38,\n  'count': 5,\n  'ratio': 0.13157894736842105},\n {'date': datetime.date(2019, 12, 4),\n  'total_count': 35,\n  'count': 4,\n  'ratio': 0.11428571428571428},\n {'date': datetime.date(2019, 12, 5),\n  'total_count': 31,\n  'count': 8,\n  'ratio': 0.25806451612903225},\n {'date': datetime.date(2019, 12, 6),\n  'total_count': 25,\n  'count': 6,\n  'ratio': 0.24},\n {'date': datetime.date(2019, 12, 7),\n  'total_count': 9,\n  'count': 1,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2019, 12, 8),\n  'total_count': 12,\n  'count': 4,\n  'ratio': 0.3333333333333333},\n {'date': datetime.date(2019, 12, 9),\n  'total_count': 50,\n  'count': 4,\n  'ratio': 0.08},\n {'date': datetime.date(2019, 12, 10),\n  'total_count': 34,\n  'count': 2,\n  'ratio': 0.058823529411764705},\n {'date': datetime.date(2019, 12, 11),\n  'total_count': 35,\n  'count': 6,\n  'ratio': 0.17142857142857143},\n {'date': datetime.date(2019, 12, 12),\n  'total_count': 32,\n  'count': 12,\n  'ratio': 0.375},\n {'date': datetime.date(2019, 12, 13),\n  'total_count': 21,\n  'count': 3,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2019, 12, 14),\n  'total_count': 6,\n  'count': 2,\n  'ratio': 0.3333333333333333},\n {'date': datetime.date(2019, 12, 15),\n  'total_count': 6,\n  'count': 1,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 12, 16),\n  'total_count': 20,\n  'count': 3,\n  'ratio': 0.15},\n {'date': datetime.date(2019, 12, 17),\n  'total_count': 30,\n  'count': 5,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 12, 18),\n  'total_count': 30,\n  'count': 5,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2019, 12, 19),\n  'total_count': 33,\n  'count': 8,\n  'ratio': 0.24242424242424243},\n {'date': datetime.date(2019, 12, 20),\n  'total_count': 22,\n  'count': 5,\n  'ratio': 0.22727272727272727},\n {'date': datetime.date(2019, 12, 21),\n  'total_count': 5,\n  'count': 1,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 12, 22),\n  'total_count': 6,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 12, 23),\n  'total_count': 17,\n  'count': 4,\n  'ratio': 0.23529411764705882},\n {'date': datetime.date(2019, 12, 24),\n  'total_count': 10,\n  'count': 2,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 12, 25),\n  'total_count': 5,\n  'count': 1,\n  'ratio': 0.2},\n {'date': datetime.date(2019, 12, 26),\n  'total_count': 9,\n  'count': 1,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2019, 12, 27),\n  'total_count': 9,\n  'count': 1,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2019, 12, 28),\n  'total_count': 5,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 12, 29),\n  'total_count': 5,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2019, 12, 30),\n  'total_count': 17,\n  'count': 1,\n  'ratio': 0.058823529411764705},\n {'date': datetime.date(2019, 12, 31),\n  'total_count': 15,\n  'count': 1,\n  'ratio': 0.06666666666666667},\n {'date': datetime.date(2020, 1, 1),\n  'total_count': 15,\n  'count': 3,\n  'ratio': 0.2},\n {'date': datetime.date(2020, 1, 2),\n  'total_count': 26,\n  'count': 3,\n  'ratio': 0.11538461538461539},\n {'date': datetime.date(2020, 1, 3),\n  'total_count': 43,\n  'count': 1,\n  'ratio': 0.023255813953488372},\n {'date': datetime.date(2020, 1, 4), 'total_count': 5, 'count': 0, 'ratio': 0},\n {'date': datetime.date(2020, 1, 5),\n  'total_count': 12,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2020, 1, 6),\n  'total_count': 29,\n  'count': 4,\n  'ratio': 0.13793103448275862},\n {'date': datetime.date(2020, 1, 7),\n  'total_count': 33,\n  'count': 6,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2020, 1, 8),\n  'total_count': 45,\n  'count': 3,\n  'ratio': 0.06666666666666667},\n {'date': datetime.date(2020, 1, 9),\n  'total_count': 38,\n  'count': 10,\n  'ratio': 0.2631578947368421},\n {'date': datetime.date(2020, 1, 10),\n  'total_count': 33,\n  'count': 4,\n  'ratio': 0.12121212121212122},\n {'date': datetime.date(2020, 1, 11),\n  'total_count': 11,\n  'count': 2,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2020, 1, 12),\n  'total_count': 10,\n  'count': 2,\n  'ratio': 0.2},\n {'date': datetime.date(2020, 1, 13),\n  'total_count': 35,\n  'count': 8,\n  'ratio': 0.22857142857142856},\n {'date': datetime.date(2020, 1, 14),\n  'total_count': 40,\n  'count': 8,\n  'ratio': 0.2},\n {'date': datetime.date(2020, 1, 15),\n  'total_count': 40,\n  'count': 11,\n  'ratio': 0.275},\n {'date': datetime.date(2020, 1, 16),\n  'total_count': 43,\n  'count': 13,\n  'ratio': 0.3023255813953488},\n {'date': datetime.date(2020, 1, 17),\n  'total_count': 24,\n  'count': 4,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2020, 1, 18),\n  'total_count': 7,\n  'count': 1,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2020, 1, 19),\n  'total_count': 15,\n  'count': 3,\n  'ratio': 0.2},\n {'date': datetime.date(2020, 1, 20),\n  'total_count': 21,\n  'count': 4,\n  'ratio': 0.19047619047619047},\n {'date': datetime.date(2020, 1, 21),\n  'total_count': 33,\n  'count': 4,\n  'ratio': 0.12121212121212122},\n {'date': datetime.date(2020, 1, 22),\n  'total_count': 117,\n  'count': 19,\n  'ratio': 0.1623931623931624},\n {'date': datetime.date(2020, 1, 23),\n  'total_count': 38,\n  'count': 9,\n  'ratio': 0.23684210526315788},\n {'date': datetime.date(2020, 1, 24),\n  'total_count': 57,\n  'count': 11,\n  'ratio': 0.19298245614035087},\n {'date': datetime.date(2020, 1, 25),\n  'total_count': 12,\n  'count': 1,\n  'ratio': 0.08333333333333333},\n {'date': datetime.date(2020, 1, 26),\n  'total_count': 18,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2020, 1, 27),\n  'total_count': 35,\n  'count': 4,\n  'ratio': 0.11428571428571428},\n {'date': datetime.date(2020, 1, 28),\n  'total_count': 78,\n  'count': 11,\n  'ratio': 0.14102564102564102},\n {'date': datetime.date(2020, 1, 29),\n  'total_count': 53,\n  'count': 9,\n  'ratio': 0.16981132075471697},\n {'date': datetime.date(2020, 1, 30),\n  'total_count': 160,\n  'count': 14,\n  'ratio': 0.0875},\n {'date': datetime.date(2020, 1, 31),\n  'total_count': 169,\n  'count': 20,\n  'ratio': 0.11834319526627218},\n {'date': datetime.date(2020, 2, 1),\n  'total_count': 47,\n  'count': 8,\n  'ratio': 0.1702127659574468},\n {'date': datetime.date(2020, 2, 2),\n  'total_count': 65,\n  'count': 4,\n  'ratio': 0.06153846153846154},\n {'date': datetime.date(2020, 2, 3),\n  'total_count': 183,\n  'count': 19,\n  'ratio': 0.10382513661202186},\n {'date': datetime.date(2020, 2, 4),\n  'total_count': 194,\n  'count': 28,\n  'ratio': 0.14432989690721648},\n {'date': datetime.date(2020, 2, 5),\n  'total_count': 158,\n  'count': 19,\n  'ratio': 0.12025316455696203},\n {'date': datetime.date(2020, 2, 6),\n  'total_count': 136,\n  'count': 23,\n  'ratio': 0.16911764705882354},\n {'date': datetime.date(2020, 2, 7),\n  'total_count': 72,\n  'count': 10,\n  'ratio': 0.1388888888888889},\n {'date': datetime.date(2020, 2, 8),\n  'total_count': 15,\n  'count': 1,\n  'ratio': 0.06666666666666667},\n {'date': datetime.date(2020, 2, 9),\n  'total_count': 22,\n  'count': 5,\n  'ratio': 0.22727272727272727},\n {'date': datetime.date(2020, 2, 10),\n  'total_count': 48,\n  'count': 11,\n  'ratio': 0.22916666666666666},\n {'date': datetime.date(2020, 2, 11),\n  'total_count': 59,\n  'count': 10,\n  'ratio': 0.1694915254237288},\n {'date': datetime.date(2020, 2, 12),\n  'total_count': 49,\n  'count': 10,\n  'ratio': 0.20408163265306123},\n {'date': datetime.date(2020, 2, 13),\n  'total_count': 49,\n  'count': 7,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2020, 2, 14),\n  'total_count': 50,\n  'count': 4,\n  'ratio': 0.08},\n {'date': datetime.date(2020, 2, 15),\n  'total_count': 20,\n  'count': 6,\n  'ratio': 0.3},\n {'date': datetime.date(2020, 2, 16),\n  'total_count': 14,\n  'count': 0,\n  'ratio': 0},\n {'date': datetime.date(2020, 2, 17),\n  'total_count': 13,\n  'count': 2,\n  'ratio': 0.15384615384615385},\n {'date': datetime.date(2020, 2, 18),\n  'total_count': 49,\n  'count': 10,\n  'ratio': 0.20408163265306123},\n {'date': datetime.date(2020, 2, 19),\n  'total_count': 43,\n  'count': 9,\n  'ratio': 0.20930232558139536},\n {'date': datetime.date(2020, 2, 20),\n  'total_count': 35,\n  'count': 6,\n  'ratio': 0.17142857142857143},\n {'date': datetime.date(2020, 2, 21),\n  'total_count': 24,\n  'count': 1,\n  'ratio': 0.041666666666666664},\n {'date': datetime.date(2020, 2, 22),\n  'total_count': 18,\n  'count': 2,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2020, 2, 23),\n  'total_count': 15,\n  'count': 2,\n  'ratio': 0.13333333333333333},\n {'date': datetime.date(2020, 2, 24),\n  'total_count': 45,\n  'count': 8,\n  'ratio': 0.17777777777777778},\n {'date': datetime.date(2020, 2, 25),\n  'total_count': 46,\n  'count': 7,\n  'ratio': 0.15217391304347827},\n {'date': datetime.date(2020, 2, 26),\n  'total_count': 44,\n  'count': 12,\n  'ratio': 0.2727272727272727},\n {'date': datetime.date(2020, 2, 27),\n  'total_count': 51,\n  'count': 7,\n  'ratio': 0.13725490196078433},\n {'date': datetime.date(2020, 2, 28),\n  'total_count': 61,\n  'count': 10,\n  'ratio': 0.16393442622950818},\n {'date': datetime.date(2020, 2, 29),\n  'total_count': 46,\n  'count': 5,\n  'ratio': 0.10869565217391304},\n {'date': datetime.date(2020, 3, 1),\n  'total_count': 53,\n  'count': 4,\n  'ratio': 0.07547169811320754},\n {'date': datetime.date(2020, 3, 2),\n  'total_count': 111,\n  'count': 18,\n  'ratio': 0.16216216216216217},\n {'date': datetime.date(2020, 3, 3),\n  'total_count': 201,\n  'count': 22,\n  'ratio': 0.10945273631840796},\n {'date': datetime.date(2020, 3, 4),\n  'total_count': 130,\n  'count': 18,\n  'ratio': 0.13846153846153847},\n {'date': datetime.date(2020, 3, 5),\n  'total_count': 210,\n  'count': 35,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2020, 3, 6),\n  'total_count': 580,\n  'count': 59,\n  'ratio': 0.10172413793103448},\n {'date': datetime.date(2020, 3, 7),\n  'total_count': 274,\n  'count': 21,\n  'ratio': 0.07664233576642336},\n {'date': datetime.date(2020, 3, 8),\n  'total_count': 284,\n  'count': 31,\n  'ratio': 0.10915492957746478},\n {'date': datetime.date(2020, 3, 9),\n  'total_count': 543,\n  'count': 57,\n  'ratio': 0.10497237569060773},\n {'date': datetime.date(2020, 3, 10),\n  'total_count': 651,\n  'count': 59,\n  'ratio': 0.09062980030721966},\n {'date': datetime.date(2020, 3, 11),\n  'total_count': 655,\n  'count': 70,\n  'ratio': 0.10687022900763359},\n {'date': datetime.date(2020, 3, 12),\n  'total_count': 746,\n  'count': 69,\n  'ratio': 0.09249329758713137},\n {'date': datetime.date(2020, 3, 13),\n  'total_count': 703,\n  'count': 58,\n  'ratio': 0.08250355618776671},\n {'date': datetime.date(2020, 3, 14),\n  'total_count': 324,\n  'count': 29,\n  'ratio': 0.08950617283950617},\n {'date': datetime.date(2020, 3, 15),\n  'total_count': 368,\n  'count': 37,\n  'ratio': 0.10054347826086957},\n {'date': datetime.date(2020, 3, 16),\n  'total_count': 596,\n  'count': 53,\n  'ratio': 0.08892617449664429},\n {'date': datetime.date(2020, 3, 17),\n  'total_count': 772,\n  'count': 51,\n  'ratio': 0.06606217616580311},\n {'date': datetime.date(2020, 3, 18),\n  'total_count': 718,\n  'count': 59,\n  'ratio': 0.08217270194986072},\n {'date': datetime.date(2020, 3, 19),\n  'total_count': 715,\n  'count': 48,\n  'ratio': 0.06713286713286713},\n {'date': datetime.date(2020, 3, 20),\n  'total_count': 707,\n  'count': 59,\n  'ratio': 0.08345120226308345},\n {'date': datetime.date(2020, 3, 21),\n  'total_count': 269,\n  'count': 18,\n  'ratio': 0.06691449814126393},\n {'date': datetime.date(2020, 3, 22),\n  'total_count': 288,\n  'count': 12,\n  'ratio': 0.041666666666666664},\n {'date': datetime.date(2020, 3, 23),\n  'total_count': 599,\n  'count': 41,\n  'ratio': 0.06844741235392321},\n {'date': datetime.date(2020, 3, 24),\n  'total_count': 720,\n  'count': 38,\n  'ratio': 0.05277777777777778},\n {'date': datetime.date(2020, 3, 25),\n  'total_count': 664,\n  'count': 45,\n  'ratio': 0.0677710843373494},\n {'date': datetime.date(2020, 3, 26),\n  'total_count': 722,\n  'count': 41,\n  'ratio': 0.05678670360110803},\n {'date': datetime.date(2020, 3, 27),\n  'total_count': 959,\n  'count': 31,\n  'ratio': 0.03232533889468196},\n {'date': datetime.date(2020, 3, 28),\n  'total_count': 254,\n  'count': 20,\n  'ratio': 0.07874015748031496},\n {'date': datetime.date(2020, 3, 29),\n  'total_count': 241,\n  'count': 23,\n  'ratio': 0.0954356846473029},\n {'date': datetime.date(2020, 3, 30),\n  'total_count': 581,\n  'count': 27,\n  'ratio': 0.04647160068846816},\n {'date': datetime.date(2020, 3, 31),\n  'total_count': 606,\n  'count': 42,\n  'ratio': 0.06930693069306931},\n {'date': datetime.date(2020, 4, 1),\n  'total_count': 702,\n  'count': 23,\n  'ratio': 0.03276353276353276},\n {'date': datetime.date(2020, 4, 2),\n  'total_count': 640,\n  'count': 49,\n  'ratio': 0.0765625},\n {'date': datetime.date(2020, 4, 3),\n  'total_count': 629,\n  'count': 52,\n  'ratio': 0.08267090620031796},\n {'date': datetime.date(2020, 4, 4),\n  'total_count': 262,\n  'count': 16,\n  'ratio': 0.061068702290076333},\n {'date': datetime.date(2020, 4, 5),\n  'total_count': 285,\n  'count': 23,\n  'ratio': 0.08070175438596491},\n {'date': datetime.date(2020, 4, 6),\n  'total_count': 573,\n  'count': 37,\n  'ratio': 0.06457242582897033},\n {'date': datetime.date(2020, 4, 7),\n  'total_count': 622,\n  'count': 53,\n  'ratio': 0.08520900321543408},\n {'date': datetime.date(2020, 4, 8),\n  'total_count': 670,\n  'count': 55,\n  'ratio': 0.08208955223880597},\n {'date': datetime.date(2020, 4, 9),\n  'total_count': 594,\n  'count': 42,\n  'ratio': 0.0707070707070707},\n {'date': datetime.date(2020, 4, 10),\n  'total_count': 587,\n  'count': 34,\n  'ratio': 0.05792163543441227},\n {'date': datetime.date(2020, 4, 11),\n  'total_count': 247,\n  'count': 16,\n  'ratio': 0.06477732793522267},\n {'date': datetime.date(2020, 4, 12),\n  'total_count': 255,\n  'count': 12,\n  'ratio': 0.047058823529411764},\n {'date': datetime.date(2020, 4, 13),\n  'total_count': 554,\n  'count': 45,\n  'ratio': 0.0812274368231047},\n {'date': datetime.date(2020, 4, 14),\n  'total_count': 613,\n  'count': 52,\n  'ratio': 0.08482871125611746},\n {'date': datetime.date(2020, 4, 15),\n  'total_count': 670,\n  'count': 58,\n  'ratio': 0.08656716417910448},\n {'date': datetime.date(2020, 4, 16),\n  'total_count': 694,\n  'count': 57,\n  'ratio': 0.08213256484149856},\n {'date': datetime.date(2020, 4, 17),\n  'total_count': 688,\n  'count': 36,\n  'ratio': 0.05232558139534884},\n {'date': datetime.date(2020, 4, 18),\n  'total_count': 296,\n  'count': 14,\n  'ratio': 0.0472972972972973},\n {'date': datetime.date(2020, 4, 19),\n  'total_count': 277,\n  'count': 27,\n  'ratio': 0.09747292418772563},\n {'date': datetime.date(2020, 4, 20),\n  'total_count': 521,\n  'count': 36,\n  'ratio': 0.0690978886756238},\n {'date': datetime.date(2020, 4, 21),\n  'total_count': 621,\n  'count': 51,\n  'ratio': 0.0821256038647343},\n {'date': datetime.date(2020, 4, 22),\n  'total_count': 635,\n  'count': 43,\n  'ratio': 0.06771653543307087},\n {'date': datetime.date(2020, 4, 23),\n  'total_count': 571,\n  'count': 56,\n  'ratio': 0.09807355516637478},\n {'date': datetime.date(2020, 4, 24),\n  'total_count': 635,\n  'count': 56,\n  'ratio': 0.08818897637795275},\n {'date': datetime.date(2020, 4, 25),\n  'total_count': 243,\n  'count': 22,\n  'ratio': 0.09053497942386832},\n {'date': datetime.date(2020, 4, 26),\n  'total_count': 255,\n  'count': 13,\n  'ratio': 0.050980392156862744},\n {'date': datetime.date(2020, 4, 27),\n  'total_count': 515,\n  'count': 35,\n  'ratio': 0.06796116504854369},\n {'date': datetime.date(2020, 4, 28),\n  'total_count': 587,\n  'count': 49,\n  'ratio': 0.08347529812606473},\n {'date': datetime.date(2020, 4, 29),\n  'total_count': 578,\n  'count': 33,\n  'ratio': 0.05709342560553633},\n {'date': datetime.date(2020, 4, 30),\n  'total_count': 669,\n  'count': 60,\n  'ratio': 0.08968609865470852},\n {'date': datetime.date(2020, 5, 1),\n  'total_count': 603,\n  'count': 41,\n  'ratio': 0.06799336650082918},\n {'date': datetime.date(2020, 5, 2),\n  'total_count': 255,\n  'count': 24,\n  'ratio': 0.09411764705882353},\n {'date': datetime.date(2020, 5, 3),\n  'total_count': 241,\n  'count': 18,\n  'ratio': 0.07468879668049792},\n {'date': datetime.date(2020, 5, 4),\n  'total_count': 515,\n  'count': 36,\n  'ratio': 0.06990291262135923},\n {'date': datetime.date(2020, 5, 5),\n  'total_count': 568,\n  'count': 51,\n  'ratio': 0.0897887323943662},\n {'date': datetime.date(2020, 5, 6),\n  'total_count': 556,\n  'count': 43,\n  'ratio': 0.07733812949640288},\n {'date': datetime.date(2020, 5, 7),\n  'total_count': 667,\n  'count': 50,\n  'ratio': 0.07496251874062969},\n {'date': datetime.date(2020, 5, 8),\n  'total_count': 614,\n  'count': 46,\n  'ratio': 0.0749185667752443},\n {'date': datetime.date(2020, 5, 9),\n  'total_count': 227,\n  'count': 21,\n  'ratio': 0.09251101321585903},\n {'date': datetime.date(2020, 5, 10),\n  'total_count': 230,\n  'count': 19,\n  'ratio': 0.08260869565217391},\n {'date': datetime.date(2020, 5, 11),\n  'total_count': 494,\n  'count': 51,\n  'ratio': 0.10323886639676114},\n {'date': datetime.date(2020, 5, 12),\n  'total_count': 575,\n  'count': 43,\n  'ratio': 0.07478260869565218},\n {'date': datetime.date(2020, 5, 13),\n  'total_count': 558,\n  'count': 30,\n  'ratio': 0.053763440860215055},\n {'date': datetime.date(2020, 5, 14),\n  'total_count': 570,\n  'count': 64,\n  'ratio': 0.11228070175438597},\n {'date': datetime.date(2020, 5, 15),\n  'total_count': 578,\n  'count': 38,\n  'ratio': 0.0657439446366782},\n {'date': datetime.date(2020, 5, 16),\n  'total_count': 223,\n  'count': 12,\n  'ratio': 0.053811659192825115},\n {'date': datetime.date(2020, 5, 17),\n  'total_count': 239,\n  'count': 15,\n  'ratio': 0.06276150627615062},\n {'date': datetime.date(2020, 5, 18),\n  'total_count': 559,\n  'count': 47,\n  'ratio': 0.08407871198568873},\n {'date': datetime.date(2020, 5, 19),\n  'total_count': 550,\n  'count': 35,\n  'ratio': 0.06363636363636363},\n {'date': datetime.date(2020, 5, 20),\n  'total_count': 575,\n  'count': 48,\n  'ratio': 0.08347826086956522},\n {'date': datetime.date(2020, 5, 21),\n  'total_count': 578,\n  'count': 52,\n  'ratio': 0.08996539792387544},\n {'date': datetime.date(2020, 5, 22),\n  'total_count': 573,\n  'count': 37,\n  'ratio': 0.06457242582897033},\n {'date': datetime.date(2020, 5, 23),\n  'total_count': 228,\n  'count': 19,\n  'ratio': 0.08333333333333333},\n {'date': datetime.date(2020, 5, 24),\n  'total_count': 213,\n  'count': 18,\n  'ratio': 0.08450704225352113},\n {'date': datetime.date(2020, 5, 25),\n  'total_count': 284,\n  'count': 21,\n  'ratio': 0.07394366197183098},\n {'date': datetime.date(2020, 5, 26),\n  'total_count': 548,\n  'count': 48,\n  'ratio': 0.08759124087591241},\n {'date': datetime.date(2020, 5, 27),\n  'total_count': 615,\n  'count': 54,\n  'ratio': 0.08780487804878048},\n {'date': datetime.date(2020, 5, 28),\n  'total_count': 592,\n  'count': 60,\n  'ratio': 0.10135135135135136},\n {'date': datetime.date(2020, 5, 29),\n  'total_count': 600,\n  'count': 41,\n  'ratio': 0.06833333333333333},\n {'date': datetime.date(2020, 5, 30),\n  'total_count': 256,\n  'count': 22,\n  'ratio': 0.0859375},\n {'date': datetime.date(2020, 5, 31),\n  'total_count': 309,\n  'count': 23,\n  'ratio': 0.0744336569579288},\n {'date': datetime.date(2020, 6, 1),\n  'total_count': 589,\n  'count': 58,\n  'ratio': 0.09847198641765705},\n {'date': datetime.date(2020, 6, 2),\n  'total_count': 674,\n  'count': 62,\n  'ratio': 0.09198813056379822},\n {'date': datetime.date(2020, 6, 3),\n  'total_count': 613,\n  'count': 59,\n  'ratio': 0.09624796084828711},\n {'date': datetime.date(2020, 6, 4),\n  'total_count': 646,\n  'count': 60,\n  'ratio': 0.09287925696594428},\n {'date': datetime.date(2020, 6, 5),\n  'total_count': 576,\n  'count': 50,\n  'ratio': 0.08680555555555555},\n {'date': datetime.date(2020, 6, 6),\n  'total_count': 249,\n  'count': 33,\n  'ratio': 0.13253012048192772},\n {'date': datetime.date(2020, 6, 7),\n  'total_count': 258,\n  'count': 20,\n  'ratio': 0.07751937984496124},\n {'date': datetime.date(2020, 6, 8),\n  'total_count': 489,\n  'count': 55,\n  'ratio': 0.11247443762781185},\n {'date': datetime.date(2020, 6, 9),\n  'total_count': 568,\n  'count': 55,\n  'ratio': 0.09683098591549295},\n {'date': datetime.date(2020, 6, 10),\n  'total_count': 582,\n  'count': 44,\n  'ratio': 0.07560137457044673},\n {'date': datetime.date(2020, 6, 11),\n  'total_count': 606,\n  'count': 67,\n  'ratio': 0.11056105610561057},\n {'date': datetime.date(2020, 6, 12),\n  'total_count': 587,\n  'count': 59,\n  'ratio': 0.10051107325383304},\n {'date': datetime.date(2020, 6, 13),\n  'total_count': 223,\n  'count': 30,\n  'ratio': 0.13452914798206278},\n {'date': datetime.date(2020, 6, 14),\n  'total_count': 213,\n  'count': 12,\n  'ratio': 0.056338028169014086},\n {'date': datetime.date(2020, 6, 15),\n  'total_count': 521,\n  'count': 46,\n  'ratio': 0.08829174664107485},\n {'date': datetime.date(2020, 6, 16),\n  'total_count': 559,\n  'count': 52,\n  'ratio': 0.09302325581395349},\n {'date': datetime.date(2020, 6, 17),\n  'total_count': 594,\n  'count': 45,\n  'ratio': 0.07575757575757576},\n {'date': datetime.date(2020, 6, 18),\n  'total_count': 678,\n  'count': 60,\n  'ratio': 0.08849557522123894},\n {'date': datetime.date(2020, 6, 19),\n  'total_count': 525,\n  'count': 30,\n  'ratio': 0.05714285714285714},\n {'date': datetime.date(2020, 6, 20),\n  'total_count': 258,\n  'count': 24,\n  'ratio': 0.09302325581395349},\n {'date': datetime.date(2020, 6, 21),\n  'total_count': 228,\n  'count': 26,\n  'ratio': 0.11403508771929824},\n {'date': datetime.date(2020, 6, 22),\n  'total_count': 498,\n  'count': 50,\n  'ratio': 0.10040160642570281},\n {'date': datetime.date(2020, 6, 23),\n  'total_count': 579,\n  'count': 51,\n  'ratio': 0.08808290155440414},\n {'date': datetime.date(2020, 6, 24),\n  'total_count': 552,\n  'count': 48,\n  'ratio': 0.08695652173913043},\n {'date': datetime.date(2020, 6, 25),\n  'total_count': 588,\n  'count': 52,\n  'ratio': 0.08843537414965986},\n {'date': datetime.date(2020, 6, 26),\n  'total_count': 568,\n  'count': 47,\n  'ratio': 0.08274647887323944},\n {'date': datetime.date(2020, 6, 27),\n  'total_count': 224,\n  'count': 14,\n  'ratio': 0.0625},\n {'date': datetime.date(2020, 6, 28),\n  'total_count': 231,\n  'count': 19,\n  'ratio': 0.08225108225108226},\n {'date': datetime.date(2020, 6, 29),\n  'total_count': 520,\n  'count': 59,\n  'ratio': 0.11346153846153846},\n {'date': datetime.date(2020, 6, 30),\n  'total_count': 597,\n  'count': 56,\n  'ratio': 0.09380234505862646},\n {'date': datetime.date(2020, 7, 1),\n  'total_count': 575,\n  'count': 46,\n  'ratio': 0.08},\n {'date': datetime.date(2020, 7, 2),\n  'total_count': 599,\n  'count': 47,\n  'ratio': 0.07846410684474124},\n {'date': datetime.date(2020, 7, 3),\n  'total_count': 394,\n  'count': 32,\n  'ratio': 0.08121827411167512},\n {'date': datetime.date(2020, 7, 4),\n  'total_count': 192,\n  'count': 9,\n  'ratio': 0.046875},\n {'date': datetime.date(2020, 7, 5),\n  'total_count': 201,\n  'count': 14,\n  'ratio': 0.06965174129353234},\n {'date': datetime.date(2020, 7, 6),\n  'total_count': 490,\n  'count': 34,\n  'ratio': 0.06938775510204082},\n {'date': datetime.date(2020, 7, 7),\n  'total_count': 577,\n  'count': 58,\n  'ratio': 0.10051993067590988},\n {'date': datetime.date(2020, 7, 8),\n  'total_count': 576,\n  'count': 40,\n  'ratio': 0.06944444444444445},\n {'date': datetime.date(2020, 7, 9),\n  'total_count': 611,\n  'count': 64,\n  'ratio': 0.10474631751227496},\n {'date': datetime.date(2020, 7, 10),\n  'total_count': 605,\n  'count': 61,\n  'ratio': 0.10082644628099173},\n {'date': datetime.date(2020, 7, 11),\n  'total_count': 226,\n  'count': 30,\n  'ratio': 0.13274336283185842},\n {'date': datetime.date(2020, 7, 12),\n  'total_count': 211,\n  'count': 16,\n  'ratio': 0.07582938388625593},\n {'date': datetime.date(2020, 7, 13),\n  'total_count': 487,\n  'count': 49,\n  'ratio': 0.10061601642710473},\n {'date': datetime.date(2020, 7, 14),\n  'total_count': 549,\n  'count': 53,\n  'ratio': 0.0965391621129326},\n {'date': datetime.date(2020, 7, 15),\n  'total_count': 492,\n  'count': 57,\n  'ratio': 0.11585365853658537},\n {'date': datetime.date(2020, 7, 16),\n  'total_count': 577,\n  'count': 63,\n  'ratio': 0.10918544194107452},\n {'date': datetime.date(2020, 7, 17),\n  'total_count': 511,\n  'count': 51,\n  'ratio': 0.09980430528375733},\n {'date': datetime.date(2020, 7, 18),\n  'total_count': 185,\n  'count': 25,\n  'ratio': 0.13513513513513514},\n {'date': datetime.date(2020, 7, 19),\n  'total_count': 196,\n  'count': 27,\n  'ratio': 0.1377551020408163},\n {'date': datetime.date(2020, 7, 20),\n  'total_count': 440,\n  'count': 38,\n  'ratio': 0.08636363636363636},\n {'date': datetime.date(2020, 7, 21),\n  'total_count': 551,\n  'count': 69,\n  'ratio': 0.12522686025408347},\n {'date': datetime.date(2020, 7, 22),\n  'total_count': 590,\n  'count': 57,\n  'ratio': 0.09661016949152543},\n {'date': datetime.date(2020, 7, 23),\n  'total_count': 589,\n  'count': 54,\n  'ratio': 0.09168081494057725},\n {'date': datetime.date(2020, 7, 24),\n  'total_count': 491,\n  'count': 54,\n  'ratio': 0.109979633401222},\n {'date': datetime.date(2020, 7, 25),\n  'total_count': 140,\n  'count': 20,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2020, 7, 26),\n  'total_count': 143,\n  'count': 18,\n  'ratio': 0.1258741258741259},\n {'date': datetime.date(2020, 7, 27),\n  'total_count': 340,\n  'count': 35,\n  'ratio': 0.10294117647058823},\n {'date': datetime.date(2020, 7, 28),\n  'total_count': 457,\n  'count': 50,\n  'ratio': 0.10940919037199125},\n {'date': datetime.date(2020, 7, 29),\n  'total_count': 513,\n  'count': 75,\n  'ratio': 0.14619883040935672},\n {'date': datetime.date(2020, 7, 30),\n  'total_count': 600,\n  'count': 63,\n  'ratio': 0.105},\n {'date': datetime.date(2020, 7, 31),\n  'total_count': 533,\n  'count': 77,\n  'ratio': 0.14446529080675422},\n {'date': datetime.date(2020, 8, 1),\n  'total_count': 165,\n  'count': 17,\n  'ratio': 0.10303030303030303},\n {'date': datetime.date(2020, 8, 2),\n  'total_count': 203,\n  'count': 28,\n  'ratio': 0.13793103448275862},\n {'date': datetime.date(2020, 8, 3),\n  'total_count': 444,\n  'count': 50,\n  'ratio': 0.11261261261261261},\n {'date': datetime.date(2020, 8, 4),\n  'total_count': 458,\n  'count': 55,\n  'ratio': 0.12008733624454149},\n {'date': datetime.date(2020, 8, 5),\n  'total_count': 384,\n  'count': 49,\n  'ratio': 0.12760416666666666},\n {'date': datetime.date(2020, 8, 6),\n  'total_count': 450,\n  'count': 49,\n  'ratio': 0.10888888888888888},\n {'date': datetime.date(2020, 8, 7),\n  'total_count': 493,\n  'count': 43,\n  'ratio': 0.0872210953346856},\n {'date': datetime.date(2020, 8, 8),\n  'total_count': 179,\n  'count': 31,\n  'ratio': 0.17318435754189945},\n {'date': datetime.date(2020, 8, 9),\n  'total_count': 214,\n  'count': 30,\n  'ratio': 0.14018691588785046},\n {'date': datetime.date(2020, 8, 10),\n  'total_count': 478,\n  'count': 39,\n  'ratio': 0.08158995815899582},\n {'date': datetime.date(2020, 8, 11),\n  'total_count': 539,\n  'count': 83,\n  'ratio': 0.15398886827458255},\n {'date': datetime.date(2020, 8, 12),\n  'total_count': 513,\n  'count': 90,\n  'ratio': 0.17543859649122806},\n {'date': datetime.date(2020, 8, 13),\n  'total_count': 605,\n  'count': 105,\n  'ratio': 0.17355371900826447},\n {'date': datetime.date(2020, 8, 14),\n  'total_count': 575,\n  'count': 76,\n  'ratio': 0.13217391304347825},\n {'date': datetime.date(2020, 8, 15),\n  'total_count': 184,\n  'count': 36,\n  'ratio': 0.1956521739130435},\n {'date': datetime.date(2020, 8, 16),\n  'total_count': 213,\n  'count': 29,\n  'ratio': 0.13615023474178403},\n {'date': datetime.date(2020, 8, 17),\n  'total_count': 575,\n  'count': 144,\n  'ratio': 0.25043478260869567},\n {'date': datetime.date(2020, 8, 18),\n  'total_count': 613,\n  'count': 144,\n  'ratio': 0.23491027732463296},\n {'date': datetime.date(2020, 8, 19),\n  'total_count': 630,\n  'count': 139,\n  'ratio': 0.22063492063492063},\n {'date': datetime.date(2020, 8, 20),\n  'total_count': 685,\n  'count': 167,\n  'ratio': 0.24379562043795622},\n {'date': datetime.date(2020, 8, 21),\n  'total_count': 580,\n  'count': 84,\n  'ratio': 0.14482758620689656},\n {'date': datetime.date(2020, 8, 22),\n  'total_count': 196,\n  'count': 47,\n  'ratio': 0.23979591836734693},\n {'date': datetime.date(2020, 8, 23),\n  'total_count': 249,\n  'count': 38,\n  'ratio': 0.15261044176706828},\n {'date': datetime.date(2020, 8, 24),\n  'total_count': 583,\n  'count': 96,\n  'ratio': 0.1646655231560892},\n {'date': datetime.date(2020, 8, 25),\n  'total_count': 614,\n  'count': 145,\n  'ratio': 0.23615635179153094},\n {'date': datetime.date(2020, 8, 26),\n  'total_count': 654,\n  'count': 131,\n  'ratio': 0.20030581039755352},\n {'date': datetime.date(2020, 8, 27),\n  'total_count': 667,\n  'count': 124,\n  'ratio': 0.18590704647676162},\n {'date': datetime.date(2020, 8, 28),\n  'total_count': 600,\n  'count': 102,\n  'ratio': 0.17},\n {'date': datetime.date(2020, 8, 29),\n  'total_count': 186,\n  'count': 28,\n  'ratio': 0.15053763440860216},\n {'date': datetime.date(2020, 8, 30),\n  'total_count': 207,\n  'count': 21,\n  'ratio': 0.10144927536231885},\n {'date': datetime.date(2020, 8, 31),\n  'total_count': 504,\n  'count': 89,\n  'ratio': 0.1765873015873016},\n {'date': datetime.date(2020, 9, 1),\n  'total_count': 608,\n  'count': 74,\n  'ratio': 0.12171052631578948},\n {'date': datetime.date(2020, 9, 2),\n  'total_count': 552,\n  'count': 72,\n  'ratio': 0.13043478260869565},\n {'date': datetime.date(2020, 9, 3),\n  'total_count': 541,\n  'count': 76,\n  'ratio': 0.14048059149722736},\n {'date': datetime.date(2020, 9, 4),\n  'total_count': 551,\n  'count': 82,\n  'ratio': 0.14882032667876588},\n {'date': datetime.date(2020, 9, 5),\n  'total_count': 192,\n  'count': 10,\n  'ratio': 0.052083333333333336},\n {'date': datetime.date(2020, 9, 6),\n  'total_count': 186,\n  'count': 22,\n  'ratio': 0.11827956989247312},\n {'date': datetime.date(2020, 9, 7),\n  'total_count': 249,\n  'count': 33,\n  'ratio': 0.13253012048192772},\n {'date': datetime.date(2020, 9, 8),\n  'total_count': 576,\n  'count': 87,\n  'ratio': 0.15104166666666666},\n {'date': datetime.date(2020, 9, 9),\n  'total_count': 585,\n  'count': 55,\n  'ratio': 0.09401709401709402},\n {'date': datetime.date(2020, 9, 10),\n  'total_count': 624,\n  'count': 106,\n  'ratio': 0.16987179487179488},\n {'date': datetime.date(2020, 9, 11),\n  'total_count': 541,\n  'count': 41,\n  'ratio': 0.07578558225508318},\n {'date': datetime.date(2020, 9, 12),\n  'total_count': 195,\n  'count': 29,\n  'ratio': 0.14871794871794872},\n {'date': datetime.date(2020, 9, 13),\n  'total_count': 202,\n  'count': 29,\n  'ratio': 0.14356435643564355},\n {'date': datetime.date(2020, 9, 14),\n  'total_count': 498,\n  'count': 95,\n  'ratio': 0.19076305220883535},\n {'date': datetime.date(2020, 9, 15),\n  'total_count': 626,\n  'count': 99,\n  'ratio': 0.15814696485623003},\n {'date': datetime.date(2020, 9, 16),\n  'total_count': 558,\n  'count': 84,\n  'ratio': 0.15053763440860216},\n {'date': datetime.date(2020, 9, 17),\n  'total_count': 641,\n  'count': 113,\n  'ratio': 0.17628705148205928},\n {'date': datetime.date(2020, 9, 18),\n  'total_count': 616,\n  'count': 128,\n  'ratio': 0.2077922077922078},\n {'date': datetime.date(2020, 9, 19),\n  'total_count': 287,\n  'count': 66,\n  'ratio': 0.22996515679442509},\n {'date': datetime.date(2020, 9, 20),\n  'total_count': 220,\n  'count': 32,\n  'ratio': 0.14545454545454545},\n {'date': datetime.date(2020, 9, 21),\n  'total_count': 500,\n  'count': 89,\n  'ratio': 0.178},\n {'date': datetime.date(2020, 9, 22),\n  'total_count': 571,\n  'count': 91,\n  'ratio': 0.159369527145359},\n {'date': datetime.date(2020, 9, 23),\n  'total_count': 622,\n  'count': 78,\n  'ratio': 0.12540192926045016},\n {'date': datetime.date(2020, 9, 24),\n  'total_count': 660,\n  'count': 89,\n  'ratio': 0.13484848484848486},\n {'date': datetime.date(2020, 9, 25),\n  'total_count': 609,\n  'count': 63,\n  'ratio': 0.10344827586206896},\n {'date': datetime.date(2020, 9, 26),\n  'total_count': 262,\n  'count': 66,\n  'ratio': 0.25190839694656486},\n {'date': datetime.date(2020, 9, 27),\n  'total_count': 277,\n  'count': 48,\n  'ratio': 0.17328519855595667},\n {'date': datetime.date(2020, 9, 28),\n  'total_count': 523,\n  'count': 60,\n  'ratio': 0.1147227533460803},\n {'date': datetime.date(2020, 9, 29),\n  'total_count': 710,\n  'count': 116,\n  'ratio': 0.16338028169014085},\n {'date': datetime.date(2020, 9, 30),\n  'total_count': 658,\n  'count': 117,\n  'ratio': 0.1778115501519757},\n {'date': datetime.date(2020, 10, 1),\n  'total_count': 623,\n  'count': 89,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2020, 10, 2),\n  'total_count': 703,\n  'count': 135,\n  'ratio': 0.19203413940256045},\n {'date': datetime.date(2020, 10, 3),\n  'total_count': 307,\n  'count': 55,\n  'ratio': 0.1791530944625407},\n {'date': datetime.date(2020, 10, 4),\n  'total_count': 270,\n  'count': 41,\n  'ratio': 0.15185185185185185},\n {'date': datetime.date(2020, 10, 5),\n  'total_count': 550,\n  'count': 109,\n  'ratio': 0.19818181818181818},\n {'date': datetime.date(2020, 10, 6),\n  'total_count': 643,\n  'count': 113,\n  'ratio': 0.17573872472783825},\n {'date': datetime.date(2020, 10, 7),\n  'total_count': 741,\n  'count': 144,\n  'ratio': 0.19433198380566802},\n {'date': datetime.date(2020, 10, 8),\n  'total_count': 695,\n  'count': 132,\n  'ratio': 0.18992805755395684},\n {'date': datetime.date(2020, 10, 9),\n  'total_count': 567,\n  'count': 93,\n  'ratio': 0.164021164021164},\n {'date': datetime.date(2020, 10, 10),\n  'total_count': 248,\n  'count': 54,\n  'ratio': 0.21774193548387097},\n {'date': datetime.date(2020, 10, 11),\n  'total_count': 238,\n  'count': 28,\n  'ratio': 0.11764705882352941},\n {'date': datetime.date(2020, 10, 12),\n  'total_count': 523,\n  'count': 85,\n  'ratio': 0.16252390057361377},\n {'date': datetime.date(2020, 10, 13),\n  'total_count': 666,\n  'count': 117,\n  'ratio': 0.17567567567567569},\n {'date': datetime.date(2020, 10, 14),\n  'total_count': 652,\n  'count': 118,\n  'ratio': 0.18098159509202455},\n {'date': datetime.date(2020, 10, 15),\n  'total_count': 686,\n  'count': 136,\n  'ratio': 0.19825072886297376},\n {'date': datetime.date(2020, 10, 16),\n  'total_count': 613,\n  'count': 108,\n  'ratio': 0.1761827079934747},\n {'date': datetime.date(2020, 10, 17),\n  'total_count': 196,\n  'count': 73,\n  'ratio': 0.37244897959183676},\n {'date': datetime.date(2020, 10, 18),\n  'total_count': 235,\n  'count': 73,\n  'ratio': 0.31063829787234043},\n {'date': datetime.date(2020, 10, 19),\n  'total_count': 505,\n  'count': 120,\n  'ratio': 0.2376237623762376},\n {'date': datetime.date(2020, 10, 20),\n  'total_count': 635,\n  'count': 72,\n  'ratio': 0.11338582677165354},\n {'date': datetime.date(2020, 10, 21),\n  'total_count': 599,\n  'count': 105,\n  'ratio': 0.17529215358931552},\n {'date': datetime.date(2020, 10, 22),\n  'total_count': 743,\n  'count': 118,\n  'ratio': 0.15881561238223418},\n {'date': datetime.date(2020, 10, 23),\n  'total_count': 604,\n  'count': 84,\n  'ratio': 0.1390728476821192},\n {'date': datetime.date(2020, 10, 24),\n  'total_count': 270,\n  'count': 50,\n  'ratio': 0.18518518518518517},\n {'date': datetime.date(2020, 10, 25),\n  'total_count': 255,\n  'count': 46,\n  'ratio': 0.1803921568627451},\n {'date': datetime.date(2020, 10, 26),\n  'total_count': 542,\n  'count': 89,\n  'ratio': 0.16420664206642066},\n {'date': datetime.date(2020, 10, 27),\n  'total_count': 654,\n  'count': 116,\n  'ratio': 0.17737003058103976},\n {'date': datetime.date(2020, 10, 28),\n  'total_count': 648,\n  'count': 138,\n  'ratio': 0.21296296296296297},\n {'date': datetime.date(2020, 10, 29),\n  'total_count': 700,\n  'count': 120,\n  'ratio': 0.17142857142857143},\n {'date': datetime.date(2020, 10, 30),\n  'total_count': 607,\n  'count': 99,\n  'ratio': 0.1630971993410214},\n {'date': datetime.date(2020, 10, 31),\n  'total_count': 301,\n  'count': 54,\n  'ratio': 0.17940199335548174},\n {'date': datetime.date(2020, 11, 1),\n  'total_count': 311,\n  'count': 78,\n  'ratio': 0.2508038585209003},\n {'date': datetime.date(2020, 11, 2),\n  'total_count': 708,\n  'count': 164,\n  'ratio': 0.23163841807909605},\n {'date': datetime.date(2020, 11, 3),\n  'total_count': 1087,\n  'count': 224,\n  'ratio': 0.20607175712971482},\n {'date': datetime.date(2020, 11, 4),\n  'total_count': 653,\n  'count': 128,\n  'ratio': 0.19601837672281777},\n {'date': datetime.date(2020, 11, 5),\n  'total_count': 671,\n  'count': 97,\n  'ratio': 0.14456035767511177},\n {'date': datetime.date(2020, 11, 6),\n  'total_count': 647,\n  'count': 101,\n  'ratio': 0.1561051004636785},\n {'date': datetime.date(2020, 11, 7),\n  'total_count': 412,\n  'count': 120,\n  'ratio': 0.2912621359223301},\n {'date': datetime.date(2020, 11, 8),\n  'total_count': 289,\n  'count': 83,\n  'ratio': 0.28719723183391005},\n {'date': datetime.date(2020, 11, 9),\n  'total_count': 550,\n  'count': 108,\n  'ratio': 0.19636363636363635},\n {'date': datetime.date(2020, 11, 10),\n  'total_count': 650,\n  'count': 129,\n  'ratio': 0.19846153846153847},\n {'date': datetime.date(2020, 11, 11),\n  'total_count': 573,\n  'count': 87,\n  'ratio': 0.1518324607329843},\n {'date': datetime.date(2020, 11, 12),\n  'total_count': 657,\n  'count': 106,\n  'ratio': 0.1613394216133942},\n {'date': datetime.date(2020, 11, 13),\n  'total_count': 571,\n  'count': 69,\n  'ratio': 0.12084063047285463},\n {'date': datetime.date(2020, 11, 14),\n  'total_count': 188,\n  'count': 21,\n  'ratio': 0.11170212765957446},\n {'date': datetime.date(2020, 11, 15),\n  'total_count': 253,\n  'count': 32,\n  'ratio': 0.12648221343873517},\n {'date': datetime.date(2020, 11, 16),\n  'total_count': 511,\n  'count': 84,\n  'ratio': 0.1643835616438356},\n {'date': datetime.date(2020, 11, 17),\n  'total_count': 629,\n  'count': 87,\n  'ratio': 0.1383147853736089},\n {'date': datetime.date(2020, 11, 18),\n  'total_count': 580,\n  'count': 75,\n  'ratio': 0.12931034482758622},\n {'date': datetime.date(2020, 11, 19),\n  'total_count': 613,\n  'count': 50,\n  'ratio': 0.08156606851549755},\n {'date': datetime.date(2020, 11, 20),\n  'total_count': 574,\n  'count': 65,\n  'ratio': 0.1132404181184669},\n {'date': datetime.date(2020, 11, 21),\n  'total_count': 184,\n  'count': 13,\n  'ratio': 0.07065217391304347},\n {'date': datetime.date(2020, 11, 22),\n  'total_count': 211,\n  'count': 46,\n  'ratio': 0.21800947867298578},\n {'date': datetime.date(2020, 11, 23),\n  'total_count': 535,\n  'count': 61,\n  'ratio': 0.11401869158878504},\n {'date': datetime.date(2020, 11, 24),\n  'total_count': 503,\n  'count': 60,\n  'ratio': 0.11928429423459244},\n {'date': datetime.date(2020, 11, 25),\n  'total_count': 539,\n  'count': 64,\n  'ratio': 0.11873840445269017},\n {'date': datetime.date(2020, 11, 26),\n  'total_count': 272,\n  'count': 20,\n  'ratio': 0.07352941176470588},\n {'date': datetime.date(2020, 11, 27),\n  'total_count': 366,\n  'count': 43,\n  'ratio': 0.11748633879781421},\n {'date': datetime.date(2020, 11, 28),\n  'total_count': 213,\n  'count': 23,\n  'ratio': 0.107981220657277},\n {'date': datetime.date(2020, 11, 29),\n  'total_count': 204,\n  'count': 11,\n  'ratio': 0.05392156862745098},\n {'date': datetime.date(2020, 11, 30),\n  'total_count': 457,\n  'count': 57,\n  'ratio': 0.12472647702407003},\n {'date': datetime.date(2020, 12, 1),\n  'total_count': 569,\n  'count': 72,\n  'ratio': 0.1265377855887522},\n {'date': datetime.date(2020, 12, 2),\n  'total_count': 605,\n  'count': 90,\n  'ratio': 0.1487603305785124},\n {'date': datetime.date(2020, 12, 3),\n  'total_count': 662,\n  'count': 106,\n  'ratio': 0.16012084592145015},\n {'date': datetime.date(2020, 12, 4),\n  'total_count': 560,\n  'count': 77,\n  'ratio': 0.1375},\n {'date': datetime.date(2020, 12, 5),\n  'total_count': 212,\n  'count': 22,\n  'ratio': 0.10377358490566038},\n {'date': datetime.date(2020, 12, 6),\n  'total_count': 216,\n  'count': 14,\n  'ratio': 0.06481481481481481},\n {'date': datetime.date(2020, 12, 7),\n  'total_count': 488,\n  'count': 42,\n  'ratio': 0.0860655737704918},\n {'date': datetime.date(2020, 12, 8),\n  'total_count': 582,\n  'count': 61,\n  'ratio': 0.10481099656357389},\n {'date': datetime.date(2020, 12, 9),\n  'total_count': 582,\n  'count': 72,\n  'ratio': 0.12371134020618557},\n {'date': datetime.date(2020, 12, 10),\n  'total_count': 573,\n  'count': 57,\n  'ratio': 0.09947643979057591},\n {'date': datetime.date(2020, 12, 11),\n  'total_count': 534,\n  'count': 47,\n  'ratio': 0.08801498127340825},\n {'date': datetime.date(2020, 12, 12),\n  'total_count': 267,\n  'count': 17,\n  'ratio': 0.06367041198501873},\n {'date': datetime.date(2020, 12, 13),\n  'total_count': 242,\n  'count': 29,\n  'ratio': 0.11983471074380166},\n {'date': datetime.date(2020, 12, 14),\n  'total_count': 557,\n  'count': 76,\n  'ratio': 0.13644524236983843},\n {'date': datetime.date(2020, 12, 15),\n  'total_count': 573,\n  'count': 58,\n  'ratio': 0.1012216404886562},\n {'date': datetime.date(2020, 12, 16),\n  'total_count': 591,\n  'count': 67,\n  'ratio': 0.11336717428087986},\n {'date': datetime.date(2020, 12, 17),\n  'total_count': 546,\n  'count': 45,\n  'ratio': 0.08241758241758242},\n {'date': datetime.date(2020, 12, 18),\n  'total_count': 542,\n  'count': 81,\n  'ratio': 0.14944649446494465},\n {'date': datetime.date(2020, 12, 19),\n  'total_count': 197,\n  'count': 28,\n  'ratio': 0.14213197969543148},\n {'date': datetime.date(2020, 12, 20),\n  'total_count': 232,\n  'count': 31,\n  'ratio': 0.1336206896551724},\n {'date': datetime.date(2020, 12, 21),\n  'total_count': 482,\n  'count': 29,\n  'ratio': 0.06016597510373444},\n {'date': datetime.date(2020, 12, 22),\n  'total_count': 532,\n  'count': 66,\n  'ratio': 0.12406015037593984},\n {'date': datetime.date(2020, 12, 23),\n  'total_count': 480,\n  'count': 36,\n  'ratio': 0.075},\n {'date': datetime.date(2020, 12, 24),\n  'total_count': 379,\n  'count': 40,\n  'ratio': 0.10554089709762533},\n {'date': datetime.date(2020, 12, 25),\n  'total_count': 157,\n  'count': 7,\n  'ratio': 0.044585987261146494},\n {'date': datetime.date(2020, 12, 26),\n  'total_count': 133,\n  'count': 5,\n  'ratio': 0.03759398496240601},\n {'date': datetime.date(2020, 12, 27),\n  'total_count': 178,\n  'count': 27,\n  'ratio': 0.15168539325842698},\n {'date': datetime.date(2020, 12, 28),\n  'total_count': 330,\n  'count': 38,\n  'ratio': 0.11515151515151516},\n {'date': datetime.date(2020, 12, 29),\n  'total_count': 386,\n  'count': 30,\n  'ratio': 0.07772020725388601},\n {'date': datetime.date(2020, 12, 30),\n  'total_count': 387,\n  'count': 35,\n  'ratio': 0.09043927648578812},\n {'date': datetime.date(2020, 12, 31),\n  'total_count': 412,\n  'count': 36,\n  'ratio': 0.08737864077669903},\n {'date': datetime.date(2021, 1, 1),\n  'total_count': 282,\n  'count': 15,\n  'ratio': 0.05319148936170213},\n {'date': datetime.date(2021, 1, 2),\n  'total_count': 151,\n  'count': 14,\n  'ratio': 0.09271523178807947},\n {'date': datetime.date(2021, 1, 3),\n  'total_count': 275,\n  'count': 85,\n  'ratio': 0.3090909090909091},\n {'date': datetime.date(2021, 1, 4),\n  'total_count': 469,\n  'count': 79,\n  'ratio': 0.16844349680170576},\n {'date': datetime.date(2021, 1, 5),\n  'total_count': 575,\n  'count': 84,\n  'ratio': 0.14608695652173914},\n {'date': datetime.date(2021, 1, 6),\n  'total_count': 700,\n  'count': 77,\n  'ratio': 0.11},\n {'date': datetime.date(2021, 1, 7),\n  'total_count': 698,\n  'count': 98,\n  'ratio': 0.14040114613180515},\n {'date': datetime.date(2021, 1, 8),\n  'total_count': 586,\n  'count': 48,\n  'ratio': 0.08191126279863481},\n {'date': datetime.date(2021, 1, 9),\n  'total_count': 265,\n  'count': 66,\n  'ratio': 0.2490566037735849},\n {'date': datetime.date(2021, 1, 10),\n  'total_count': 249,\n  'count': 44,\n  'ratio': 0.17670682730923695},\n {'date': datetime.date(2021, 1, 11),\n  'total_count': 588,\n  'count': 110,\n  'ratio': 0.1870748299319728},\n {'date': datetime.date(2021, 1, 12),\n  'total_count': 626,\n  'count': 111,\n  'ratio': 0.17731629392971246},\n {'date': datetime.date(2021, 1, 13),\n  'total_count': 617,\n  'count': 98,\n  'ratio': 0.15883306320907617},\n {'date': datetime.date(2021, 1, 14),\n  'total_count': 614,\n  'count': 98,\n  'ratio': 0.15960912052117263},\n {'date': datetime.date(2021, 1, 15),\n  'total_count': 578,\n  'count': 86,\n  'ratio': 0.14878892733564014},\n {'date': datetime.date(2021, 1, 16),\n  'total_count': 285,\n  'count': 41,\n  'ratio': 0.14385964912280702},\n {'date': datetime.date(2021, 1, 17),\n  'total_count': 257,\n  'count': 28,\n  'ratio': 0.10894941634241245},\n {'date': datetime.date(2021, 1, 18),\n  'total_count': 368,\n  'count': 44,\n  'ratio': 0.11956521739130435},\n {'date': datetime.date(2021, 1, 19),\n  'total_count': 616,\n  'count': 102,\n  'ratio': 0.16558441558441558},\n {'date': datetime.date(2021, 1, 20),\n  'total_count': 722,\n  'count': 183,\n  'ratio': 0.25346260387811637},\n {'date': datetime.date(2021, 1, 21),\n  'total_count': 598,\n  'count': 97,\n  'ratio': 0.16220735785953178},\n {'date': datetime.date(2021, 1, 22),\n  'total_count': 555,\n  'count': 55,\n  'ratio': 0.0990990990990991},\n {'date': datetime.date(2021, 1, 23),\n  'total_count': 224,\n  'count': 32,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2021, 1, 24),\n  'total_count': 232,\n  'count': 20,\n  'ratio': 0.08620689655172414},\n {'date': datetime.date(2021, 1, 25),\n  'total_count': 491,\n  'count': 60,\n  'ratio': 0.12219959266802444},\n {'date': datetime.date(2021, 1, 26),\n  'total_count': 564,\n  'count': 54,\n  'ratio': 0.09574468085106383},\n {'date': datetime.date(2021, 1, 27),\n  'total_count': 625,\n  'count': 59,\n  'ratio': 0.0944},\n {'date': datetime.date(2021, 1, 28),\n  'total_count': 581,\n  'count': 52,\n  'ratio': 0.08950086058519793},\n {'date': datetime.date(2021, 1, 29),\n  'total_count': 528,\n  'count': 58,\n  'ratio': 0.10984848484848485},\n {'date': datetime.date(2021, 1, 30),\n  'total_count': 156,\n  'count': 14,\n  'ratio': 0.08974358974358974},\n {'date': datetime.date(2021, 1, 31),\n  'total_count': 151,\n  'count': 20,\n  'ratio': 0.13245033112582782},\n {'date': datetime.date(2021, 2, 1),\n  'total_count': 479,\n  'count': 56,\n  'ratio': 0.11691022964509394},\n {'date': datetime.date(2021, 2, 2),\n  'total_count': 576,\n  'count': 51,\n  'ratio': 0.08854166666666667},\n {'date': datetime.date(2021, 2, 3),\n  'total_count': 559,\n  'count': 75,\n  'ratio': 0.13416815742397137},\n {'date': datetime.date(2021, 2, 4),\n  'total_count': 568,\n  'count': 82,\n  'ratio': 0.1443661971830986},\n {'date': datetime.date(2021, 2, 5),\n  'total_count': 540,\n  'count': 72,\n  'ratio': 0.13333333333333333},\n {'date': datetime.date(2021, 2, 6),\n  'total_count': 239,\n  'count': 30,\n  'ratio': 0.12552301255230125},\n {'date': datetime.date(2021, 2, 7),\n  'total_count': 293,\n  'count': 19,\n  'ratio': 0.06484641638225255},\n {'date': datetime.date(2021, 2, 8),\n  'total_count': 484,\n  'count': 63,\n  'ratio': 0.13016528925619836},\n {'date': datetime.date(2021, 2, 9),\n  'total_count': 589,\n  'count': 71,\n  'ratio': 0.12054329371816638},\n {'date': datetime.date(2021, 2, 10),\n  'total_count': 543,\n  'count': 53,\n  'ratio': 0.09760589318600368},\n {'date': datetime.date(2021, 2, 11),\n  'total_count': 558,\n  'count': 57,\n  'ratio': 0.10215053763440861},\n {'date': datetime.date(2021, 2, 12),\n  'total_count': 522,\n  'count': 62,\n  'ratio': 0.11877394636015326},\n {'date': datetime.date(2021, 2, 13),\n  'total_count': 237,\n  'count': 34,\n  'ratio': 0.14345991561181434},\n {'date': datetime.date(2021, 2, 14),\n  'total_count': 219,\n  'count': 38,\n  'ratio': 0.1735159817351598},\n {'date': datetime.date(2021, 2, 15),\n  'total_count': 276,\n  'count': 28,\n  'ratio': 0.10144927536231885},\n {'date': datetime.date(2021, 2, 16),\n  'total_count': 490,\n  'count': 61,\n  'ratio': 0.12448979591836734},\n {'date': datetime.date(2021, 2, 17),\n  'total_count': 564,\n  'count': 96,\n  'ratio': 0.1702127659574468},\n {'date': datetime.date(2021, 2, 18),\n  'total_count': 613,\n  'count': 67,\n  'ratio': 0.10929853181076672},\n {'date': datetime.date(2021, 2, 19),\n  'total_count': 536,\n  'count': 67,\n  'ratio': 0.125},\n {'date': datetime.date(2021, 2, 20),\n  'total_count': 193,\n  'count': 30,\n  'ratio': 0.15544041450777202},\n {'date': datetime.date(2021, 2, 21),\n  'total_count': 205,\n  'count': 20,\n  'ratio': 0.0975609756097561},\n {'date': datetime.date(2021, 2, 22),\n  'total_count': 465,\n  'count': 51,\n  'ratio': 0.10967741935483871},\n {'date': datetime.date(2021, 2, 23),\n  'total_count': 533,\n  'count': 69,\n  'ratio': 0.1294559099437148},\n {'date': datetime.date(2021, 2, 24),\n  'total_count': 496,\n  'count': 46,\n  'ratio': 0.09274193548387097},\n {'date': datetime.date(2021, 2, 25),\n  'total_count': 528,\n  'count': 55,\n  'ratio': 0.10416666666666667},\n {'date': datetime.date(2021, 2, 26),\n  'total_count': 547,\n  'count': 61,\n  'ratio': 0.11151736745886655},\n {'date': datetime.date(2021, 2, 27),\n  'total_count': 191,\n  'count': 21,\n  'ratio': 0.1099476439790576},\n {'date': datetime.date(2021, 2, 28),\n  'total_count': 258,\n  'count': 31,\n  'ratio': 0.12015503875968993},\n {'date': datetime.date(2021, 3, 1),\n  'total_count': 422,\n  'count': 40,\n  'ratio': 0.0947867298578199},\n {'date': datetime.date(2021, 3, 2),\n  'total_count': 539,\n  'count': 70,\n  'ratio': 0.12987012987012986},\n {'date': datetime.date(2021, 3, 3),\n  'total_count': 515,\n  'count': 51,\n  'ratio': 0.09902912621359224},\n {'date': datetime.date(2021, 3, 4),\n  'total_count': 510,\n  'count': 62,\n  'ratio': 0.12156862745098039},\n {'date': datetime.date(2021, 3, 5),\n  'total_count': 571,\n  'count': 53,\n  'ratio': 0.09281961471103327},\n {'date': datetime.date(2021, 3, 6),\n  'total_count': 214,\n  'count': 32,\n  'ratio': 0.14953271028037382},\n {'date': datetime.date(2021, 3, 7),\n  'total_count': 248,\n  'count': 19,\n  'ratio': 0.07661290322580645},\n {'date': datetime.date(2021, 3, 8),\n  'total_count': 501,\n  'count': 59,\n  'ratio': 0.11776447105788423},\n {'date': datetime.date(2021, 3, 9),\n  'total_count': 491,\n  'count': 47,\n  'ratio': 0.09572301425661914},\n {'date': datetime.date(2021, 3, 10),\n  'total_count': 529,\n  'count': 71,\n  'ratio': 0.1342155009451796},\n {'date': datetime.date(2021, 3, 11),\n  'total_count': 575,\n  'count': 63,\n  'ratio': 0.10956521739130434},\n {'date': datetime.date(2021, 3, 12),\n  'total_count': 492,\n  'count': 59,\n  'ratio': 0.11991869918699187},\n {'date': datetime.date(2021, 3, 13),\n  'total_count': 192,\n  'count': 21,\n  'ratio': 0.109375},\n {'date': datetime.date(2021, 3, 14),\n  'total_count': 241,\n  'count': 22,\n  'ratio': 0.0912863070539419},\n {'date': datetime.date(2021, 3, 15),\n  'total_count': 529,\n  'count': 62,\n  'ratio': 0.11720226843100189},\n {'date': datetime.date(2021, 3, 16),\n  'total_count': 522,\n  'count': 73,\n  'ratio': 0.13984674329501914},\n {'date': datetime.date(2021, 3, 17),\n  'total_count': 552,\n  'count': 76,\n  'ratio': 0.13768115942028986},\n {'date': datetime.date(2021, 3, 18),\n  'total_count': 583,\n  'count': 59,\n  'ratio': 0.10120068610634649},\n {'date': datetime.date(2021, 3, 19),\n  'total_count': 562,\n  'count': 74,\n  'ratio': 0.13167259786476868},\n {'date': datetime.date(2021, 3, 20),\n  'total_count': 208,\n  'count': 10,\n  'ratio': 0.04807692307692308},\n {'date': datetime.date(2021, 3, 21),\n  'total_count': 224,\n  'count': 26,\n  'ratio': 0.11607142857142858},\n {'date': datetime.date(2021, 3, 22),\n  'total_count': 523,\n  'count': 56,\n  'ratio': 0.10707456978967496},\n {'date': datetime.date(2021, 3, 23),\n  'total_count': 572,\n  'count': 92,\n  'ratio': 0.16083916083916083},\n {'date': datetime.date(2021, 3, 24),\n  'total_count': 528,\n  'count': 75,\n  'ratio': 0.14204545454545456},\n {'date': datetime.date(2021, 3, 25),\n  'total_count': 604,\n  'count': 119,\n  'ratio': 0.19701986754966888},\n {'date': datetime.date(2021, 3, 26),\n  'total_count': 503,\n  'count': 54,\n  'ratio': 0.1073558648111332},\n {'date': datetime.date(2021, 3, 27),\n  'total_count': 225,\n  'count': 16,\n  'ratio': 0.07111111111111111},\n {'date': datetime.date(2021, 3, 28),\n  'total_count': 183,\n  'count': 19,\n  'ratio': 0.10382513661202186},\n {'date': datetime.date(2021, 3, 29),\n  'total_count': 480,\n  'count': 46,\n  'ratio': 0.09583333333333334},\n {'date': datetime.date(2021, 3, 30),\n  'total_count': 488,\n  'count': 59,\n  'ratio': 0.12090163934426229},\n {'date': datetime.date(2021, 3, 31),\n  'total_count': 537,\n  'count': 39,\n  'ratio': 0.07262569832402235},\n {'date': datetime.date(2021, 4, 1),\n  'total_count': 529,\n  'count': 45,\n  'ratio': 0.08506616257088846},\n {'date': datetime.date(2021, 4, 2),\n  'total_count': 519,\n  'count': 65,\n  'ratio': 0.1252408477842004},\n {'date': datetime.date(2021, 4, 3),\n  'total_count': 236,\n  'count': 18,\n  'ratio': 0.07627118644067797},\n {'date': datetime.date(2021, 4, 4),\n  'total_count': 206,\n  'count': 14,\n  'ratio': 0.06796116504854369},\n {'date': datetime.date(2021, 4, 5),\n  'total_count': 476,\n  'count': 83,\n  'ratio': 0.17436974789915966},\n {'date': datetime.date(2021, 4, 6),\n  'total_count': 586,\n  'count': 53,\n  'ratio': 0.09044368600682594},\n {'date': datetime.date(2021, 4, 7),\n  'total_count': 516,\n  'count': 64,\n  'ratio': 0.12403100775193798},\n {'date': datetime.date(2021, 4, 8),\n  'total_count': 362,\n  'count': 44,\n  'ratio': 0.12154696132596685},\n {'date': datetime.date(2021, 4, 9),\n  'total_count': 309,\n  'count': 29,\n  'ratio': 0.09385113268608414},\n {'date': datetime.date(2021, 4, 10),\n  'total_count': 97,\n  'count': 13,\n  'ratio': 0.13402061855670103},\n {'date': datetime.date(2021, 4, 11),\n  'total_count': 114,\n  'count': 15,\n  'ratio': 0.13157894736842105},\n {'date': datetime.date(2021, 4, 12),\n  'total_count': 348,\n  'count': 31,\n  'ratio': 0.08908045977011494},\n {'date': datetime.date(2021, 4, 13),\n  'total_count': 453,\n  'count': 34,\n  'ratio': 0.07505518763796909},\n {'date': datetime.date(2021, 4, 14),\n  'total_count': 517,\n  'count': 52,\n  'ratio': 0.10058027079303675},\n {'date': datetime.date(2021, 4, 15),\n  'total_count': 580,\n  'count': 53,\n  'ratio': 0.09137931034482759},\n {'date': datetime.date(2021, 4, 16),\n  'total_count': 527,\n  'count': 41,\n  'ratio': 0.0777988614800759},\n {'date': datetime.date(2021, 4, 17),\n  'total_count': 191,\n  'count': 16,\n  'ratio': 0.08376963350785341},\n {'date': datetime.date(2021, 4, 18),\n  'total_count': 126,\n  'count': 15,\n  'ratio': 0.11904761904761904},\n {'date': datetime.date(2021, 4, 19),\n  'total_count': 449,\n  'count': 51,\n  'ratio': 0.11358574610244988},\n {'date': datetime.date(2021, 4, 20),\n  'total_count': 487,\n  'count': 44,\n  'ratio': 0.09034907597535935},\n {'date': datetime.date(2021, 4, 21),\n  'total_count': 459,\n  'count': 46,\n  'ratio': 0.10021786492374728},\n {'date': datetime.date(2021, 4, 22),\n  'total_count': 492,\n  'count': 38,\n  'ratio': 0.07723577235772358},\n {'date': datetime.date(2021, 4, 23),\n  'total_count': 438,\n  'count': 41,\n  'ratio': 0.09360730593607305},\n {'date': datetime.date(2021, 4, 24),\n  'total_count': 186,\n  'count': 14,\n  'ratio': 0.07526881720430108},\n {'date': datetime.date(2021, 4, 25),\n  'total_count': 190,\n  'count': 16,\n  'ratio': 0.08421052631578947},\n {'date': datetime.date(2021, 4, 26),\n  'total_count': 411,\n  'count': 49,\n  'ratio': 0.1192214111922141},\n {'date': datetime.date(2021, 4, 27),\n  'total_count': 416,\n  'count': 64,\n  'ratio': 0.15384615384615385},\n {'date': datetime.date(2021, 4, 28),\n  'total_count': 491,\n  'count': 58,\n  'ratio': 0.11812627291242363},\n {'date': datetime.date(2021, 4, 29),\n  'total_count': 559,\n  'count': 56,\n  'ratio': 0.1001788908765653},\n {'date': datetime.date(2021, 4, 30),\n  'total_count': 505,\n  'count': 58,\n  'ratio': 0.11485148514851486},\n {'date': datetime.date(2021, 5, 1),\n  'total_count': 221,\n  'count': 16,\n  'ratio': 0.07239819004524888},\n {'date': datetime.date(2021, 5, 2),\n  'total_count': 184,\n  'count': 12,\n  'ratio': 0.06521739130434782},\n {'date': datetime.date(2021, 5, 3),\n  'total_count': 418,\n  'count': 36,\n  'ratio': 0.0861244019138756},\n {'date': datetime.date(2021, 5, 4),\n  'total_count': 485,\n  'count': 49,\n  'ratio': 0.10103092783505155},\n {'date': datetime.date(2021, 5, 5),\n  'total_count': 475,\n  'count': 53,\n  'ratio': 0.11157894736842106},\n {'date': datetime.date(2021, 5, 6),\n  'total_count': 519,\n  'count': 68,\n  'ratio': 0.13102119460500963},\n {'date': datetime.date(2021, 5, 7),\n  'total_count': 501,\n  'count': 59,\n  'ratio': 0.11776447105788423},\n {'date': datetime.date(2021, 5, 8),\n  'total_count': 165,\n  'count': 12,\n  'ratio': 0.07272727272727272},\n {'date': datetime.date(2021, 5, 9),\n  'total_count': 184,\n  'count': 18,\n  'ratio': 0.09782608695652174},\n {'date': datetime.date(2021, 5, 10),\n  'total_count': 426,\n  'count': 50,\n  'ratio': 0.11737089201877934},\n {'date': datetime.date(2021, 5, 11),\n  'total_count': 492,\n  'count': 47,\n  'ratio': 0.09552845528455285},\n {'date': datetime.date(2021, 5, 12),\n  'total_count': 532,\n  'count': 59,\n  'ratio': 0.11090225563909774},\n {'date': datetime.date(2021, 5, 13),\n  'total_count': 588,\n  'count': 87,\n  'ratio': 0.14795918367346939},\n {'date': datetime.date(2021, 5, 14),\n  'total_count': 488,\n  'count': 55,\n  'ratio': 0.11270491803278689},\n {'date': datetime.date(2021, 5, 15),\n  'total_count': 225,\n  'count': 28,\n  'ratio': 0.12444444444444444},\n {'date': datetime.date(2021, 5, 16),\n  'total_count': 170,\n  'count': 32,\n  'ratio': 0.18823529411764706},\n {'date': datetime.date(2021, 5, 17),\n  'total_count': 421,\n  'count': 48,\n  'ratio': 0.11401425178147269},\n {'date': datetime.date(2021, 5, 18),\n  'total_count': 500,\n  'count': 54,\n  'ratio': 0.108},\n {'date': datetime.date(2021, 5, 19),\n  'total_count': 519,\n  'count': 56,\n  'ratio': 0.10789980732177264},\n {'date': datetime.date(2021, 5, 20),\n  'total_count': 519,\n  'count': 56,\n  'ratio': 0.10789980732177264},\n {'date': datetime.date(2021, 5, 21),\n  'total_count': 511,\n  'count': 41,\n  'ratio': 0.08023483365949119},\n {'date': datetime.date(2021, 5, 22),\n  'total_count': 197,\n  'count': 33,\n  'ratio': 0.16751269035532995},\n {'date': datetime.date(2021, 5, 23),\n  'total_count': 186,\n  'count': 7,\n  'ratio': 0.03763440860215054},\n {'date': datetime.date(2021, 5, 24),\n  'total_count': 453,\n  'count': 31,\n  'ratio': 0.0684326710816777},\n {'date': datetime.date(2021, 5, 25),\n  'total_count': 522,\n  'count': 47,\n  'ratio': 0.09003831417624521},\n {'date': datetime.date(2021, 5, 26),\n  'total_count': 504,\n  'count': 51,\n  'ratio': 0.10119047619047619},\n {'date': datetime.date(2021, 5, 27),\n  'total_count': 546,\n  'count': 51,\n  'ratio': 0.09340659340659341},\n {'date': datetime.date(2021, 5, 28),\n  'total_count': 462,\n  'count': 57,\n  'ratio': 0.12337662337662338},\n {'date': datetime.date(2021, 5, 29),\n  'total_count': 224,\n  'count': 13,\n  'ratio': 0.05803571428571429},\n {'date': datetime.date(2021, 5, 30),\n  'total_count': 177,\n  'count': 12,\n  'ratio': 0.06779661016949153},\n {'date': datetime.date(2021, 5, 31),\n  'total_count': 258,\n  'count': 21,\n  'ratio': 0.08139534883720931},\n {'date': datetime.date(2021, 6, 1),\n  'total_count': 425,\n  'count': 55,\n  'ratio': 0.12941176470588237},\n {'date': datetime.date(2021, 6, 2),\n  'total_count': 482,\n  'count': 71,\n  'ratio': 0.14730290456431536},\n {'date': datetime.date(2021, 6, 3),\n  'total_count': 504,\n  'count': 61,\n  'ratio': 0.12103174603174603},\n {'date': datetime.date(2021, 6, 4),\n  'total_count': 507,\n  'count': 46,\n  'ratio': 0.09072978303747535},\n {'date': datetime.date(2021, 6, 5),\n  'total_count': 209,\n  'count': 43,\n  'ratio': 0.20574162679425836},\n {'date': datetime.date(2021, 6, 6),\n  'total_count': 220,\n  'count': 18,\n  'ratio': 0.08181818181818182},\n {'date': datetime.date(2021, 6, 7),\n  'total_count': 445,\n  'count': 59,\n  'ratio': 0.13258426966292136},\n {'date': datetime.date(2021, 6, 8),\n  'total_count': 485,\n  'count': 45,\n  'ratio': 0.09278350515463918},\n {'date': datetime.date(2021, 6, 9),\n  'total_count': 519,\n  'count': 34,\n  'ratio': 0.06551059730250482},\n {'date': datetime.date(2021, 6, 10),\n  'total_count': 531,\n  'count': 52,\n  'ratio': 0.09792843691148775},\n {'date': datetime.date(2021, 6, 11),\n  'total_count': 518,\n  'count': 48,\n  'ratio': 0.09266409266409266},\n {'date': datetime.date(2021, 6, 12),\n  'total_count': 194,\n  'count': 17,\n  'ratio': 0.08762886597938144},\n {'date': datetime.date(2021, 6, 13),\n  'total_count': 241,\n  'count': 55,\n  'ratio': 0.22821576763485477},\n {'date': datetime.date(2021, 6, 14),\n  'total_count': 419,\n  'count': 55,\n  'ratio': 0.13126491646778043},\n {'date': datetime.date(2021, 6, 15),\n  'total_count': 514,\n  'count': 64,\n  'ratio': 0.1245136186770428},\n {'date': datetime.date(2021, 6, 16),\n  'total_count': 540,\n  'count': 78,\n  'ratio': 0.14444444444444443},\n {'date': datetime.date(2021, 6, 17),\n  'total_count': 502,\n  'count': 54,\n  'ratio': 0.10756972111553785},\n {'date': datetime.date(2021, 6, 18),\n  'total_count': 442,\n  'count': 58,\n  'ratio': 0.13122171945701358},\n {'date': datetime.date(2021, 6, 19),\n  'total_count': 189,\n  'count': 16,\n  'ratio': 0.08465608465608465},\n {'date': datetime.date(2021, 6, 20),\n  'total_count': 211,\n  'count': 30,\n  'ratio': 0.14218009478672985},\n {'date': datetime.date(2021, 6, 21),\n  'total_count': 390,\n  'count': 58,\n  'ratio': 0.14871794871794872},\n {'date': datetime.date(2021, 6, 22),\n  'total_count': 514,\n  'count': 98,\n  'ratio': 0.19066147859922178},\n {'date': datetime.date(2021, 6, 23),\n  'total_count': 506,\n  'count': 83,\n  'ratio': 0.16403162055335968},\n {'date': datetime.date(2021, 6, 24),\n  'total_count': 493,\n  'count': 44,\n  'ratio': 0.08924949290060852},\n {'date': datetime.date(2021, 6, 25),\n  'total_count': 481,\n  'count': 44,\n  'ratio': 0.09147609147609148},\n {'date': datetime.date(2021, 6, 26),\n  'total_count': 180,\n  'count': 12,\n  'ratio': 0.06666666666666667},\n {'date': datetime.date(2021, 6, 27),\n  'total_count': 191,\n  'count': 18,\n  'ratio': 0.09424083769633508},\n {'date': datetime.date(2021, 6, 28),\n  'total_count': 418,\n  'count': 31,\n  'ratio': 0.07416267942583732},\n {'date': datetime.date(2021, 6, 29),\n  'total_count': 499,\n  'count': 46,\n  'ratio': 0.09218436873747494},\n {'date': datetime.date(2021, 6, 30),\n  'total_count': 556,\n  'count': 50,\n  'ratio': 0.08992805755395683},\n {'date': datetime.date(2021, 7, 1),\n  'total_count': 515,\n  'count': 53,\n  'ratio': 0.1029126213592233},\n {'date': datetime.date(2021, 7, 2),\n  'total_count': 494,\n  'count': 36,\n  'ratio': 0.0728744939271255},\n {'date': datetime.date(2021, 7, 3),\n  'total_count': 180,\n  'count': 11,\n  'ratio': 0.06111111111111111},\n {'date': datetime.date(2021, 7, 4),\n  'total_count': 155,\n  'count': 20,\n  'ratio': 0.12903225806451613},\n {'date': datetime.date(2021, 7, 5),\n  'total_count': 239,\n  'count': 21,\n  'ratio': 0.08786610878661087},\n {'date': datetime.date(2021, 7, 6),\n  'total_count': 499,\n  'count': 53,\n  'ratio': 0.1062124248496994},\n {'date': datetime.date(2021, 7, 7),\n  'total_count': 473,\n  'count': 40,\n  'ratio': 0.08456659619450317},\n {'date': datetime.date(2021, 7, 8),\n  'total_count': 527,\n  'count': 52,\n  'ratio': 0.09867172675521822},\n {'date': datetime.date(2021, 7, 9),\n  'total_count': 440,\n  'count': 24,\n  'ratio': 0.05454545454545454},\n {'date': datetime.date(2021, 7, 10),\n  'total_count': 190,\n  'count': 27,\n  'ratio': 0.14210526315789473},\n {'date': datetime.date(2021, 7, 11),\n  'total_count': 247,\n  'count': 12,\n  'ratio': 0.048582995951417005},\n {'date': datetime.date(2021, 7, 12),\n  'total_count': 420,\n  'count': 63,\n  'ratio': 0.15},\n {'date': datetime.date(2021, 7, 13),\n  'total_count': 505,\n  'count': 72,\n  'ratio': 0.14257425742574256},\n {'date': datetime.date(2021, 7, 14),\n  'total_count': 483,\n  'count': 36,\n  'ratio': 0.07453416149068323},\n {'date': datetime.date(2021, 7, 15),\n  'total_count': 470,\n  'count': 47,\n  'ratio': 0.1},\n {'date': datetime.date(2021, 7, 16),\n  'total_count': 483,\n  'count': 49,\n  'ratio': 0.10144927536231885},\n {'date': datetime.date(2021, 7, 17),\n  'total_count': 174,\n  'count': 30,\n  'ratio': 0.1724137931034483},\n {'date': datetime.date(2021, 7, 18),\n  'total_count': 186,\n  'count': 11,\n  'ratio': 0.05913978494623656},\n {'date': datetime.date(2021, 7, 19),\n  'total_count': 459,\n  'count': 59,\n  'ratio': 0.12854030501089325},\n {'date': datetime.date(2021, 7, 20),\n  'total_count': 544,\n  'count': 63,\n  'ratio': 0.11580882352941177},\n {'date': datetime.date(2021, 7, 21),\n  'total_count': 493,\n  'count': 55,\n  'ratio': 0.11156186612576065},\n {'date': datetime.date(2021, 7, 22),\n  'total_count': 532,\n  'count': 50,\n  'ratio': 0.09398496240601503},\n {'date': datetime.date(2021, 7, 23),\n  'total_count': 551,\n  'count': 39,\n  'ratio': 0.07078039927404718},\n {'date': datetime.date(2021, 7, 24),\n  'total_count': 228,\n  'count': 13,\n  'ratio': 0.05701754385964912},\n {'date': datetime.date(2021, 7, 25),\n  'total_count': 215,\n  'count': 31,\n  'ratio': 0.14418604651162792},\n {'date': datetime.date(2021, 7, 26),\n  'total_count': 462,\n  'count': 43,\n  'ratio': 0.09307359307359307},\n {'date': datetime.date(2021, 7, 27),\n  'total_count': 576,\n  'count': 44,\n  'ratio': 0.0763888888888889},\n {'date': datetime.date(2021, 7, 28),\n  'total_count': 567,\n  'count': 49,\n  'ratio': 0.08641975308641975},\n {'date': datetime.date(2021, 7, 29),\n  'total_count': 560,\n  'count': 39,\n  'ratio': 0.06964285714285715},\n {'date': datetime.date(2021, 7, 30),\n  'total_count': 542,\n  'count': 46,\n  'ratio': 0.08487084870848709},\n {'date': datetime.date(2021, 7, 31),\n  'total_count': 178,\n  'count': 14,\n  'ratio': 0.07865168539325842},\n {'date': datetime.date(2021, 8, 1),\n  'total_count': 204,\n  'count': 8,\n  'ratio': 0.0392156862745098},\n {'date': datetime.date(2021, 8, 2),\n  'total_count': 440,\n  'count': 35,\n  'ratio': 0.07954545454545454},\n {'date': datetime.date(2021, 8, 3),\n  'total_count': 553,\n  'count': 70,\n  'ratio': 0.12658227848101267},\n {'date': datetime.date(2021, 8, 4),\n  'total_count': 511,\n  'count': 48,\n  'ratio': 0.09393346379647749},\n {'date': datetime.date(2021, 8, 5),\n  'total_count': 536,\n  'count': 48,\n  'ratio': 0.08955223880597014},\n {'date': datetime.date(2021, 8, 6),\n  'total_count': 511,\n  'count': 29,\n  'ratio': 0.05675146771037182},\n {'date': datetime.date(2021, 8, 7),\n  'total_count': 178,\n  'count': 14,\n  'ratio': 0.07865168539325842},\n {'date': datetime.date(2021, 8, 8),\n  'total_count': 208,\n  'count': 18,\n  'ratio': 0.08653846153846154},\n {'date': datetime.date(2021, 8, 9),\n  'total_count': 394,\n  'count': 34,\n  'ratio': 0.08629441624365482},\n {'date': datetime.date(2021, 8, 10),\n  'total_count': 477,\n  'count': 57,\n  'ratio': 0.11949685534591195},\n {'date': datetime.date(2021, 8, 11),\n  'total_count': 456,\n  'count': 30,\n  'ratio': 0.06578947368421052},\n {'date': datetime.date(2021, 8, 12),\n  'total_count': 508,\n  'count': 63,\n  'ratio': 0.12401574803149606},\n {'date': datetime.date(2021, 8, 13),\n  'total_count': 396,\n  'count': 35,\n  'ratio': 0.08838383838383838},\n {'date': datetime.date(2021, 8, 14),\n  'total_count': 156,\n  'count': 11,\n  'ratio': 0.07051282051282051},\n {'date': datetime.date(2021, 8, 15),\n  'total_count': 203,\n  'count': 13,\n  'ratio': 0.06403940886699508},\n {'date': datetime.date(2021, 8, 16),\n  'total_count': 397,\n  'count': 34,\n  'ratio': 0.08564231738035265},\n {'date': datetime.date(2021, 8, 17),\n  'total_count': 473,\n  'count': 28,\n  'ratio': 0.05919661733615222},\n {'date': datetime.date(2021, 8, 18),\n  'total_count': 452,\n  'count': 59,\n  'ratio': 0.13053097345132744},\n {'date': datetime.date(2021, 8, 19),\n  'total_count': 455,\n  'count': 23,\n  'ratio': 0.05054945054945055},\n {'date': datetime.date(2021, 8, 20),\n  'total_count': 404,\n  'count': 30,\n  'ratio': 0.07425742574257425},\n {'date': datetime.date(2021, 8, 21),\n  'total_count': 156,\n  'count': 14,\n  'ratio': 0.08974358974358974},\n {'date': datetime.date(2021, 8, 22),\n  'total_count': 164,\n  'count': 11,\n  'ratio': 0.06707317073170732},\n {'date': datetime.date(2021, 8, 23),\n  'total_count': 350,\n  'count': 28,\n  'ratio': 0.08},\n {'date': datetime.date(2021, 8, 24),\n  'total_count': 414,\n  'count': 34,\n  'ratio': 0.0821256038647343},\n {'date': datetime.date(2021, 8, 25),\n  'total_count': 437,\n  'count': 38,\n  'ratio': 0.08695652173913043},\n {'date': datetime.date(2021, 8, 26),\n  'total_count': 414,\n  'count': 45,\n  'ratio': 0.10869565217391304},\n {'date': datetime.date(2021, 8, 27),\n  'total_count': 431,\n  'count': 28,\n  'ratio': 0.06496519721577726},\n {'date': datetime.date(2021, 8, 28),\n  'total_count': 170,\n  'count': 9,\n  'ratio': 0.052941176470588235},\n {'date': datetime.date(2021, 8, 29),\n  'total_count': 196,\n  'count': 19,\n  'ratio': 0.09693877551020408},\n {'date': datetime.date(2021, 8, 30),\n  'total_count': 393,\n  'count': 21,\n  'ratio': 0.05343511450381679},\n {'date': datetime.date(2021, 8, 31),\n  'total_count': 452,\n  'count': 48,\n  'ratio': 0.10619469026548672},\n {'date': datetime.date(2021, 9, 1),\n  'total_count': 476,\n  'count': 37,\n  'ratio': 0.07773109243697479},\n {'date': datetime.date(2021, 9, 2),\n  'total_count': 609,\n  'count': 53,\n  'ratio': 0.08702791461412152},\n {'date': datetime.date(2021, 9, 3),\n  'total_count': 498,\n  'count': 37,\n  'ratio': 0.07429718875502007},\n {'date': datetime.date(2021, 9, 4),\n  'total_count': 224,\n  'count': 12,\n  'ratio': 0.05357142857142857},\n {'date': datetime.date(2021, 9, 5),\n  'total_count': 210,\n  'count': 14,\n  'ratio': 0.06666666666666667},\n {'date': datetime.date(2021, 9, 6),\n  'total_count': 252,\n  'count': 20,\n  'ratio': 0.07936507936507936},\n {'date': datetime.date(2021, 9, 7),\n  'total_count': 495,\n  'count': 47,\n  'ratio': 0.09494949494949495},\n {'date': datetime.date(2021, 9, 8),\n  'total_count': 544,\n  'count': 38,\n  'ratio': 0.06985294117647059},\n {'date': datetime.date(2021, 9, 9),\n  'total_count': 602,\n  'count': 46,\n  'ratio': 0.07641196013289037},\n {'date': datetime.date(2021, 9, 10),\n  'total_count': 592,\n  'count': 65,\n  'ratio': 0.1097972972972973},\n {'date': datetime.date(2021, 9, 11),\n  'total_count': 361,\n  'count': 71,\n  'ratio': 0.19667590027700832},\n {'date': datetime.date(2021, 9, 12),\n  'total_count': 252,\n  'count': 18,\n  'ratio': 0.07142857142857142},\n {'date': datetime.date(2021, 9, 13),\n  'total_count': 519,\n  'count': 63,\n  'ratio': 0.12138728323699421},\n {'date': datetime.date(2021, 9, 14),\n  'total_count': 608,\n  'count': 75,\n  'ratio': 0.12335526315789473},\n {'date': datetime.date(2021, 9, 15),\n  'total_count': 573,\n  'count': 83,\n  'ratio': 0.14485165794066318},\n {'date': datetime.date(2021, 9, 16),\n  'total_count': 540,\n  'count': 53,\n  'ratio': 0.09814814814814815},\n {'date': datetime.date(2021, 9, 17),\n  'total_count': 529,\n  'count': 46,\n  'ratio': 0.08695652173913043},\n {'date': datetime.date(2021, 9, 18),\n  'total_count': 220,\n  'count': 26,\n  'ratio': 0.11818181818181818},\n {'date': datetime.date(2021, 9, 19),\n  'total_count': 254,\n  'count': 21,\n  'ratio': 0.08267716535433071},\n {'date': datetime.date(2021, 9, 20),\n  'total_count': 437,\n  'count': 48,\n  'ratio': 0.10983981693363844},\n {'date': datetime.date(2021, 9, 21),\n  'total_count': 550,\n  'count': 46,\n  'ratio': 0.08363636363636363},\n {'date': datetime.date(2021, 9, 22),\n  'total_count': 492,\n  'count': 33,\n  'ratio': 0.06707317073170732},\n {'date': datetime.date(2021, 9, 23),\n  'total_count': 609,\n  'count': 38,\n  'ratio': 0.06239737274220033},\n {'date': datetime.date(2021, 9, 24),\n  'total_count': 556,\n  'count': 52,\n  'ratio': 0.09352517985611511},\n {'date': datetime.date(2021, 9, 25),\n  'total_count': 230,\n  'count': 24,\n  'ratio': 0.10434782608695652},\n {'date': datetime.date(2021, 9, 26),\n  'total_count': 314,\n  'count': 19,\n  'ratio': 0.06050955414012739},\n {'date': datetime.date(2021, 9, 27),\n  'total_count': 474,\n  'count': 71,\n  'ratio': 0.14978902953586498},\n {'date': datetime.date(2021, 9, 28),\n  'total_count': 510,\n  'count': 58,\n  'ratio': 0.11372549019607843},\n {'date': datetime.date(2021, 9, 29),\n  'total_count': 506,\n  'count': 45,\n  'ratio': 0.08893280632411067},\n {'date': datetime.date(2021, 9, 30),\n  'total_count': 620,\n  'count': 52,\n  'ratio': 0.08387096774193549},\n {'date': datetime.date(2021, 10, 1),\n  'total_count': 511,\n  'count': 54,\n  'ratio': 0.10567514677103718},\n {'date': datetime.date(2021, 10, 2),\n  'total_count': 222,\n  'count': 16,\n  'ratio': 0.07207207207207207},\n {'date': datetime.date(2021, 10, 3),\n  'total_count': 227,\n  'count': 19,\n  'ratio': 0.08370044052863436},\n {'date': datetime.date(2021, 10, 4),\n  'total_count': 462,\n  'count': 46,\n  'ratio': 0.09956709956709957},\n {'date': datetime.date(2021, 10, 5),\n  'total_count': 552,\n  'count': 42,\n  'ratio': 0.07608695652173914},\n {'date': datetime.date(2021, 10, 6),\n  'total_count': 485,\n  'count': 38,\n  'ratio': 0.07835051546391752},\n {'date': datetime.date(2021, 10, 7),\n  'total_count': 551,\n  'count': 44,\n  'ratio': 0.07985480943738657},\n {'date': datetime.date(2021, 10, 8),\n  'total_count': 519,\n  'count': 55,\n  'ratio': 0.10597302504816955},\n {'date': datetime.date(2021, 10, 9),\n  'total_count': 246,\n  'count': 12,\n  'ratio': 0.04878048780487805},\n {'date': datetime.date(2021, 10, 10),\n  'total_count': 210,\n  'count': 9,\n  'ratio': 0.04285714285714286},\n {'date': datetime.date(2021, 10, 11),\n  'total_count': 402,\n  'count': 31,\n  'ratio': 0.07711442786069651},\n {'date': datetime.date(2021, 10, 12),\n  'total_count': 475,\n  'count': 49,\n  'ratio': 0.1031578947368421},\n {'date': datetime.date(2021, 10, 13),\n  'total_count': 511,\n  'count': 46,\n  'ratio': 0.09001956947162426},\n {'date': datetime.date(2021, 10, 14),\n  'total_count': 618,\n  'count': 50,\n  'ratio': 0.08090614886731391},\n {'date': datetime.date(2021, 10, 15),\n  'total_count': 489,\n  'count': 42,\n  'ratio': 0.08588957055214724},\n {'date': datetime.date(2021, 10, 16),\n  'total_count': 242,\n  'count': 11,\n  'ratio': 0.045454545454545456},\n {'date': datetime.date(2021, 10, 17),\n  'total_count': 207,\n  'count': 26,\n  'ratio': 0.12560386473429952},\n {'date': datetime.date(2021, 10, 18),\n  'total_count': 484,\n  'count': 47,\n  'ratio': 0.09710743801652892},\n {'date': datetime.date(2021, 10, 19),\n  'total_count': 529,\n  'count': 43,\n  'ratio': 0.08128544423440454},\n {'date': datetime.date(2021, 10, 20),\n  'total_count': 559,\n  'count': 48,\n  'ratio': 0.08586762075134168},\n {'date': datetime.date(2021, 10, 21),\n  'total_count': 648,\n  'count': 47,\n  'ratio': 0.07253086419753087},\n {'date': datetime.date(2021, 10, 22),\n  'total_count': 512,\n  'count': 43,\n  'ratio': 0.083984375},\n {'date': datetime.date(2021, 10, 23),\n  'total_count': 221,\n  'count': 12,\n  'ratio': 0.05429864253393665},\n {'date': datetime.date(2021, 10, 24),\n  'total_count': 186,\n  'count': 8,\n  'ratio': 0.043010752688172046},\n {'date': datetime.date(2021, 10, 25),\n  'total_count': 469,\n  'count': 35,\n  'ratio': 0.07462686567164178},\n {'date': datetime.date(2021, 10, 26),\n  'total_count': 552,\n  'count': 64,\n  'ratio': 0.11594202898550725},\n {'date': datetime.date(2021, 10, 27),\n  'total_count': 560,\n  'count': 42,\n  'ratio': 0.075},\n {'date': datetime.date(2021, 10, 28),\n  'total_count': 618,\n  'count': 56,\n  'ratio': 0.09061488673139159},\n {'date': datetime.date(2021, 10, 29),\n  'total_count': 529,\n  'count': 40,\n  'ratio': 0.07561436672967864},\n {'date': datetime.date(2021, 10, 30),\n  'total_count': 260,\n  'count': 20,\n  'ratio': 0.07692307692307693},\n {'date': datetime.date(2021, 10, 31),\n  'total_count': 263,\n  'count': 22,\n  'ratio': 0.08365019011406843},\n {'date': datetime.date(2021, 11, 1),\n  'total_count': 523,\n  'count': 54,\n  'ratio': 0.10325047801147227},\n {'date': datetime.date(2021, 11, 2),\n  'total_count': 681,\n  'count': 94,\n  'ratio': 0.13803230543318648},\n {'date': datetime.date(2021, 11, 3),\n  'total_count': 606,\n  'count': 103,\n  'ratio': 0.16996699669966997},\n {'date': datetime.date(2021, 11, 4),\n  'total_count': 565,\n  'count': 52,\n  'ratio': 0.0920353982300885},\n {'date': datetime.date(2021, 11, 5),\n  'total_count': 533,\n  'count': 42,\n  'ratio': 0.07879924953095685},\n {'date': datetime.date(2021, 11, 6),\n  'total_count': 290,\n  'count': 20,\n  'ratio': 0.06896551724137931},\n {'date': datetime.date(2021, 11, 7),\n  'total_count': 229,\n  'count': 12,\n  'ratio': 0.05240174672489083},\n {'date': datetime.date(2021, 11, 8),\n  'total_count': 471,\n  'count': 41,\n  'ratio': 0.0870488322717622},\n {'date': datetime.date(2021, 11, 9),\n  'total_count': 569,\n  'count': 53,\n  'ratio': 0.09314586994727592},\n {'date': datetime.date(2021, 11, 10),\n  'total_count': 665,\n  'count': 56,\n  'ratio': 0.08421052631578947},\n {'date': datetime.date(2021, 11, 11),\n  'total_count': 567,\n  'count': 53,\n  'ratio': 0.09347442680776014},\n {'date': datetime.date(2021, 11, 12),\n  'total_count': 603,\n  'count': 67,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2021, 11, 13),\n  'total_count': 279,\n  'count': 15,\n  'ratio': 0.053763440860215055},\n {'date': datetime.date(2021, 11, 14),\n  'total_count': 218,\n  'count': 13,\n  'ratio': 0.05963302752293578},\n {'date': datetime.date(2021, 11, 15),\n  'total_count': 502,\n  'count': 43,\n  'ratio': 0.08565737051792828},\n {'date': datetime.date(2021, 11, 16),\n  'total_count': 575,\n  'count': 44,\n  'ratio': 0.07652173913043478},\n {'date': datetime.date(2021, 11, 17),\n  'total_count': 603,\n  'count': 49,\n  'ratio': 0.0812603648424544},\n {'date': datetime.date(2021, 11, 18),\n  'total_count': 666,\n  'count': 44,\n  'ratio': 0.06606606606606606},\n {'date': datetime.date(2021, 11, 19),\n  'total_count': 313,\n  'count': 27,\n  'ratio': 0.08626198083067092},\n {'date': datetime.date(2021, 11, 20),\n  'total_count': 256,\n  'count': 18,\n  'ratio': 0.0703125},\n {'date': datetime.date(2021, 11, 21),\n  'total_count': 179,\n  'count': 18,\n  'ratio': 0.1005586592178771},\n {'date': datetime.date(2021, 11, 22),\n  'total_count': 315,\n  'count': 30,\n  'ratio': 0.09523809523809523},\n {'date': datetime.date(2021, 11, 23),\n  'total_count': 420,\n  'count': 35,\n  'ratio': 0.08333333333333333},\n {'date': datetime.date(2021, 11, 24),\n  'total_count': 515,\n  'count': 37,\n  'ratio': 0.07184466019417475},\n {'date': datetime.date(2021, 11, 25),\n  'total_count': 312,\n  'count': 21,\n  'ratio': 0.0673076923076923},\n {'date': datetime.date(2021, 11, 26),\n  'total_count': 385,\n  'count': 11,\n  'ratio': 0.02857142857142857},\n {'date': datetime.date(2021, 11, 27),\n  'total_count': 199,\n  'count': 11,\n  'ratio': 0.05527638190954774},\n {'date': datetime.date(2021, 11, 28),\n  'total_count': 276,\n  'count': 27,\n  'ratio': 0.09782608695652174},\n {'date': datetime.date(2021, 11, 29),\n  'total_count': 450,\n  'count': 37,\n  'ratio': 0.08222222222222222},\n {'date': datetime.date(2021, 11, 30),\n  'total_count': 526,\n  'count': 40,\n  'ratio': 0.07604562737642585},\n {'date': datetime.date(2021, 12, 1),\n  'total_count': 511,\n  'count': 44,\n  'ratio': 0.08610567514677103},\n {'date': datetime.date(2021, 12, 2),\n  'total_count': 576,\n  'count': 65,\n  'ratio': 0.11284722222222222},\n {'date': datetime.date(2021, 12, 3),\n  'total_count': 494,\n  'count': 34,\n  'ratio': 0.06882591093117409},\n {'date': datetime.date(2021, 12, 4),\n  'total_count': 212,\n  'count': 15,\n  'ratio': 0.07075471698113207},\n {'date': datetime.date(2021, 12, 5),\n  'total_count': 280,\n  'count': 22,\n  'ratio': 0.07857142857142857},\n {'date': datetime.date(2021, 12, 6),\n  'total_count': 482,\n  'count': 41,\n  'ratio': 0.08506224066390042},\n {'date': datetime.date(2021, 12, 7),\n  'total_count': 463,\n  'count': 34,\n  'ratio': 0.0734341252699784},\n {'date': datetime.date(2021, 12, 8),\n  'total_count': 512,\n  'count': 62,\n  'ratio': 0.12109375},\n {'date': datetime.date(2021, 12, 9),\n  'total_count': 556,\n  'count': 65,\n  'ratio': 0.11690647482014388},\n {'date': datetime.date(2021, 12, 10),\n  'total_count': 513,\n  'count': 34,\n  'ratio': 0.06627680311890838},\n {'date': datetime.date(2021, 12, 11),\n  'total_count': 209,\n  'count': 10,\n  'ratio': 0.04784688995215311},\n {'date': datetime.date(2021, 12, 12),\n  'total_count': 222,\n  'count': 20,\n  'ratio': 0.09009009009009009},\n {'date': datetime.date(2021, 12, 13),\n  'total_count': 444,\n  'count': 33,\n  'ratio': 0.07432432432432433},\n {'date': datetime.date(2021, 12, 14),\n  'total_count': 504,\n  'count': 49,\n  'ratio': 0.09722222222222222},\n {'date': datetime.date(2021, 12, 15),\n  'total_count': 476,\n  'count': 29,\n  'ratio': 0.06092436974789916},\n {'date': datetime.date(2021, 12, 16),\n  'total_count': 424,\n  'count': 43,\n  'ratio': 0.10141509433962265},\n {'date': datetime.date(2021, 12, 17),\n  'total_count': 488,\n  'count': 46,\n  'ratio': 0.0942622950819672},\n {'date': datetime.date(2021, 12, 18),\n  'total_count': 193,\n  'count': 8,\n  'ratio': 0.04145077720207254},\n {'date': datetime.date(2021, 12, 19),\n  'total_count': 168,\n  'count': 9,\n  'ratio': 0.05357142857142857},\n {'date': datetime.date(2021, 12, 20),\n  'total_count': 158,\n  'count': 14,\n  'ratio': 0.08860759493670886},\n {'date': datetime.date(2021, 12, 21),\n  'total_count': 194,\n  'count': 14,\n  'ratio': 0.07216494845360824},\n {'date': datetime.date(2021, 12, 22),\n  'total_count': 248,\n  'count': 33,\n  'ratio': 0.13306451612903225},\n {'date': datetime.date(2021, 12, 23),\n  'total_count': 290,\n  'count': 29,\n  'ratio': 0.1},\n {'date': datetime.date(2021, 12, 24),\n  'total_count': 175,\n  'count': 16,\n  'ratio': 0.09142857142857143},\n {'date': datetime.date(2021, 12, 25),\n  'total_count': 84,\n  'count': 8,\n  'ratio': 0.09523809523809523},\n {'date': datetime.date(2021, 12, 26),\n  'total_count': 141,\n  'count': 17,\n  'ratio': 0.12056737588652482},\n {'date': datetime.date(2021, 12, 27),\n  'total_count': 288,\n  'count': 27,\n  'ratio': 0.09375},\n {'date': datetime.date(2021, 12, 28),\n  'total_count': 347,\n  'count': 28,\n  'ratio': 0.08069164265129683},\n {'date': datetime.date(2021, 12, 29),\n  'total_count': 360,\n  'count': 32,\n  'ratio': 0.08888888888888889},\n {'date': datetime.date(2021, 12, 30),\n  'total_count': 379,\n  'count': 26,\n  'ratio': 0.06860158311345646},\n {'date': datetime.date(2021, 12, 31),\n  'total_count': 303,\n  'count': 25,\n  'ratio': 0.08250825082508251},\n {'date': datetime.date(2022, 1, 1),\n  'total_count': 159,\n  'count': 9,\n  'ratio': 0.05660377358490566},\n {'date': datetime.date(2022, 1, 2),\n  'total_count': 192,\n  'count': 8,\n  'ratio': 0.041666666666666664},\n {'date': datetime.date(2022, 1, 3),\n  'total_count': 399,\n  'count': 41,\n  'ratio': 0.10275689223057644},\n {'date': datetime.date(2022, 1, 4),\n  'total_count': 471,\n  'count': 39,\n  'ratio': 0.08280254777070063},\n {'date': datetime.date(2022, 1, 5),\n  'total_count': 484,\n  'count': 46,\n  'ratio': 0.09504132231404959},\n {'date': datetime.date(2022, 1, 6),\n  'total_count': 537,\n  'count': 57,\n  'ratio': 0.10614525139664804},\n {'date': datetime.date(2022, 1, 7),\n  'total_count': 482,\n  'count': 56,\n  'ratio': 0.11618257261410789},\n {'date': datetime.date(2022, 1, 8),\n  'total_count': 225,\n  'count': 25,\n  'ratio': 0.1111111111111111},\n {'date': datetime.date(2022, 1, 9),\n  'total_count': 218,\n  'count': 19,\n  'ratio': 0.0871559633027523},\n {'date': datetime.date(2022, 1, 10),\n  'total_count': 490,\n  'count': 45,\n  'ratio': 0.09183673469387756},\n {'date': datetime.date(2022, 1, 11),\n  'total_count': 507,\n  'count': 38,\n  'ratio': 0.07495069033530571},\n {'date': datetime.date(2022, 1, 12),\n  'total_count': 489,\n  'count': 37,\n  'ratio': 0.07566462167689161},\n {'date': datetime.date(2022, 1, 13),\n  'total_count': 539,\n  'count': 57,\n  'ratio': 0.10575139146567718},\n {'date': datetime.date(2022, 1, 14),\n  'total_count': 544,\n  'count': 50,\n  'ratio': 0.09191176470588236},\n {'date': datetime.date(2022, 1, 15),\n  'total_count': 285,\n  'count': 20,\n  'ratio': 0.07017543859649122},\n {'date': datetime.date(2022, 1, 16),\n  'total_count': 231,\n  'count': 26,\n  'ratio': 0.11255411255411256},\n {'date': datetime.date(2022, 1, 17),\n  'total_count': 290,\n  'count': 34,\n  'ratio': 0.11724137931034483},\n {'date': datetime.date(2022, 1, 18),\n  'total_count': 470,\n  'count': 56,\n  'ratio': 0.11914893617021277},\n {'date': datetime.date(2022, 1, 19),\n  'total_count': 529,\n  'count': 57,\n  'ratio': 0.10775047258979206},\n {'date': datetime.date(2022, 1, 20),\n  'total_count': 562,\n  'count': 63,\n  'ratio': 0.11209964412811388},\n {'date': datetime.date(2022, 1, 21),\n  'total_count': 538,\n  'count': 51,\n  'ratio': 0.09479553903345725},\n {'date': datetime.date(2022, 1, 22),\n  'total_count': 233,\n  'count': 16,\n  'ratio': 0.06866952789699571},\n {'date': datetime.date(2022, 1, 23),\n  'total_count': 270,\n  'count': 16,\n  'ratio': 0.05925925925925926},\n {'date': datetime.date(2022, 1, 24),\n  'total_count': 475,\n  'count': 48,\n  'ratio': 0.10105263157894737},\n {'date': datetime.date(2022, 1, 25),\n  'total_count': 321,\n  'count': 43,\n  'ratio': 0.13395638629283488},\n {'date': datetime.date(2022, 1, 26),\n  'total_count': 195,\n  'count': 39,\n  'ratio': 0.2},\n {'date': datetime.date(2022, 1, 27),\n  'total_count': 168,\n  'count': 28,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2022, 1, 28),\n  'total_count': 204,\n  'count': 35,\n  'ratio': 0.1715686274509804},\n {'date': datetime.date(2022, 1, 29),\n  'total_count': 102,\n  'count': 13,\n  'ratio': 0.12745098039215685},\n {'date': datetime.date(2022, 1, 30),\n  'total_count': 88,\n  'count': 8,\n  'ratio': 0.09090909090909091},\n {'date': datetime.date(2022, 1, 31),\n  'total_count': 152,\n  'count': 43,\n  'ratio': 0.28289473684210525},\n {'date': datetime.date(2022, 2, 1),\n  'total_count': 177,\n  'count': 26,\n  'ratio': 0.14689265536723164},\n {'date': datetime.date(2022, 2, 2),\n  'total_count': 203,\n  'count': 38,\n  'ratio': 0.18719211822660098},\n {'date': datetime.date(2022, 2, 3),\n  'total_count': 238,\n  'count': 39,\n  'ratio': 0.1638655462184874},\n {'date': datetime.date(2022, 2, 4),\n  'total_count': 232,\n  'count': 50,\n  'ratio': 0.21551724137931033},\n {'date': datetime.date(2022, 2, 5),\n  'total_count': 89,\n  'count': 13,\n  'ratio': 0.14606741573033707},\n {'date': datetime.date(2022, 2, 6),\n  'total_count': 92,\n  'count': 17,\n  'ratio': 0.18478260869565216},\n {'date': datetime.date(2022, 2, 7),\n  'total_count': 255,\n  'count': 43,\n  'ratio': 0.16862745098039217},\n {'date': datetime.date(2022, 2, 8),\n  'total_count': 273,\n  'count': 51,\n  'ratio': 0.18681318681318682},\n {'date': datetime.date(2022, 2, 9),\n  'total_count': 218,\n  'count': 39,\n  'ratio': 0.17889908256880735},\n {'date': datetime.date(2022, 2, 10),\n  'total_count': 234,\n  'count': 34,\n  'ratio': 0.1452991452991453},\n {'date': datetime.date(2022, 2, 11),\n  'total_count': 187,\n  'count': 28,\n  'ratio': 0.1497326203208556},\n {'date': datetime.date(2022, 2, 12),\n  'total_count': 43,\n  'count': 11,\n  'ratio': 0.2558139534883721},\n {'date': datetime.date(2022, 2, 13),\n  'total_count': 59,\n  'count': 7,\n  'ratio': 0.11864406779661017},\n {'date': datetime.date(2022, 2, 14),\n  'total_count': 165,\n  'count': 30,\n  'ratio': 0.18181818181818182},\n {'date': datetime.date(2022, 2, 15),\n  'total_count': 236,\n  'count': 50,\n  'ratio': 0.211864406779661},\n {'date': datetime.date(2022, 2, 16),\n  'total_count': 216,\n  'count': 38,\n  'ratio': 0.17592592592592593},\n {'date': datetime.date(2022, 2, 17),\n  'total_count': 251,\n  'count': 53,\n  'ratio': 0.21115537848605578},\n {'date': datetime.date(2022, 2, 18),\n  'total_count': 202,\n  'count': 44,\n  'ratio': 0.21782178217821782},\n {'date': datetime.date(2022, 2, 19),\n  'total_count': 86,\n  'count': 19,\n  'ratio': 0.22093023255813954},\n {'date': datetime.date(2022, 2, 20),\n  'total_count': 106,\n  'count': 13,\n  'ratio': 0.12264150943396226},\n {'date': datetime.date(2022, 2, 21),\n  'total_count': 132,\n  'count': 23,\n  'ratio': 0.17424242424242425},\n {'date': datetime.date(2022, 2, 22),\n  'total_count': 226,\n  'count': 37,\n  'ratio': 0.16371681415929204},\n {'date': datetime.date(2022, 2, 23),\n  'total_count': 207,\n  'count': 24,\n  'ratio': 0.11594202898550725},\n {'date': datetime.date(2022, 2, 24),\n  'total_count': 302,\n  'count': 83,\n  'ratio': 0.27483443708609273},\n {'date': datetime.date(2022, 2, 25),\n  'total_count': 297,\n  'count': 49,\n  'ratio': 0.16498316498316498},\n {'date': datetime.date(2022, 2, 26),\n  'total_count': 124,\n  'count': 26,\n  'ratio': 0.20967741935483872},\n {'date': datetime.date(2022, 2, 27),\n  'total_count': 135,\n  'count': 21,\n  'ratio': 0.15555555555555556},\n {'date': datetime.date(2022, 2, 28),\n  'total_count': 219,\n  'count': 41,\n  'ratio': 0.1872146118721461},\n {'date': datetime.date(2022, 3, 1),\n  'total_count': 349,\n  'count': 53,\n  'ratio': 0.1518624641833811},\n {'date': datetime.date(2022, 3, 2),\n  'total_count': 255,\n  'count': 77,\n  'ratio': 0.30196078431372547},\n {'date': datetime.date(2022, 3, 3),\n  'total_count': 258,\n  'count': 38,\n  'ratio': 0.14728682170542637},\n {'date': datetime.date(2022, 3, 4),\n  'total_count': 242,\n  'count': 42,\n  'ratio': 0.17355371900826447},\n {'date': datetime.date(2022, 3, 5),\n  'total_count': 114,\n  'count': 13,\n  'ratio': 0.11403508771929824},\n {'date': datetime.date(2022, 3, 6),\n  'total_count': 115,\n  'count': 16,\n  'ratio': 0.1391304347826087},\n {'date': datetime.date(2022, 3, 7),\n  'total_count': 208,\n  'count': 30,\n  'ratio': 0.14423076923076922},\n {'date': datetime.date(2022, 3, 8),\n  'total_count': 276,\n  'count': 46,\n  'ratio': 0.16666666666666666},\n {'date': datetime.date(2022, 3, 9),\n  'total_count': 268,\n  'count': 33,\n  'ratio': 0.12313432835820895},\n {'date': datetime.date(2022, 3, 10),\n  'total_count': 249,\n  'count': 34,\n  'ratio': 0.13654618473895583},\n {'date': datetime.date(2022, 3, 11),\n  'total_count': 240,\n  'count': 34,\n  'ratio': 0.14166666666666666},\n {'date': datetime.date(2022, 3, 12),\n  'total_count': 103,\n  'count': 22,\n  'ratio': 0.21359223300970873},\n {'date': datetime.date(2022, 3, 13),\n  'total_count': 151,\n  'count': 17,\n  'ratio': 0.11258278145695365},\n {'date': datetime.date(2022, 3, 14),\n  'total_count': 184,\n  'count': 19,\n  'ratio': 0.10326086956521739},\n {'date': datetime.date(2022, 3, 15),\n  'total_count': 183,\n  'count': 23,\n  'ratio': 0.12568306010928962},\n {'date': datetime.date(2022, 3, 16),\n  'total_count': 196,\n  'count': 28,\n  'ratio': 0.14285714285714285},\n {'date': datetime.date(2022, 3, 17),\n  'total_count': 234,\n  'count': 28,\n  'ratio': 0.11965811965811966},\n {'date': datetime.date(2022, 3, 18),\n  'total_count': 204,\n  'count': 42,\n  'ratio': 0.20588235294117646},\n {'date': datetime.date(2022, 3, 19),\n  'total_count': 72,\n  'count': 13,\n  'ratio': 0.18055555555555555},\n {'date': datetime.date(2022, 3, 20),\n  'total_count': 93,\n  'count': 9,\n  'ratio': 0.0967741935483871},\n {'date': datetime.date(2022, 3, 21),\n  'total_count': 212,\n  'count': 36,\n  'ratio': 0.16981132075471697},\n {'date': datetime.date(2022, 3, 22),\n  'total_count': 211,\n  'count': 35,\n  'ratio': 0.16587677725118483},\n {'date': datetime.date(2022, 3, 23),\n  'total_count': 243,\n  'count': 37,\n  'ratio': 0.1522633744855967},\n {'date': datetime.date(2022, 3, 24),\n  'total_count': 169,\n  'count': 24,\n  'ratio': 0.14201183431952663},\n {'date': datetime.date(2022, 3, 25),\n  'total_count': 157,\n  'count': 19,\n  'ratio': 0.12101910828025478},\n {'date': datetime.date(2022, 3, 26),\n  'total_count': 68,\n  'count': 14,\n  'ratio': 0.20588235294117646},\n ...]\n\n\n\n# and you can chart attention over time with some simple notebook work (using Bokeh here)\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\ndf = pd.DataFrame.from_dict(results)\ndf['date']= pd.to_datetime(df['date'])\nsource = ColumnDataSource(df)\np = figure(x_axis_type=\"datetime\", width=900, height=250)\np.line(x='date', y='count', line_width=2, source=source)  # your could use `ratio` instead of `count` to see normalized attention\nshow(p)\n\n\n  \n\n\n\n\n\n\nNormalizing within a Source\n\nresults = search_api.story_count(my_query, start_date, end_date, source_ids=sources)\nsource_ratio = results['relevant'] / results['total']\nf'{source_ratio:.2%} of the media sources {sources} stories were about {my_query_name}'\n\n'11.46% of the media sources [1, 2, 3] stories were about politics'"
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html#research-within-a-country---using-collections",
    "href": "data/Raw_Data/mediacloud_api.html#research-within-a-country---using-collections",
    "title": "Media Cloud: Measuring Attention",
    "section": "Research Within a Country - using collections",
    "text": "Research Within a Country - using collections\n\n# check in our collection of country-level US National media sources\nUS_NATIONAL_COLLECTION = 34412234\nresults = search_api.story_count(my_query, start_date, end_date, collection_ids=[US_NATIONAL_COLLECTION])\nus_country_ratio = results['relevant'] / results['total']\n# '{:.2%} of stories from national-level US media sources mentioneded \"climate change\"'.format(us_country_ratio)\nf'{us_country_ratio:.2%} of stories from national-level US media sources mentioneded {my_query_name}'\n\n'4.94% of stories from national-level US media sources mentioneded politics'\n\n\n\n# now we can compare this to the source-level coverage\ncoverage_ratio = 1 / (source_ratio / us_country_ratio)\nf'{my_query_name} received {coverage_ratio:.2%} times less coverage in {sources} than you might expect based on other US national papers'\n\n'politics received 43.07% times less coverage in [1, 2, 3] than you might expect based on other US national papers'\n\n\n\n# or compare to another country (India in this case)\nINDIA_NATIONAL = 34412118\nresults = search_api.story_count('\"climate change\"', start_date, end_date, collection_ids=[INDIA_NATIONAL])\nindia_country_ratio = results['relevant'] / results['total']\nf'{india_country_ratio:.2%} of stories from national-level Indian media sources mentioned {my_query_name}'\n\n'0.50% of stories from national-level Indian media sources mentioned politics'\n\n\n\ncoverage_ratio =  1 / (india_country_ratio / us_country_ratio)\nf'at the national level {my_query_name} is covered {coverage_ratio:.2%} times less in India than the US'\n\n'at the national level politics is covered 987.39% times less in India than the US'"
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html#listing-stories",
    "href": "data/Raw_Data/mediacloud_api.html#listing-stories",
    "title": "Media Cloud: Measuring Attention",
    "section": "Listing Stories",
    "text": "Listing Stories\n\n# grab the most recent stories about this issue\nstories, _ = search_api.story_list(my_query, start_date, end_date)\nstories[:3]\n\n[{'id': '5b95e9e6c1aafd926e6adf48bb3fd28e7241fff54322889d811bda065e6bbde8',\n  'media_name': 'yourmiddleeast.com',\n  'media_url': 'yourmiddleeast.com',\n  'title': 'The Arabs’ Moment',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://yourmiddleeast.com/2020/03/06/the-arabs-moment/',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 44, 377339)},\n {'id': 'f709469e307fcba31cfd1fca3a3165c76754c9b8a42f72f1e42423b38f791977',\n  'media_name': 'yahoo.com',\n  'media_url': 'yahoo.com',\n  'title': \"Black voters power Joe Biden's Super Tuesday success\",\n  'publish_date': datetime.date(2020, 3, 5),\n  'url': 'https://news.yahoo.com/black-voters-power-bidens-super-055038667.html',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 42, 376413)},\n {'id': 'cd8607202f26bed7d766b5a884decf83e73ebbeea46095ebbb6f0fb1379ba6b9',\n  'media_name': 'patrika.com',\n  'media_url': 'patrika.com',\n  'title': 'ईरान: Coronavirus से विदेश मंत्री के सलाहकार की गई जान, देश में अब तक 107 की मौत',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://www.patrika.com/gulf-news/iran-foreign-minister-s-advisor-hossein-sheikholeslam-dead-by-coronavirus-107-dead-in-country-so-far-5864564/',\n  'language': 'hi',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 39, 796407)}]\n\n\n\n# let's fetch all the stories matching our query on one day\nall_stories = []\nmore_stories = True\npagination_token = None\nstory_start_date = dt.date(2023,11,29)\nstory_end_date = dt.date(2023,11,30)\nwhile more_stories:\n    page, pagination_token = search_api.story_list(my_query, story_start_date, story_end_date,\n                                                   collection_ids=[US_NATIONAL_COLLECTION],\n                                                   pagination_token=pagination_token)\n    all_stories += page\n    more_stories = pagination_token is not None\nlen(all_stories)\n\n1203\n\n\n\nWriting a CSV of Story Data\n\nimport csv\n\nstory_start_date_str = story_start_date.strftime('%Y%m%d')\nstory_end_date_str = story_end_date.strftime('%Y%m%d')\n\nfieldnames = ['id', 'publish_date', 'title', 'url', 'language', 'media_name', 'media_url', 'indexed_date']\noutput_filename = f\"./mediacloud_api_data/{my_query_name.replace(' ', '_')}-storylist-{story_start_date_str}_{story_end_date_str}.csv\"\noutput_dir = os.path.dirname(output_filename)\n\nif output_dir and not os.path.exists(output_dir):\n    os.makedirs(output_dir)  # Create the directory if it doesn't exist\n    \nwith open(output_filename, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n    writer.writeheader()\n    for s in all_stories:\n        writer.writerow(s)"
  },
  {
    "objectID": "data/Raw_Data/mediacloud_api.html#top-media-sources",
    "href": "data/Raw_Data/mediacloud_api.html#top-media-sources",
    "title": "Media Cloud: Measuring Attention",
    "section": "Top Media Sources",
    "text": "Top Media Sources\n\n# List media producing the most stories matching the search\nINDIA_NATIONAL = 34412118\nresults = search_api.sources(my_query, start_date, end_date, collection_ids=[INDIA_NATIONAL])\nresults\n\n[{'source': 'amarujala.com', 'count': 392592},\n {'source': 'indiatimes.com', 'count': 178544},\n {'source': 'patrika.com', 'count': 66994},\n {'source': 'jagran.com', 'count': 50268},\n {'source': 'news18.com', 'count': 28673},\n {'source': 'business-standard.com', 'count': 16273},\n {'source': 'indianexpress.com', 'count': 16006},\n {'source': 'thehindu.com', 'count': 13750},\n {'source': 'newindianexpress.com', 'count': 12385},\n {'source': 'india.com', 'count': 8146},\n {'source': 'hindustantimes.com', 'count': 7252},\n {'source': 'ndtv.com', 'count': 7020},\n {'source': 'freepressjournal.in', 'count': 6847},\n {'source': 'firstpost.com', 'count': 4973},\n {'source': 'thewire.in', 'count': 4889},\n {'source': 'thequint.com', 'count': 4038},\n {'source': 'sify.com', 'count': 3908},\n {'source': 'scroll.in', 'count': 3602},\n {'source': 'tribuneindia.com', 'count': 3583},\n {'source': 'rediff.com', 'count': 3539},\n {'source': 'newsclick.in', 'count': 3473},\n {'source': 'indiasnews.net', 'count': 3073},\n {'source': 'financialexpress.com', 'count': 2862},\n {'source': 'thehindubusinessline.com', 'count': 2802},\n {'source': 'oneindia.com', 'count': 2704},\n {'source': 'opindia.com', 'count': 2257},\n {'source': 'mid-day.com', 'count': 2093},\n {'source': 'swarajyamag.com', 'count': 2088},\n {'source': 'countercurrents.org', 'count': 2043},\n {'source': 'dnaindia.com', 'count': 1780},\n {'source': 'ipanewspack.com', 'count': 1767},\n {'source': 'thelogicalindian.com', 'count': 1761},\n {'source': 'livemint.com', 'count': 1579},\n {'source': 'businessworld.in', 'count': 1445},\n {'source': 'moneycontrol.com', 'count': 1376},\n {'source': 'theweek.in', 'count': 1247},\n {'source': 'deccanchronicle.com', 'count': 1083},\n {'source': 'ibtimes.co.in', 'count': 916},\n {'source': 'feminisminindia.com', 'count': 876},\n {'source': 'newslaundry.com', 'count': 718},\n {'source': 'youthkiawaaz.com', 'count': 687},\n {'source': 'pinkvilla.com', 'count': 671},\n {'source': 'republicworld.com', 'count': 411},\n {'source': 'outlookindia.com', 'count': 403},\n {'source': 'asianage.com', 'count': 391},\n {'source': 'livehindustan.com', 'count': 377},\n {'source': 'huffingtonpost.in', 'count': 336},\n {'source': 'bhaskar.com', 'count': 318},\n {'source': 'boomlive.in', 'count': 271},\n {'source': 'idrw.org', 'count': 249},\n {'source': 'newsx.com', 'count': 192},\n {'source': 'northeasttoday.in', 'count': 172},\n {'source': 'assamtribune.com', 'count': 159},\n {'source': 'kractivist.org', 'count': 144},\n {'source': 'deccanherald.com', 'count': 136},\n {'source': 'indiantelevision.com', 'count': 131},\n {'source': 'manoramaonline.com', 'count': 127},\n {'source': 'mxmindia.com', 'count': 118},\n {'source': 'newsonair.com', 'count': 118},\n {'source': 'afaqs.com', 'count': 109},\n {'source': 'filmfare.com', 'count': 96},\n {'source': 'thebetterindia.com', 'count': 93},\n {'source': 'forbesindia.com', 'count': 84},\n {'source': 'espncricinfo.com', 'count': 66},\n {'source': 'inkhabar.com', 'count': 63},\n {'source': 'medianama.com', 'count': 61},\n {'source': 'homegrown.co.in', 'count': 55},\n {'source': 'thenewsminute.com', 'count': 55},\n {'source': 'telegraphindia.com', 'count': 49},\n {'source': 'swaminomics.org', 'count': 42},\n {'source': 'dinakaran.com', 'count': 41},\n {'source': 'missmalini.com', 'count': 41},\n {'source': 'indiafacts.org', 'count': 40},\n {'source': 'ruralindiaonline.org', 'count': 39},\n {'source': 'dailypioneer.com', 'count': 34},\n {'source': 'epw.in', 'count': 30},\n {'source': 'abclive.in', 'count': 27},\n {'source': 'idronline.org', 'count': 27},\n {'source': 'intoday.in', 'count': 27},\n {'source': 'indiaspend.com', 'count': 25},\n {'source': 'caravanmagazine.in', 'count': 18},\n {'source': 'socialsamosa.com', 'count': 17},\n {'source': 'ramachandraguha.in', 'count': 12},\n {'source': 'thedelhiwalla.com', 'count': 11},\n {'source': 'dayafterindia.com', 'count': 8},\n {'source': 'factchecker.in', 'count': 6},\n {'source': 'indiaresists.com', 'count': 6},\n {'source': 'dalitweb.org', 'count': 5},\n {'source': 'exchange4media.com', 'count': 5},\n {'source': 'scoopwhoop.com', 'count': 5},\n {'source': 'mapsofindia.com', 'count': 4},\n {'source': 'ptinews.com', 'count': 3},\n {'source': 'onlineindiannews.com', 'count': 2},\n {'source': 'worldsnap.com', 'count': 2},\n {'source': 'abplive.in', 'count': 1},\n {'source': 'dailythanthi.com', 'count': 1}]"
  },
  {
    "objectID": "data/Raw_Data/x_api.html",
    "href": "data/Raw_Data/x_api.html",
    "title": "DSAN5000 Project",
    "section": "",
    "text": "import json\nwith open('/Users/pengli/.api-keys.json') as f:\n    keys = json.load(f)\n\n# Authentication credentials\napi_key = keys['X_API_KEY']\napi_key_secret = keys['X_API_KEY_SECRET']\naccess_token = keys['x_access_token']\naccess_token_secret = keys['x_access_token_secret']\n\n\nimport tweepy\n\n# Authenticate and create API object\nauth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\napi = tweepy.API(auth)\n\n# Test the connection\nuser = api.verify_credentials()\nprint(f\"Authenticated as: {user.screen_name}\")\n\nAuthenticated as: pngleee\n\n\n\nclient = tweepy.Client(bearer_token=\"your_bearer_token\")\n\n# Search recent tweets\nquery = \"misinformation -is:retweet lang:en\"\ntweets = client.search_recent_tweets(query=query, max_results=10)\n\n# Print tweets\nfor tweet in tweets.data:\n    print(tweet.text)\n\n\n---------------------------------------------------------------------------\nUnauthorized                              Traceback (most recent call last)\nCell In[5], line 5\n      3 # Search recent tweets\n      4 query = \"misinformation -is:retweet lang:en\"\n----&gt; 5 tweets = client.search_recent_tweets(query=query, max_results=10)\n      7 # Print tweets\n      8 for tweet in tweets.data:\n\nFile /opt/anaconda3/lib/python3.12/site-packages/tweepy/client.py:1266, in Client.search_recent_tweets(self, query, user_auth, **params)\n   1174 \"\"\"search_recent_tweets( \\\n   1175     query, *, end_time=None, expansions=None, max_results=None, \\\n   1176     media_fields=None, next_token=None, place_fields=None, \\\n   (...)\n   1263 .. _Academic Research Project: https://developer.twitter.com/en/docs/projects\n   1264 \"\"\"\n   1265 params[\"query\"] = query\n-&gt; 1266 return self._make_request(\n   1267     \"GET\", \"/2/tweets/search/recent\", params=params,\n   1268     endpoint_parameters=(\n   1269         \"end_time\", \"expansions\", \"max_results\", \"media.fields\",\n   1270         \"next_token\", \"place.fields\", \"poll.fields\", \"query\",\n   1271         \"since_id\", \"sort_order\", \"start_time\", \"tweet.fields\",\n   1272         \"until_id\", \"user.fields\"\n   1273     ), data_type=Tweet, user_auth=user_auth\n   1274 )\n\nFile /opt/anaconda3/lib/python3.12/site-packages/tweepy/client.py:129, in BaseClient._make_request(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\n    123 def _make_request(\n    124     self, method, route, params={}, endpoint_parameters=(), json=None,\n    125     data_type=None, user_auth=False\n    126 ):\n    127     request_params = self._process_params(params, endpoint_parameters)\n--&gt; 129     response = self.request(method, route, params=request_params,\n    130                             json=json, user_auth=user_auth)\n    132     if self.return_type is requests.Response:\n    133         return response\n\nFile /opt/anaconda3/lib/python3.12/site-packages/tweepy/client.py:98, in BaseClient.request(self, method, route, params, json, user_auth)\n     96     raise BadRequest(response)\n     97 if response.status_code == 401:\n---&gt; 98     raise Unauthorized(response)\n     99 if response.status_code == 403:\n    100     raise Forbidden(response)\n\nUnauthorized: 401 Unauthorized\nUnauthorized"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "LIAR Dataset: A benchmark dataset for fake news detection, containing 12.8K human-labeled short statements from various contexts, including news articles, TV or radio interviews, campaign speeches, social media posts, and fact-checking websites. Each statement is annotated with labels indicating their truthfulness level.\nMediaCloud API Data: A comprehensive platform offering access to a vast collection of news articles and media content. The API enables programmatic access to analyze news coverage, track topics across different media sources, and study media ecosystems over time.\nNews API Data: A simple HTTP REST API for searching and retrieving live articles from all over the web. It provides access to headlines and articles from over 80,000 news sources worldwide, supporting features like keyword search, source filtering, and date range queries.\nLink to Raw Datasets",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/data-collection/main.html#set-up-api-key",
    "href": "technical-details/data-collection/main.html#set-up-api-key",
    "title": "Data Collection",
    "section": "Set Up API Key",
    "text": "Set Up API Key\n# Set up your API key and import needed things\nimport os, mediacloud.api\nfrom importlib.metadata import version\nfrom dotenv import load_dotenv\nimport datetime as dt\nfrom IPython.display import JSON\nimport bokeh.io\n\nbokeh.io.reset_output()\nbokeh.io.output_notebook()\nsearch_api = mediacloud.api.SearchApi(API_KEY)\nf'Using Media Cloud python client v{version(\"mediacloud\")}'\n\n\n&lt;a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"&gt;&lt;/a&gt;\n&lt;span id=\"f82c1b51-f0a0-43e6-aa84-8c61f4d54897\"&gt;Loading BokehJS ...&lt;/span&gt;\n\n'Using Media Cloud python client v4.3.0'",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/data-collection/main.html#attention-from-a-single-media-source",
    "href": "technical-details/data-collection/main.html#attention-from-a-single-media-source",
    "title": "Data Collection",
    "section": "Attention from a Single Media Source",
    "text": "Attention from a Single Media Source\n# check how many stories include the query phrase in the Washington Post (media id #2)\nmy_query = 'politics' # note the double quotes used to indicate use of the whole phrase\nstart_date = dt.date(2019, 7, 1)\nend_date = dt.date(2023, 7, 1)\nsources = [1, 2, 3] # NY Times, Washington Post, CS Monitor\n\nsearch_api.story_count(my_query, start_date, end_date, source_ids=sources)\nmy_query_name = my_query.replace('\"', '')\n# you can see this count by day as well\nresults = search_api.story_count_over_time(my_query, start_date, end_date, source_ids=sources)\n# and you can chart attention over time with some simple notebook work (using Bokeh here)\nimport pandas as pd\nfrom bokeh.plotting import figure, show\nfrom bokeh.models import ColumnDataSource\ndf = pd.DataFrame.from_dict(results)\ndf['date']= pd.to_datetime(df['date'])\nsource = ColumnDataSource(df)\np = figure(x_axis_type=\"datetime\", width=900, height=250)\np.line(x='date', y='count', line_width=2, source=source)  # your could use `ratio` instead of `count` to see normalized attention\nshow(p)\n\n\n\n\nNormalizing within a Source\nresults = search_api.story_count(my_query, start_date, end_date, source_ids=sources)\nsource_ratio = results['relevant'] / results['total']\nf'{source_ratio:.2%} of the media sources {sources} stories were about {my_query_name}'\n'11.46% of the media sources [1, 2, 3] stories were about politics'",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/data-collection/main.html#research-within-a-country---using-collections",
    "href": "technical-details/data-collection/main.html#research-within-a-country---using-collections",
    "title": "Data Collection",
    "section": "Research Within a Country - using collections",
    "text": "Research Within a Country - using collections\n# check in our collection of country-level US National media sources\nUS_NATIONAL_COLLECTION = 34412234\nresults = search_api.story_count(my_query, start_date, end_date, collection_ids=[US_NATIONAL_COLLECTION])\nus_country_ratio = results['relevant'] / results['total']\n# '{:.2%} of stories from national-level US media sources mentioneded \"climate change\"'.format(us_country_ratio)\nf'{us_country_ratio:.2%} of stories from national-level US media sources mentioneded {my_query_name}'\n'4.94% of stories from national-level US media sources mentioneded politics'\n# now we can compare this to the source-level coverage\ncoverage_ratio = 1 / (source_ratio / us_country_ratio)\nf'{my_query_name} received {coverage_ratio:.2%} times less coverage in {sources} than you might expect based on other US national papers'\n'politics received 43.07% times less coverage in [1, 2, 3] than you might expect based on other US national papers'\n# or compare to another country (India in this case)\nINDIA_NATIONAL = 34412118\nresults = search_api.story_count('\"climate change\"', start_date, end_date, collection_ids=[INDIA_NATIONAL])\nindia_country_ratio = results['relevant'] / results['total']\nf'{india_country_ratio:.2%} of stories from national-level Indian media sources mentioned {my_query_name}'\n'0.50% of stories from national-level Indian media sources mentioned politics'\ncoverage_ratio =  1 / (india_country_ratio / us_country_ratio)\nf'at the national level {my_query_name} is covered {coverage_ratio:.2%} times less in India than the US'\n'at the national level politics is covered 987.39% times less in India than the US'",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/data-collection/main.html#listing-stories",
    "href": "technical-details/data-collection/main.html#listing-stories",
    "title": "Data Collection",
    "section": "Listing Stories",
    "text": "Listing Stories\n# grab the most recent stories about this issue\nstories, _ = search_api.story_list(my_query, start_date, end_date)\nstories[:3]\n[{'id': '5b95e9e6c1aafd926e6adf48bb3fd28e7241fff54322889d811bda065e6bbde8',\n  'media_name': 'yourmiddleeast.com',\n  'media_url': 'yourmiddleeast.com',\n  'title': 'The Arabs’ Moment',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://yourmiddleeast.com/2020/03/06/the-arabs-moment/',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 44, 377339)},\n {'id': 'f709469e307fcba31cfd1fca3a3165c76754c9b8a42f72f1e42423b38f791977',\n  'media_name': 'yahoo.com',\n  'media_url': 'yahoo.com',\n  'title': \"Black voters power Joe Biden's Super Tuesday success\",\n  'publish_date': datetime.date(2020, 3, 5),\n  'url': 'https://news.yahoo.com/black-voters-power-bidens-super-055038667.html',\n  'language': 'en',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 42, 376413)},\n {'id': 'cd8607202f26bed7d766b5a884decf83e73ebbeea46095ebbb6f0fb1379ba6b9',\n  'media_name': 'patrika.com',\n  'media_url': 'patrika.com',\n  'title': 'ईरान: Coronavirus से विदेश मंत्री के सलाहकार की गई जान, देश में अब तक 107 की मौत',\n  'publish_date': datetime.date(2020, 3, 6),\n  'url': 'https://www.patrika.com/gulf-news/iran-foreign-minister-s-advisor-hossein-sheikholeslam-dead-by-coronavirus-107-dead-in-country-so-far-5864564/',\n  'language': 'hi',\n  'indexed_date': datetime.datetime(2024, 11, 25, 4, 59, 39, 796407)}]\n# let's fetch all the stories matching our query on one day\nall_stories = []\nmore_stories = True\npagination_token = None\nstory_start_date = dt.date(2023,11,29)\nstory_end_date = dt.date(2023,11,30)\nwhile more_stories:\n    page, pagination_token = search_api.story_list(my_query, story_start_date, story_end_date,\n                                                   collection_ids=[US_NATIONAL_COLLECTION],\n                                                   pagination_token=pagination_token)\n    all_stories += page\n    more_stories = pagination_token is not None\nlen(all_stories)\n1203\n\nWriting a CSV of Story Data\nimport csv\n\nstory_start_date_str = story_start_date.strftime('%Y%m%d')\nstory_end_date_str = story_end_date.strftime('%Y%m%d')\n\nfieldnames = ['id', 'publish_date', 'title', 'url', 'language', 'media_name', 'media_url', 'indexed_date']\noutput_filename = f\"./mediacloud_api_data/{my_query_name.replace(' ', '_')}-storylist-{story_start_date_str}_{story_end_date_str}.csv\"\noutput_dir = os.path.dirname(output_filename)\n\nif output_dir and not os.path.exists(output_dir):\n    os.makedirs(output_dir)  # Create the directory if it doesn't exist\n    \nwith open(output_filename, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n    writer.writeheader()\n    for s in all_stories:\n        writer.writerow(s)",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/data-collection/main.html#top-media-sources",
    "href": "technical-details/data-collection/main.html#top-media-sources",
    "title": "Data Collection",
    "section": "Top Media Sources",
    "text": "Top Media Sources\n# List media producing the most stories matching the search\nINDIA_NATIONAL = 34412118\nresults = search_api.sources(my_query, start_date, end_date, collection_ids=[INDIA_NATIONAL])",
    "crumbs": [
      "Technical details",
      "Data Collection"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "This notebook demonstrates a workflow for supervised learning, specifically focusing on detecting misinformation using linguistic features and sentiment analysis. The dataset used is the LIAR dataset, which contains labeled statements.\n\n\nWe start by loading the dataset and exploring its structure and contents.\n\nimport pandas as pd\n\n# Load the dataset\nliar_data = pd.read_csv('../../data/Clean_Data/liar_dataset/train.csv')\n\n# Explore the data\nprint(liar_data.info())\nliar_data.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10240 entries, 0 to 10239\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   10240 non-null  object \n 1   Label                10240 non-null  object \n 2   Statement            10240 non-null  object \n 3   Subjects             10238 non-null  object \n 4   Speaker              10238 non-null  object \n 5   Job_Title            7342 non-null   object \n 6   State_Info           8030 non-null   object \n 7   Party                10238 non-null  object \n 8   Barely_True_Count    10238 non-null  float64\n 9   False_Count          10238 non-null  float64\n 10  Half_True_Count      10238 non-null  float64\n 11  Mostly_True_Count    10238 non-null  float64\n 12  Pants_On_Fire_Count  10238 non-null  float64\n 13  Context              10138 non-null  object \ndtypes: float64(5), object(9)\nmemory usage: 1.1+ MB\nNone\n\n\n\n\n\n\n\n\n\nID\nLabel\nStatement\nSubjects\nSpeaker\nJob_Title\nState_Info\nParty\nBarely_True_Count\nFalse_Count\nHalf_True_Count\nMostly_True_Count\nPants_On_Fire_Count\nContext\n\n\n\n\n0\n2635.json\nfalse\nSays the Annies List political group supports ...\nabortion\ndwayne-bohac\nState representative\nTexas\nrepublican\n0.0\n1.0\n0.0\n0.0\n0.0\na mailer\n\n\n1\n10540.json\nhalf-true\nWhen did the decline of coal start? It started...\nenergy,history,job-accomplishments\nscott-surovell\nState delegate\nVirginia\ndemocrat\n0.0\n0.0\n1.0\n1.0\n0.0\na floor speech.\n\n\n2\n324.json\nmostly-true\nHillary Clinton agrees with John McCain \"by vo...\nforeign-policy\nbarack-obama\nPresident\nIllinois\ndemocrat\n70.0\n71.0\n160.0\n163.0\n9.0\nDenver\n\n\n3\n1123.json\nfalse\nHealth care reform legislation is likely to ma...\nhealth-care\nblog-posting\nNaN\nNaN\nnone\n7.0\n19.0\n3.0\n5.0\n44.0\na news release\n\n\n4\n9028.json\nhalf-true\nThe economic turnaround started at the end of ...\neconomy,jobs\ncharlie-crist\nNaN\nFlorida\ndemocrat\n15.0\n9.0\n20.0\n19.0\n2.0\nan interview on CNN\n\n\n\n\n\n\n\n\nliar_data['Label'].value_counts()\n\nLabel\nhalf-true      2114\nfalse          1995\nmostly-true    1962\ntrue           1676\nbarely-true    1654\npants-fire      839\nName: count, dtype: int64\n\n\n\n\n\nText data is preprocessed to remove stopwords, punctuation, and to tokenize the text. This step is crucial for extracting meaningful features from the text.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\n# Preprocessing function\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [t for t in tokens if t.isalnum()]\n    tokens = [t for t in tokens if t not in stopwords.words('english') + list(string.punctuation)]\n    return ' '.join(tokens)\n\n# Apply preprocessing\nliar_data['cleaned_text'] = liar_data['Statement'].apply(preprocess_text)\n\n[nltk_data] Downloading package punkt to /Users/pengli/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n\n\n\n\n\nLinguistic features are extracted using TF-IDF for lexical features and POS tagging for syntactic features.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nnltk.download('averaged_perceptron_tagger_eng')\n\n# Extract lexical features using TF-IDF\nvectorizer = TfidfVectorizer(max_features=1000)  # Adjust as needed\nX_lexical = vectorizer.fit_transform(liar_data['cleaned_text'])\n\n# Extract syntactic features using POS tagging\ndef pos_features(text):\n    tokens = word_tokenize(text)\n    pos_tags = nltk.pos_tag(tokens)\n    pos_counts = nltk.FreqDist(tag for _, tag in pos_tags)\n    return pos_counts\n\nliar_data['pos_features'] = liar_data['cleaned_text'].apply(pos_features)\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\n\nliar_data['pos_features']\n\n0                   {'VBZ': 2, 'NNS': 2, 'JJ': 2, 'NN': 2}\n1          {'NN': 8, 'VBD': 2, 'JJ': 1, 'VBN': 1, 'CD': 1}\n2        {'JJ': 2, 'NN': 6, 'VBZ': 1, 'VBG': 1, 'VB': 1...\n3                             {'NN': 7, 'JJ': 2, 'NNS': 1}\n4                             {'JJ': 2, 'NN': 2, 'VBD': 1}\n                               ...                        \n10235              {'JJR': 1, 'NN': 3, 'NNS': 2, 'VBP': 2}\n10236               {'NNS': 2, 'VBP': 1, 'NN': 2, 'JJ': 2}\n10237    {'VBZ': 2, 'JJ': 5, 'NN': 6, 'VBD': 1, 'NNS': ...\n10238                                  {'VBG': 2, 'NN': 4}\n10239    {'NN': 5, 'NNS': 3, 'JJ': 1, 'VBG': 2, 'VBP': ...\nName: pos_features, Length: 10240, dtype: object\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x['MD']), data=liar_data)\nplt.title('Modal Verb Usage by Label')\nplt.show()\n\n# Plot the distribution of noun usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('NN', 0)), data=liar_data)\nplt.title('Noun Usage by Label')\nplt.show()\n\n# Plot the distribution of verb usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('VB', 0)), data=liar_data)\nplt.title('Verb Usage by Label')\nplt.show()\n\n# Plot the distribution of adjective usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('JJ', 0)), data=liar_data)\nplt.title('Adjective Usage by Label')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart-of-Speech (POS) tagging is a process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. Here are some common POS tags and their meanings:\n\nNN: Noun, singular or mass\nNNS: Noun, plural\nVB: Verb, base form\nVBD: Verb, past tense\nVBG: Verb, gerund or present participle\nVBZ: Verb, 3rd person singular present\nJJ: Adjective\nRB: Adverb\nIN: Preposition or subordinating conjunction\nCD: Cardinal number\n\nIn the context of the liar_data DataFrame, the pos_features column contains the POS tagging results for each statement. For example:\n\n{‘VBZ’: 2, ‘NNS’: 2, ‘JJ’: 2, ‘NN’: 2}: This indicates that the statement contains 2 verbs in the 3rd person singular present form, 2 plural nouns, 2 adjectives, and 2 singular nouns.\n{‘NN’: 8, ‘VBD’: 2, ‘JJ’: 1, ‘VBN’: 1, ‘CD’: 1}: This indicates that the statement contains 8 singular nouns, 2 past tense verbs, 1 adjective, 1 past participle verb, and 1 cardinal number.\n\nThese features help in understanding the syntactic structure of the statements and can be used to extract meaningful patterns for detecting misinformation. Yet in this research, POS tagging analysis did not yield strong correlations with truthfulness categories, suggesting that syntactic features alone may not be reliable indicators of misinformation.\n\n\n\n\nWe split the data into training and testing sets, train a RandomForestClassifier, and evaluate its performance.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Split data\nX = X_lexical  # Or combine with other feature sets\ny = liar_data['Label']  # Target variable\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a classifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n barely-true       0.19      0.15      0.17       482\n       false       0.24      0.33      0.28       603\n   half-true       0.26      0.27      0.26       675\n mostly-true       0.24      0.28      0.26       582\n  pants-fire       0.23      0.11      0.15       236\n        true       0.22      0.17      0.19       494\n\n    accuracy                           0.23      3072\n   macro avg       0.23      0.22      0.22      3072\nweighted avg       0.23      0.23      0.23      3072\n\n\n\nInterpretation: While the model’s overall accuracy remains modest, its ability to differentiate between truthfulness categories provides valuable insights into linguistic markers of misinformation.\n\n\n\nFeature importance is analyzed to understand which linguistic markers are most indicative of misinformation.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot feature importance\nfeature_importance = clf.feature_importances_\nindices = np.argsort(feature_importance)[-10:]  # Top 10 features\nplt.barh(range(len(indices)), feature_importance[indices])\nplt.yticks(range(len(indices)), [vectorizer.get_feature_names_out()[i] for i in indices])\nplt.xlabel('Feature Importance')\nplt.title('Top Linguistic Markers')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe top linguistic markers indicative of misinformation, specifically for the “pants-fire” and “false” labels, are identified through feature importance analysis. These markers are derived from the TF-IDF vectorization of the cleaned text data. The top features for both labels are as follows:\n\nstates\nwould\nyears\npeople\npresident\ntax\nobama\nstate\npercent\nsays\n\nThese markers highlight common terms that frequently appear in statements labeled as “pants-fire” or “false.” Understanding these markers can help in identifying patterns and characteristics of misinformation in textual data.\n\n\n\n\nSentiment analysis is performed using two methods: - VADER sentiment analysis - Hugging Face Transformers\nThe results are visualized to understand the distribution of sentiment across different truthfulness categories.\n\n\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\nsia = SentimentIntensityAnalyzer()\n\nliar_data['sentiment_score'] = liar_data['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\nliar_data['sentiment_label'] = liar_data['sentiment_score'].apply(\n    lambda x: 'positive' if x &gt; 0 else ('negative' if x &lt; 0 else 'neutral')\n)\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\n\nliar_data.groupby(['Label', 'sentiment_label']).size()\n\nLabel        sentiment_label\nbarely-true  negative           534\n             neutral            536\n             positive           584\nfalse        negative           579\n             neutral            767\n             positive           649\nhalf-true    negative           736\n             neutral            665\n             positive           713\nmostly-true  negative           651\n             neutral            685\n             positive           626\npants-fire   negative           263\n             neutral            285\n             positive           291\ntrue         negative           508\n             neutral            633\n             positive           535\ndtype: int64\n\n\n\nsns.countplot(data=liar_data, x='Label', hue='sentiment_label')\nplt.title('VADER Sentiment Distribution Across Truthfulness Categories')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by Label and sentiment_label and count occurrences\nsentiment_counts = liar_data.groupby(['Label', 'sentiment_label']).size().unstack(fill_value=0)\n\n# Calculate the ratio of positive to negative sentiment for each label\nsentiment_counts['positive_to_negative_ratio'] = sentiment_counts['positive'] / sentiment_counts['negative']\n\n# Display the result\nprint(sentiment_counts[['positive', 'negative', 'positive_to_negative_ratio']])\n\n# Plot the ratio of positive to negative sentiment for each label\nsentiment_counts[['positive', 'negative']].plot(kind='bar', stacked=True)\nplt.title('VADER - Stacked Bar Plot of Positive and Negative Sentiment Counts by Label')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\nsentiment_label  positive  negative  positive_to_negative_ratio\nLabel                                                          \nbarely-true           584       534                    1.093633\nfalse                 649       579                    1.120898\nhalf-true             713       736                    0.968750\nmostly-true           626       651                    0.961598\npants-fire            291       263                    1.106464\ntrue                  535       508                    1.053150\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import pipeline\n\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\nliar_data['sentiment'] = liar_data['cleaned_text'].apply(lambda x: sentiment_pipeline(x)[0]['label'])\n\n/Users/pengli/Documents/GitHub/Project-5000-identify-misinformation/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cpu\n\n\n\nliar_data.groupby(['Label', 'sentiment']).size()\n\nLabel        sentiment\nbarely-true  NEGATIVE     1209\n             POSITIVE      445\nfalse        NEGATIVE     1394\n             POSITIVE      601\nhalf-true    NEGATIVE     1515\n             POSITIVE      599\nmostly-true  NEGATIVE     1344\n             POSITIVE      618\npants-fire   NEGATIVE      636\n             POSITIVE      203\ntrue         NEGATIVE     1130\n             POSITIVE      546\ndtype: int64\n\n\n\nsns.countplot(data=liar_data, x='Label', hue='sentiment')\nplt.title('Distilbert Sentiment Distribution Across Truthfulness Categories')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by Label and sentiment_label and count occurrences\nsentiment2_counts = liar_data.groupby(['Label', 'sentiment']).size().unstack(fill_value=0)\n\n# Calculate the ratio of POSITIVE to NEGATIVE sentiment for each label\nsentiment2_counts['POSITIVE_to_NEGATIVE_ratio'] = sentiment2_counts['POSITIVE'] / sentiment2_counts['NEGATIVE']\n\n# Display the result\nprint(sentiment2_counts[['POSITIVE', 'NEGATIVE', 'POSITIVE_to_NEGATIVE_ratio']])\n\n# Plot the ratio of POSITIVE to NEGATIVE sentiment for each label\nsentiment2_counts[['POSITIVE', 'NEGATIVE']].plot(kind='bar', stacked=True)\nplt.title('Distilbert - Stacked Bar Plot of POSITIVE and NEGATIVE Sentiment Counts by Label')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\nsentiment    POSITIVE  NEGATIVE  POSITIVE_to_NEGATIVE_ratio\nLabel                                                      \nbarely-true       445      1209                    0.368073\nfalse             601      1394                    0.431133\nhalf-true         599      1515                    0.395380\nmostly-true       618      1344                    0.459821\npants-fire        203       636                    0.319182\ntrue              546      1130                    0.483186\n\n\n\n\n\n\n\n\n\n\nliar_data.to_csv('su_processed_liar_data.csv', index=False)",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#load-and-explore-the-dataset",
    "href": "technical-details/supervised-learning/main.html#load-and-explore-the-dataset",
    "title": "Supervised Learning",
    "section": "",
    "text": "We start by loading the dataset and exploring its structure and contents.\n\nimport pandas as pd\n\n# Load the dataset\nliar_data = pd.read_csv('../../data/Clean_Data/liar_dataset/train.csv')\n\n# Explore the data\nprint(liar_data.info())\nliar_data.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 10240 entries, 0 to 10239\nData columns (total 14 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   ID                   10240 non-null  object \n 1   Label                10240 non-null  object \n 2   Statement            10240 non-null  object \n 3   Subjects             10238 non-null  object \n 4   Speaker              10238 non-null  object \n 5   Job_Title            7342 non-null   object \n 6   State_Info           8030 non-null   object \n 7   Party                10238 non-null  object \n 8   Barely_True_Count    10238 non-null  float64\n 9   False_Count          10238 non-null  float64\n 10  Half_True_Count      10238 non-null  float64\n 11  Mostly_True_Count    10238 non-null  float64\n 12  Pants_On_Fire_Count  10238 non-null  float64\n 13  Context              10138 non-null  object \ndtypes: float64(5), object(9)\nmemory usage: 1.1+ MB\nNone\n\n\n\n\n\n\n\n\n\nID\nLabel\nStatement\nSubjects\nSpeaker\nJob_Title\nState_Info\nParty\nBarely_True_Count\nFalse_Count\nHalf_True_Count\nMostly_True_Count\nPants_On_Fire_Count\nContext\n\n\n\n\n0\n2635.json\nfalse\nSays the Annies List political group supports ...\nabortion\ndwayne-bohac\nState representative\nTexas\nrepublican\n0.0\n1.0\n0.0\n0.0\n0.0\na mailer\n\n\n1\n10540.json\nhalf-true\nWhen did the decline of coal start? It started...\nenergy,history,job-accomplishments\nscott-surovell\nState delegate\nVirginia\ndemocrat\n0.0\n0.0\n1.0\n1.0\n0.0\na floor speech.\n\n\n2\n324.json\nmostly-true\nHillary Clinton agrees with John McCain \"by vo...\nforeign-policy\nbarack-obama\nPresident\nIllinois\ndemocrat\n70.0\n71.0\n160.0\n163.0\n9.0\nDenver\n\n\n3\n1123.json\nfalse\nHealth care reform legislation is likely to ma...\nhealth-care\nblog-posting\nNaN\nNaN\nnone\n7.0\n19.0\n3.0\n5.0\n44.0\na news release\n\n\n4\n9028.json\nhalf-true\nThe economic turnaround started at the end of ...\neconomy,jobs\ncharlie-crist\nNaN\nFlorida\ndemocrat\n15.0\n9.0\n20.0\n19.0\n2.0\nan interview on CNN\n\n\n\n\n\n\n\n\nliar_data['Label'].value_counts()\n\nLabel\nhalf-true      2114\nfalse          1995\nmostly-true    1962\ntrue           1676\nbarely-true    1654\npants-fire      839\nName: count, dtype: int64",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#preprocess-text-data",
    "href": "technical-details/supervised-learning/main.html#preprocess-text-data",
    "title": "Supervised Learning",
    "section": "",
    "text": "Text data is preprocessed to remove stopwords, punctuation, and to tokenize the text. This step is crucial for extracting meaningful features from the text.\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport string\n\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\n# Preprocessing function\ndef preprocess_text(text):\n    tokens = word_tokenize(text.lower())\n    tokens = [t for t in tokens if t.isalnum()]\n    tokens = [t for t in tokens if t not in stopwords.words('english') + list(string.punctuation)]\n    return ' '.join(tokens)\n\n# Apply preprocessing\nliar_data['cleaned_text'] = liar_data['Statement'].apply(preprocess_text)\n\n[nltk_data] Downloading package punkt to /Users/pengli/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#extract-linguistic-features",
    "href": "technical-details/supervised-learning/main.html#extract-linguistic-features",
    "title": "Supervised Learning",
    "section": "",
    "text": "Linguistic features are extracted using TF-IDF for lexical features and POS tagging for syntactic features.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nnltk.download('averaged_perceptron_tagger_eng')\n\n# Extract lexical features using TF-IDF\nvectorizer = TfidfVectorizer(max_features=1000)  # Adjust as needed\nX_lexical = vectorizer.fit_transform(liar_data['cleaned_text'])\n\n# Extract syntactic features using POS tagging\ndef pos_features(text):\n    tokens = word_tokenize(text)\n    pos_tags = nltk.pos_tag(tokens)\n    pos_counts = nltk.FreqDist(tag for _, tag in pos_tags)\n    return pos_counts\n\nliar_data['pos_features'] = liar_data['cleaned_text'].apply(pos_features)\n\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n[nltk_data]       date!\n\n\n\nliar_data['pos_features']\n\n0                   {'VBZ': 2, 'NNS': 2, 'JJ': 2, 'NN': 2}\n1          {'NN': 8, 'VBD': 2, 'JJ': 1, 'VBN': 1, 'CD': 1}\n2        {'JJ': 2, 'NN': 6, 'VBZ': 1, 'VBG': 1, 'VB': 1...\n3                             {'NN': 7, 'JJ': 2, 'NNS': 1}\n4                             {'JJ': 2, 'NN': 2, 'VBD': 1}\n                               ...                        \n10235              {'JJR': 1, 'NN': 3, 'NNS': 2, 'VBP': 2}\n10236               {'NNS': 2, 'VBP': 1, 'NN': 2, 'JJ': 2}\n10237    {'VBZ': 2, 'JJ': 5, 'NN': 6, 'VBD': 1, 'NNS': ...\n10238                                  {'VBG': 2, 'NN': 4}\n10239    {'NN': 5, 'NNS': 3, 'JJ': 1, 'VBG': 2, 'VBP': ...\nName: pos_features, Length: 10240, dtype: object\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x['MD']), data=liar_data)\nplt.title('Modal Verb Usage by Label')\nplt.show()\n\n# Plot the distribution of noun usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('NN', 0)), data=liar_data)\nplt.title('Noun Usage by Label')\nplt.show()\n\n# Plot the distribution of verb usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('VB', 0)), data=liar_data)\nplt.title('Verb Usage by Label')\nplt.show()\n\n# Plot the distribution of adjective usage by label\nsns.boxplot(x='Label', y=liar_data['pos_features'].apply(lambda x: x.get('JJ', 0)), data=liar_data)\nplt.title('Adjective Usage by Label')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart-of-Speech (POS) tagging is a process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context. Here are some common POS tags and their meanings:\n\nNN: Noun, singular or mass\nNNS: Noun, plural\nVB: Verb, base form\nVBD: Verb, past tense\nVBG: Verb, gerund or present participle\nVBZ: Verb, 3rd person singular present\nJJ: Adjective\nRB: Adverb\nIN: Preposition or subordinating conjunction\nCD: Cardinal number\n\nIn the context of the liar_data DataFrame, the pos_features column contains the POS tagging results for each statement. For example:\n\n{‘VBZ’: 2, ‘NNS’: 2, ‘JJ’: 2, ‘NN’: 2}: This indicates that the statement contains 2 verbs in the 3rd person singular present form, 2 plural nouns, 2 adjectives, and 2 singular nouns.\n{‘NN’: 8, ‘VBD’: 2, ‘JJ’: 1, ‘VBN’: 1, ‘CD’: 1}: This indicates that the statement contains 8 singular nouns, 2 past tense verbs, 1 adjective, 1 past participle verb, and 1 cardinal number.\n\nThese features help in understanding the syntactic structure of the statements and can be used to extract meaningful patterns for detecting misinformation. Yet in this research, POS tagging analysis did not yield strong correlations with truthfulness categories, suggesting that syntactic features alone may not be reliable indicators of misinformation.",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#correlate-features-with-misinformation",
    "href": "technical-details/supervised-learning/main.html#correlate-features-with-misinformation",
    "title": "Supervised Learning",
    "section": "",
    "text": "We split the data into training and testing sets, train a RandomForestClassifier, and evaluate its performance.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Split data\nX = X_lexical  # Or combine with other feature sets\ny = liar_data['Label']  # Target variable\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train a classifier\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Evaluate\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n barely-true       0.19      0.15      0.17       482\n       false       0.24      0.33      0.28       603\n   half-true       0.26      0.27      0.26       675\n mostly-true       0.24      0.28      0.26       582\n  pants-fire       0.23      0.11      0.15       236\n        true       0.22      0.17      0.19       494\n\n    accuracy                           0.23      3072\n   macro avg       0.23      0.22      0.22      3072\nweighted avg       0.23      0.23      0.23      3072\n\n\n\nInterpretation: While the model’s overall accuracy remains modest, its ability to differentiate between truthfulness categories provides valuable insights into linguistic markers of misinformation.",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#analyze-results",
    "href": "technical-details/supervised-learning/main.html#analyze-results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Feature importance is analyzed to understand which linguistic markers are most indicative of misinformation.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot feature importance\nfeature_importance = clf.feature_importances_\nindices = np.argsort(feature_importance)[-10:]  # Top 10 features\nplt.barh(range(len(indices)), feature_importance[indices])\nplt.yticks(range(len(indices)), [vectorizer.get_feature_names_out()[i] for i in indices])\nplt.xlabel('Feature Importance')\nplt.title('Top Linguistic Markers')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe top linguistic markers indicative of misinformation, specifically for the “pants-fire” and “false” labels, are identified through feature importance analysis. These markers are derived from the TF-IDF vectorization of the cleaned text data. The top features for both labels are as follows:\n\nstates\nwould\nyears\npeople\npresident\ntax\nobama\nstate\npercent\nsays\n\nThese markers highlight common terms that frequently appear in statements labeled as “pants-fire” or “false.” Understanding these markers can help in identifying patterns and characteristics of misinformation in textual data.",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#sentiment-analysis",
    "href": "technical-details/supervised-learning/main.html#sentiment-analysis",
    "title": "Supervised Learning",
    "section": "",
    "text": "Sentiment analysis is performed using two methods: - VADER sentiment analysis - Hugging Face Transformers\nThe results are visualized to understand the distribution of sentiment across different truthfulness categories.\n\n\n\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\n\nsia = SentimentIntensityAnalyzer()\n\nliar_data['sentiment_score'] = liar_data['cleaned_text'].apply(lambda x: sia.polarity_scores(x)['compound'])\nliar_data['sentiment_label'] = liar_data['sentiment_score'].apply(\n    lambda x: 'positive' if x &gt; 0 else ('negative' if x &lt; 0 else 'neutral')\n)\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /Users/pengli/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n\n\n\nliar_data.groupby(['Label', 'sentiment_label']).size()\n\nLabel        sentiment_label\nbarely-true  negative           534\n             neutral            536\n             positive           584\nfalse        negative           579\n             neutral            767\n             positive           649\nhalf-true    negative           736\n             neutral            665\n             positive           713\nmostly-true  negative           651\n             neutral            685\n             positive           626\npants-fire   negative           263\n             neutral            285\n             positive           291\ntrue         negative           508\n             neutral            633\n             positive           535\ndtype: int64\n\n\n\nsns.countplot(data=liar_data, x='Label', hue='sentiment_label')\nplt.title('VADER Sentiment Distribution Across Truthfulness Categories')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by Label and sentiment_label and count occurrences\nsentiment_counts = liar_data.groupby(['Label', 'sentiment_label']).size().unstack(fill_value=0)\n\n# Calculate the ratio of positive to negative sentiment for each label\nsentiment_counts['positive_to_negative_ratio'] = sentiment_counts['positive'] / sentiment_counts['negative']\n\n# Display the result\nprint(sentiment_counts[['positive', 'negative', 'positive_to_negative_ratio']])\n\n# Plot the ratio of positive to negative sentiment for each label\nsentiment_counts[['positive', 'negative']].plot(kind='bar', stacked=True)\nplt.title('VADER - Stacked Bar Plot of Positive and Negative Sentiment Counts by Label')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\nsentiment_label  positive  negative  positive_to_negative_ratio\nLabel                                                          \nbarely-true           584       534                    1.093633\nfalse                 649       579                    1.120898\nhalf-true             713       736                    0.968750\nmostly-true           626       651                    0.961598\npants-fire            291       263                    1.106464\ntrue                  535       508                    1.053150\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom transformers import pipeline\n\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\nliar_data['sentiment'] = liar_data['cleaned_text'].apply(lambda x: sentiment_pipeline(x)[0]['label'])\n\n/Users/pengli/Documents/GitHub/Project-5000-identify-misinformation/.env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nNo model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nDevice set to use cpu\n\n\n\nliar_data.groupby(['Label', 'sentiment']).size()\n\nLabel        sentiment\nbarely-true  NEGATIVE     1209\n             POSITIVE      445\nfalse        NEGATIVE     1394\n             POSITIVE      601\nhalf-true    NEGATIVE     1515\n             POSITIVE      599\nmostly-true  NEGATIVE     1344\n             POSITIVE      618\npants-fire   NEGATIVE      636\n             POSITIVE      203\ntrue         NEGATIVE     1130\n             POSITIVE      546\ndtype: int64\n\n\n\nsns.countplot(data=liar_data, x='Label', hue='sentiment')\nplt.title('Distilbert Sentiment Distribution Across Truthfulness Categories')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Group by Label and sentiment_label and count occurrences\nsentiment2_counts = liar_data.groupby(['Label', 'sentiment']).size().unstack(fill_value=0)\n\n# Calculate the ratio of POSITIVE to NEGATIVE sentiment for each label\nsentiment2_counts['POSITIVE_to_NEGATIVE_ratio'] = sentiment2_counts['POSITIVE'] / sentiment2_counts['NEGATIVE']\n\n# Display the result\nprint(sentiment2_counts[['POSITIVE', 'NEGATIVE', 'POSITIVE_to_NEGATIVE_ratio']])\n\n# Plot the ratio of POSITIVE to NEGATIVE sentiment for each label\nsentiment2_counts[['POSITIVE', 'NEGATIVE']].plot(kind='bar', stacked=True)\nplt.title('Distilbert - Stacked Bar Plot of POSITIVE and NEGATIVE Sentiment Counts by Label')\nplt.xlabel('Label')\nplt.ylabel('Count')\nplt.show()\n\nsentiment    POSITIVE  NEGATIVE  POSITIVE_to_NEGATIVE_ratio\nLabel                                                      \nbarely-true       445      1209                    0.368073\nfalse             601      1394                    0.431133\nhalf-true         599      1515                    0.395380\nmostly-true       618      1344                    0.459821\npants-fire        203       636                    0.319182\ntrue              546      1130                    0.483186\n\n\n\n\n\n\n\n\n\n\nliar_data.to_csv('su_processed_liar_data.csv', index=False)",
    "crumbs": [
      "Technical details",
      "Supervised Learning"
    ]
  }
]