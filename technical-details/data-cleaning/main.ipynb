{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through each of our datasources cleaning and combining datasets where necessary. It includes the following sections:\n",
    "\n",
    "1. **Mediacloud API Data**:\n",
    "   - Loads data from CSV files.\n",
    "   - Drops duplicate columns (`media_url`).\n",
    "   - Saves the cleaned data back to CSV files.\n",
    "\n",
    "2. **Liar Dataset**:\n",
    "   - Loads data from TSV files.\n",
    "   - Defines column names.\n",
    "   - Saves the data in CSV format.\n",
    "\n",
    "3. **NewsAPI Dataset**:\n",
    "   - Loads data from a JSON file.\n",
    "   - Extracts and processes various fields (source, author, title, description, URL, content, and publication date).\n",
    "   - Saves the processed data to a CSV file.\n",
    "\n",
    "4. **ChatGPT Integration**:\n",
    "   - Attempts to use OpenAI's GPT model to rate the truthfulness of news articles.\n",
    "   - Defines a function to interact with the GPT model.\n",
    "   - Loads the cleaned NewsAPI data and sends prompts to the GPT model for rating.\n",
    "\n",
    "5. **Conclusion**:\n",
    "   - Notes that OpenAI's models do not have access to real-time data for claim validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mediacloud API Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the media_url column and media_name column are exactly the same we don't need both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"../../data/Raw_Data/mediacloud_api_data/covid-story-list.csv\")\n",
    "df2 = pd.read_csv(\"../../data/Raw_Data/mediacloud_api_data/politics-storylist-20231129_20231130.csv\")\n",
    "\n",
    "# Drop Duplicate Column\n",
    "df = df.drop(columns=['media_url'])\n",
    "df2 = df2.drop(columns=['media_url'])\n",
    "\n",
    "# Save\n",
    "df.to_csv((\"../../data/Clean_Data/mediacloud_api_data/covid-story-list.csv\"), index=None)\n",
    "df2.to_csv((\"../../data/Clean_Data/mediacloud_api_data/politics-storylist-20231129_20231130.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Liar Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is stored in a TSV file. Lets extract it and store it in an easier to use format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "columns = [\"ID\", \"Label\", \"Statement\", \"Subjects\", \"Speaker\", \"Job_Title\", \"State_Info\", \"Party\", \"Barely_True_Count\", \"False_Count\", \"Half_True_Count\", \"Mostly_True_Count\", \"Pants_On_Fire_Count\", \"Context\"]\n",
    "\n",
    "# Load the data and add the column names\n",
    "test_df = pd.read_csv(\"../../data/Raw_Data/liar_dataset/test.tsv\", sep='\\t', names=columns)\n",
    "train_df = pd.read_csv(\"../../data/Raw_Data/liar_dataset/train.tsv\", sep='\\t', names=columns)\n",
    "valid_df = pd.read_csv(\"../../data/Raw_Data/liar_dataset/valid.tsv\", sep='\\t', names=columns)\n",
    "\n",
    "# Save the data in csv format\n",
    "test_df.to_csv((\"../../data/Clean_Data/liar_dataset/test.csv\"), index=None)\n",
    "train_df.to_csv((\"../../data/Clean_Data/liar_dataset/train.csv\"), index=None)\n",
    "valid_df.to_csv((\"../../data/Clean_Data/liar_dataset/valid.csv\"), index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NewsAPI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is stored in a json file so we have to extract it and convert it to a usable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Import data\n",
    "with open(\"../../data/Raw_Data/news_api_data/politics_20241124_214935.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Split up data\n",
    "sources = []\n",
    "authors = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "urls = []\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "time = []\n",
    "content = []\n",
    "\n",
    "for article in data[\"articles\"]:\n",
    "    sources.append(article[\"source\"][\"name\"])\n",
    "    authors.append(article[\"author\"])\n",
    "    titles.append(article[\"title\"])\n",
    "    descriptions.append(article[\"description\"])\n",
    "    urls.append(article[\"url\"])\n",
    "    content.append(article[\"content\"])\n",
    "    x = datetime.fromisoformat(article[\"publishedAt\"].replace(\"Z\", \"+00:00\"))\n",
    "    year.append(x.year)\n",
    "    month.append(x.month)\n",
    "    day.append(x.day)\n",
    "    time.append(\"AM\" if x.hour < 12 else \"PM\")\n",
    "\n",
    "\n",
    "# Create dataframe for data and save data\n",
    "df = pd.DataFrame({\n",
    "    \"Source\": sources,\n",
    "    \"Author\": authors,\n",
    "    \"Title\": titles,\n",
    "    \"Description\": descriptions,\n",
    "    \"URL\": urls,\n",
    "    \"Post_Year\": year,\n",
    "    \"Post_Month\": month,\n",
    "    \"Post_Day\": day,\n",
    "    \"Post_Time\": time,\n",
    "    \"Post\": content\n",
    "})\n",
    "\n",
    "# Convert to csv\n",
    "df.to_csv(\"../../data/Clean_Data/news_api_data/news_api_data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this dataset, I am going to have chatgpt give me a rating on whether is beleive the information in each article so we can test what our model thinks vs chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import API key from secure file\n",
    "\n",
    "import json\n",
    "with open('../../config.json') as f:\n",
    "    keys = json.load(f)\n",
    "API_KEY = keys['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "model=\"gpt-4o\"\n",
    "temperature=0.0\n",
    "max_tokens= 50\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "def chat_with_gpt(prompt, model=model, temperature=temperature, max_tokens=max_tokens):\n",
    "    \"\"\"\n",
    "    Sends a prompt to the OpenAI ChatGPT model and returns the response.\n",
    "\n",
    "    :param prompt: The input text prompt to send to the model.\n",
    "    :param model: The model to use (e.g., \"gpt-3.5-turbo\").\n",
    "    :param temperature: Sampling temperature (controls creativity).\n",
    "    :param max_tokens: Maximum number of tokens in the response.\n",
    "    :return: The model's response as a string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "\n",
    "        # Extract the assistant's reply\n",
    "        reply = response['choices'][0]['message']['content'].strip()\n",
    "        return reply\n",
    "\n",
    "    except openai.error.OpenAIError as e:\n",
    "        # Handle errors (e.g., network issues, API errors)\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../data/Clean_Data/news_api_data/news_api_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "#x = chat_with_gpt(f\"I need you to rate an article as one of the following categories: Pants-on-Fire, False, Mostly False, Half True, Mostly True, True. Please ONLY respond with one of these categories. Here is the article's source: {df['Source'][i]}, Here is the article's title: {df['Title'][i]}, Here is the article's description: {df['Description'][i]}, and here is part of the post: {df['Post'][i]}\")\n",
    "#x = chat_with_gpt(f\"Rate the truthfulness of the article using these categories: Pants-on-Fire, False, Mostly False, Half True, Mostly True, True. ONLY respond with one. Evaluate the claim from - Source: {df['Source'][i]}, Title: {df['Title'][i]}, Description: {df['Description'][i]}, Post: {df['Post'][i]} - based on facts and evidence.\")\n",
    "x = chat_with_gpt(f\"Based on the following details, fact-check and rate the claim as Pants-on-Fire, False, Mostly False, Half True, Mostly True, or True: Source: {df['Source'][i]}, Title: {df['Title'][i]}, Description: {df['Description'][i]}, Post: {df['Post'][i]}. ONLY respond with one of these categories and ensure the claim aligns with verified factual information.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunetly what I found is that none of the OpenAI API models have access to real-time data in order to validate the claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "[https://stackoverflow.com/questions/127803/how-do-i-parse-an-iso-8601-formatted-date-and-time](https://stackoverflow.com/questions/127803/how-do-i-parse-an-iso-8601-formatted-date-and-time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
